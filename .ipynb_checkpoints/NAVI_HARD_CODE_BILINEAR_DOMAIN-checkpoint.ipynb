{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NAVI_HARD_CODE_DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "#Functional coding\n",
    "import functools\n",
    "from functools import partial\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Path..\n",
    "Datapath=\"DATA/Navigation/Navigation_Data.txt\"\n",
    "Labelpath=\"DATA/Navigation/Navigation_Label.txt\"\n",
    "Rewardpath=\"DATA/Navigation/Navigation_Reward.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given local path, find full path\n",
    "def PathFinder(path):\n",
    "    #python 2\n",
    "    #script_dir = os.path.dirname('__file__')\n",
    "    #fullpath = os.path.join(script_dir,path)\n",
    "    #python 3\n",
    "    fullpath=os.path.abspath(path)\n",
    "    print(fullpath)\n",
    "    return fullpath\n",
    "\n",
    "#Read Data for Deep Learning\n",
    "def ReadData(path):\n",
    "    fullpath=PathFinder(path)\n",
    "    return pd.read_csv(fullpath, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/Navigation_Data.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/Navigation_Label.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/Navigation_Reward.txt\n"
     ]
    }
   ],
   "source": [
    "S_A_pd = ReadData(Datapath)\n",
    "SP_pd = ReadData(Labelpath)\n",
    "R_pd = ReadData(Rewardpath)\n",
    "S_A_matrix=S_A_pd.as_matrix()\n",
    "SP_matrix=SP_pd.as_matrix()\n",
    "R_matrix=R_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_settings = {\n",
    "    \"dims\"          : 2,\n",
    "    \"min_maze_bound\": tf.constant(0.0,dtype=tf.float32), \n",
    "    \"max_maze_bound\": tf.constant(10.0,dtype=tf.float32), \n",
    "    \"min_act_bound\": tf.constant(-1.0,dtype=tf.float32), \n",
    "    \"max_act_bound\": tf.constant(1.0,dtype=tf.float32), \n",
    "    \"goal\"    : tf.constant(8.0,dtype=tf.float32),\n",
    "    \"penalty\" : tf.constant(1000000.0,dtype=tf.float32),\n",
    "    \"centre\"  : tf.constant(5.0,dtype=tf.float32)\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAVI(object):\n",
    "    def __init__(self, \n",
    "                 default_settings):\n",
    "        self.__dict__.update(default_settings)\n",
    "        self.zero = tf.constant(0,dtype=tf.float32)\n",
    "        self.two = tf.constant(2.0,dtype=tf.float32)\n",
    "        self.one = tf.constant(1.0,dtype=tf.float32)\n",
    "        self.lessone = tf.constant(0.99,dtype=tf.float32)\n",
    "    \n",
    "    def MINMAZEBOUND(self, dim):\n",
    "        return self.min_maze_bound\n",
    "    \n",
    "    def MAXMAZEBOUND(self, dim):\n",
    "        return self.max_maze_bound\n",
    "    \n",
    "    def MINACTIONBOUND(self, dim):\n",
    "        return self.min_act_bound\n",
    "    \n",
    "    def MAXACTIONBOUND(self, dim):\n",
    "        return self.max_act_bound\n",
    "    \n",
    "    def GOAL(self, dim):\n",
    "        return self.goal\n",
    "    \n",
    "    def CENTER(self, dim):\n",
    "        return self.centre\n",
    "    \n",
    "    def PENALTY(self):\n",
    "        return self.penalty\n",
    "    \n",
    "    def _transition(self, dim, states_packed, actions_packed):\n",
    "        \n",
    "        states = tf.unpack(states_packed)\n",
    "        actions = tf.unpack(actions_packed)\n",
    "        \n",
    "        previous_state = states[dim]\n",
    "        \n",
    "        #distance to centre Manhattan\n",
    "        distance = self.zero\n",
    "        for i in range(len(states)):\n",
    "            distance+=tf.abs(states[i]-self.CENTER(i))\n",
    "        \n",
    "        #scale factor\n",
    "        scalefactor = tf.cond(distance<self.two, lambda: distance/self.two, lambda: self.one)\n",
    "        \n",
    "        #proposed location\n",
    "        proposedLoc = previous_state + actions[dim]*scalefactor\n",
    "        \n",
    "        #new state\n",
    "        new_state = tf.cond(tf.logical_and(proposedLoc <= self.MAXMAZEBOUND(dim), proposedLoc >= self.MINMAZEBOUND(dim)), \\\n",
    "                            lambda: proposedLoc, \\\n",
    "                            lambda: tf.cond(proposedLoc >self.MAXMAZEBOUND(dim), lambda:self.MAXMAZEBOUND(dim), lambda:self.MINMAZEBOUND(dim) ) \\\n",
    "                           )\n",
    "        \n",
    "        return new_state\n",
    "    \n",
    "    # For single data point\n",
    "    def _vector_trans(self, state_size, states_packed, actions_packed):\n",
    "        new_states=[]\n",
    "        for i in range(state_size):\n",
    "            new_states.append(self._transition(i,states_packed,actions_packed))\n",
    "        return tf.pack(new_states)\n",
    "    \n",
    "    def Transition(self, states, actions):\n",
    "        new_states = []\n",
    "        batch_size,state_size = states.get_shape()\n",
    "        states_list = tf.unpack(states)\n",
    "        actions_list = tf.unpack(actions)\n",
    "        for i in range(batch_size):\n",
    "            new_states.append(self._vector_trans(state_size,states_list[i],actions_list[i]))\n",
    "        return tf.pack(new_states)\n",
    "    \n",
    "    def _reward(self, state_size, states_packed, actions_packed):\n",
    "        reward = self.zero\n",
    "        states = tf.unpack(states_packed)\n",
    "        actions = tf.unpack(actions_packed)\n",
    "        \n",
    "        for i in range(state_size):\n",
    "            reward -= tf.abs(states[i]-self.GOAL(i))\n",
    "        return tf.pack([reward])\n",
    "    \n",
    "    def Reward(self, states,actions):\n",
    "        new_rewards = []\n",
    "        batch_size,state_size = states.get_shape()\n",
    "        states_list = tf.unpack(states)\n",
    "        actions_list = tf.unpack(actions)\n",
    "        for i in range(batch_size):\n",
    "            new_rewards.append(self._reward(state_size,states_list[i],actions_list[i]))\n",
    "        return tf.pack(new_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# States\n",
    "states = tf.placeholder(tf.float32,[10, 2],name=\"States\")\n",
    "\n",
    "# Actions\n",
    "actions = tf.placeholder(tf.float32,[10, 2],name=\"Actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "navi_inst = NAVI(default_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n",
      "[array([ 0.5490576,  0.       ], dtype=float32)]\n",
      "[array([ 0.5490576,  0.       ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "states_list=tf.unpack(states)\n",
    "actions_list = tf.unpack(actions)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict={states:S_A_matrix[:10,2:], actions:S_A_matrix[:10,:2]}\n",
    "new_state = navi_inst._transition(1,states_list[0],actions_list[0])\n",
    "print(sess.run([new_state], feed_dict=feed_dict))\n",
    "print(sess.run([states_list[1]], feed_dict=feed_dict))\n",
    "print(sess.run([states_list[1]], feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rewards = navi_inst.Reward(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-16.        ],\n",
       "       [-15.45094299],\n",
       "       [-15.34755898],\n",
       "       [-14.92229462],\n",
       "       [-13.87710953],\n",
       "       [-12.88495827],\n",
       "       [-11.48205948],\n",
       "       [-13.36318207],\n",
       "       [-13.89812565],\n",
       "       [-12.66272354]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict={states:S_A_matrix[:10,2:], actions:S_A_matrix[:10,:2]}\n",
    "sess.run(new_rewards,feed_dict=feed_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAVICell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, default_settings):\n",
    "        self._num_state_units = 2\n",
    "        self._num_reward_units = 3\n",
    "        self.navi = NAVI(default_settings)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_state_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_reward_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        next_state =  self.navi.Transition(state, inputs)\n",
    "        reward = self.navi.Reward(state, inputs)      \n",
    "        return tf.concat(1,[reward,next_state]), next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    def __init__(self,\n",
    "                a, # Actions\n",
    "                num_step, # Number of RNN step, this is a fixed step RNN sequence, 12 for navigation\n",
    "                num_act, # Number of actions\n",
    "                batch_size, #Batch Size\n",
    "                learning_rate=0.01): \n",
    "        self.action = tf.reshape(a,[-1,num_step,num_act]) #Reshape rewards\n",
    "        print(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_step = num_step\n",
    "        self.learning_rate = learning_rate\n",
    "        self._p_create_rnn_graph()\n",
    "        self._p_create_loss()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _p_create_rnn_graph(self):\n",
    "        cell = NAVICell(default_settings)\n",
    "        initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        print('action batch size:{0}'.format(array_ops.shape(self.action)[0]))\n",
    "        print('Initial_state shape:{0}'.format(initial_state))\n",
    "        rnn_outputs, state = tf.nn.dynamic_rnn(cell, self.action, dtype=tf.float32,initial_state=initial_state)\n",
    "        #need output intermediate states as well\n",
    "        concated = tf.concat(0,rnn_outputs)\n",
    "        print('concated shape:{0}'.format(concated.get_shape()))\n",
    "        something_unpacked =  tf.unpack(concated, axis=2)\n",
    "        self.outputs = tf.reshape(something_unpacked[0],[-1,self.num_step,1])\n",
    "        print(' self.outputs:{0}'.format(self.outputs.get_shape()))\n",
    "        self.intern_states = tf.pack([something_unpacked[1],something_unpacked[2]], axis=2)\n",
    "        self.last_state = state\n",
    "        self.pred = tf.reduce_sum(self.outputs,1)\n",
    "        print(\"self.pred:{0}\".format(self.pred))\n",
    "            \n",
    "    def _p_create_loss(self):\n",
    "\n",
    "        objective = tf.reduce_mean(self.pred) \n",
    "        self.loss = -objective\n",
    "        print(self.loss.get_shape())\n",
    "        #self.loss = -objective\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def Optimize(self,epoch=100,show_progress=False):\n",
    "        \n",
    "        new_loss = self.sess.run([self.loss])\n",
    "        print('Loss in epoch {0}: {1}'.format(\"Initial\", new_loss)) \n",
    "        if show_progress:\n",
    "            progress = []\n",
    "        for epoch in range(epoch):\n",
    "            training = self.sess.run([self.optimizer])\n",
    "            self.sess.run(tf.assign(a, tf.clip_by_value(a, -1, 1)))\n",
    "            if True:\n",
    "                new_loss = self.sess.run([self.loss])\n",
    "                print('Loss in epoch {0}: {1}'.format(epoch, new_loss))  \n",
    "            \n",
    "            if show_progress and epoch%10==0:\n",
    "                progress.append(self.sess.run(self.intern_states))\n",
    "        minimum_costs_id=self.sess.run(tf.argmax(self.pred,0))\n",
    "        print(minimum_costs_id)\n",
    "        best_action = np.round(self.sess.run(self.action)[minimum_costs_id[0]],4)\n",
    "        print('Optimal Action Squence:{0}'.format(best_action))\n",
    "        print('Best Cost: {0}'.format(self.sess.run(self.pred)[minimum_costs_id[0]]))\n",
    "        print('The last state:{0}'.format(self.sess.run(self.last_state)[minimum_costs_id[0]]))\n",
    "        print('Rewards each time step:{0}'.format(self.sess.run(self.outputs)[minimum_costs_id[0]]))\n",
    "        print('Intermediate states:{0}'.format(self.sess.run(self.intern_states)[minimum_costs_id[0]]))\n",
    "        if show_progress:\n",
    "            progress = np.array(progress)[:,minimum_costs_id[0]]\n",
    "            print('progress shape:{0}'.format(progress.shape))\n",
    "            np.savetxt(\"progress.csv\",progress.reshape((progress.shape[0],-1)),delimiter=\",\",fmt='%2.5f')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(10, 12, 2), dtype=float32)\n",
      "action batch size:Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Initial_state shape:Tensor(\"zeros:0\", shape=(10, 2), dtype=float32)\n",
      "concated shape:(10, 12, 3)\n",
      " self.outputs:(10, 12, 1)\n",
      "self.pred:Tensor(\"Sum:0\", shape=(10, 1), dtype=float32)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.truncated_normal(shape=[240],mean=0.0, stddev=0.5),name=\"action\")\n",
    "rnn_inst = ActionOptimizer(a, 12,2,10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch Initial: [177.13533]\n",
      "Loss in epoch 0: [176.6739]\n",
      "Loss in epoch 1: [176.19449]\n",
      "Loss in epoch 2: [175.68994]\n",
      "Loss in epoch 3: [175.17133]\n",
      "Loss in epoch 4: [174.63072]\n",
      "Loss in epoch 5: [174.0609]\n",
      "Loss in epoch 6: [173.49051]\n",
      "Loss in epoch 7: [172.91211]\n",
      "Loss in epoch 8: [172.32382]\n",
      "Loss in epoch 9: [171.72696]\n",
      "Loss in epoch 10: [171.12169]\n",
      "Loss in epoch 11: [170.50769]\n",
      "Loss in epoch 12: [169.87514]\n",
      "Loss in epoch 13: [169.23232]\n",
      "Loss in epoch 14: [168.57516]\n",
      "Loss in epoch 15: [167.92339]\n",
      "Loss in epoch 16: [167.26929]\n",
      "Loss in epoch 17: [166.60873]\n",
      "Loss in epoch 18: [165.94197]\n",
      "Loss in epoch 19: [165.27376]\n",
      "Loss in epoch 20: [164.60309]\n",
      "Loss in epoch 21: [163.92172]\n",
      "Loss in epoch 22: [163.22183]\n",
      "Loss in epoch 23: [162.52083]\n",
      "Loss in epoch 24: [161.82019]\n",
      "Loss in epoch 25: [161.11548]\n",
      "Loss in epoch 26: [160.40674]\n",
      "Loss in epoch 27: [159.68507]\n",
      "Loss in epoch 28: [158.97212]\n",
      "Loss in epoch 29: [158.24371]\n",
      "Loss in epoch 30: [157.51408]\n",
      "Loss in epoch 31: [156.77051]\n",
      "Loss in epoch 32: [155.97876]\n",
      "Loss in epoch 33: [155.17972]\n",
      "Loss in epoch 34: [154.32248]\n",
      "Loss in epoch 35: [153.48137]\n",
      "Loss in epoch 36: [152.64429]\n",
      "Loss in epoch 37: [151.79961]\n",
      "Loss in epoch 38: [150.96353]\n",
      "Loss in epoch 39: [150.13326]\n",
      "Loss in epoch 40: [149.33237]\n",
      "Loss in epoch 41: [148.46855]\n",
      "Loss in epoch 42: [147.60733]\n",
      "Loss in epoch 43: [146.7625]\n",
      "Loss in epoch 44: [145.92822]\n",
      "Loss in epoch 45: [145.11443]\n",
      "Loss in epoch 46: [144.31906]\n",
      "Loss in epoch 47: [143.53314]\n",
      "Loss in epoch 48: [142.75095]\n",
      "Loss in epoch 49: [141.9796]\n",
      "Loss in epoch 50: [141.21936]\n",
      "Loss in epoch 51: [140.46765]\n",
      "Loss in epoch 52: [139.7476]\n",
      "Loss in epoch 53: [139.07016]\n",
      "Loss in epoch 54: [138.40842]\n",
      "Loss in epoch 55: [137.77316]\n",
      "Loss in epoch 56: [137.14639]\n",
      "Loss in epoch 57: [136.52605]\n",
      "Loss in epoch 58: [135.91914]\n",
      "Loss in epoch 59: [135.31339]\n",
      "Loss in epoch 60: [134.72276]\n",
      "Loss in epoch 61: [134.16075]\n",
      "Loss in epoch 62: [133.60417]\n",
      "Loss in epoch 63: [133.08427]\n",
      "Loss in epoch 64: [132.5732]\n",
      "Loss in epoch 65: [132.03049]\n",
      "Loss in epoch 66: [131.51042]\n",
      "Loss in epoch 67: [130.99355]\n",
      "Loss in epoch 68: [130.48996]\n",
      "Loss in epoch 69: [129.99219]\n",
      "Loss in epoch 70: [129.51405]\n",
      "Loss in epoch 71: [129.05403]\n",
      "Loss in epoch 72: [128.5724]\n",
      "Loss in epoch 73: [128.09619]\n",
      "Loss in epoch 74: [127.62453]\n",
      "Loss in epoch 75: [127.15666]\n",
      "Loss in epoch 76: [126.68745]\n",
      "Loss in epoch 77: [126.22373]\n",
      "Loss in epoch 78: [125.77216]\n",
      "Loss in epoch 79: [125.29901]\n",
      "Loss in epoch 80: [124.79973]\n",
      "Loss in epoch 81: [124.21329]\n",
      "Loss in epoch 82: [123.60399]\n",
      "Loss in epoch 83: [122.92812]\n",
      "Loss in epoch 84: [122.27682]\n",
      "Loss in epoch 85: [121.67812]\n",
      "Loss in epoch 86: [121.09658]\n",
      "Loss in epoch 87: [120.54559]\n",
      "Loss in epoch 88: [120.02892]\n",
      "Loss in epoch 89: [119.5554]\n",
      "Loss in epoch 90: [119.08006]\n",
      "Loss in epoch 91: [118.59438]\n",
      "Loss in epoch 92: [118.10355]\n",
      "Loss in epoch 93: [117.64909]\n",
      "Loss in epoch 94: [117.1758]\n",
      "Loss in epoch 95: [116.74143]\n",
      "Loss in epoch 96: [116.32017]\n",
      "Loss in epoch 97: [115.8214]\n",
      "Loss in epoch 98: [115.33003]\n",
      "Loss in epoch 99: [114.854]\n",
      "Loss in epoch 100: [114.38971]\n",
      "Loss in epoch 101: [113.95683]\n",
      "Loss in epoch 102: [113.54678]\n",
      "Loss in epoch 103: [113.13165]\n",
      "Loss in epoch 104: [112.75737]\n",
      "Loss in epoch 105: [112.35846]\n",
      "Loss in epoch 106: [111.9905]\n",
      "Loss in epoch 107: [111.64259]\n",
      "Loss in epoch 108: [111.28467]\n",
      "Loss in epoch 109: [110.94807]\n",
      "Loss in epoch 110: [110.63078]\n",
      "Loss in epoch 111: [110.31956]\n",
      "Loss in epoch 112: [110.03336]\n",
      "Loss in epoch 113: [109.74847]\n",
      "Loss in epoch 114: [109.49302]\n",
      "Loss in epoch 115: [109.24512]\n",
      "Loss in epoch 116: [109.00332]\n",
      "Loss in epoch 117: [108.76897]\n",
      "Loss in epoch 118: [108.53499]\n",
      "Loss in epoch 119: [108.31173]\n",
      "Loss in epoch 120: [108.09253]\n",
      "Loss in epoch 121: [107.86654]\n",
      "Loss in epoch 122: [107.67069]\n",
      "Loss in epoch 123: [107.46969]\n",
      "Loss in epoch 124: [107.27959]\n",
      "Loss in epoch 125: [107.08419]\n",
      "Loss in epoch 126: [106.89362]\n",
      "Loss in epoch 127: [106.70402]\n",
      "Loss in epoch 128: [106.51096]\n",
      "Loss in epoch 129: [106.34579]\n",
      "Loss in epoch 130: [106.15868]\n",
      "Loss in epoch 131: [105.98682]\n",
      "Loss in epoch 132: [105.85006]\n",
      "Loss in epoch 133: [105.64275]\n",
      "Loss in epoch 134: [105.49361]\n",
      "Loss in epoch 135: [105.33785]\n",
      "Loss in epoch 136: [105.21078]\n",
      "Loss in epoch 137: [105.0498]\n",
      "Loss in epoch 138: [104.90306]\n",
      "Loss in epoch 139: [104.77444]\n",
      "Loss in epoch 140: [104.65578]\n",
      "Loss in epoch 141: [104.48926]\n",
      "Loss in epoch 142: [104.36794]\n",
      "Loss in epoch 143: [104.2466]\n",
      "Loss in epoch 144: [104.11001]\n",
      "Loss in epoch 145: [103.99132]\n",
      "Loss in epoch 146: [103.85905]\n",
      "Loss in epoch 147: [103.73069]\n",
      "Loss in epoch 148: [103.61804]\n",
      "Loss in epoch 149: [103.5146]\n",
      "Loss in epoch 150: [103.41768]\n",
      "Loss in epoch 151: [103.30797]\n",
      "Loss in epoch 152: [103.21935]\n",
      "Loss in epoch 153: [103.15]\n",
      "Loss in epoch 154: [103.04256]\n",
      "Loss in epoch 155: [102.98451]\n",
      "Loss in epoch 156: [102.86069]\n",
      "Loss in epoch 157: [102.79033]\n",
      "Loss in epoch 158: [102.68559]\n",
      "Loss in epoch 159: [102.61989]\n",
      "Loss in epoch 160: [102.53359]\n",
      "Loss in epoch 161: [102.48186]\n",
      "Loss in epoch 162: [102.37099]\n",
      "Loss in epoch 163: [102.3196]\n",
      "Loss in epoch 164: [102.24254]\n",
      "Loss in epoch 165: [102.13633]\n",
      "Loss in epoch 166: [102.07823]\n",
      "Loss in epoch 167: [102.05405]\n",
      "Loss in epoch 168: [101.95357]\n",
      "Loss in epoch 169: [101.92844]\n",
      "Loss in epoch 170: [101.87678]\n",
      "Loss in epoch 171: [101.79668]\n",
      "Loss in epoch 172: [101.73678]\n",
      "Loss in epoch 173: [101.69897]\n",
      "Loss in epoch 174: [101.65981]\n",
      "Loss in epoch 175: [101.6281]\n",
      "Loss in epoch 176: [101.5472]\n",
      "Loss in epoch 177: [101.49083]\n",
      "Loss in epoch 178: [101.45586]\n",
      "Loss in epoch 179: [101.42881]\n",
      "Loss in epoch 180: [101.3624]\n",
      "Loss in epoch 181: [101.34874]\n",
      "Loss in epoch 182: [101.28381]\n",
      "Loss in epoch 183: [101.26787]\n",
      "Loss in epoch 184: [101.20979]\n",
      "Loss in epoch 185: [101.19546]\n",
      "Loss in epoch 186: [101.14413]\n",
      "Loss in epoch 187: [101.12099]\n",
      "Loss in epoch 188: [101.0645]\n",
      "Loss in epoch 189: [101.06929]\n",
      "Loss in epoch 190: [100.98679]\n",
      "Loss in epoch 191: [100.95534]\n",
      "Loss in epoch 192: [100.9651]\n",
      "Loss in epoch 193: [100.92807]\n",
      "Loss in epoch 194: [100.86935]\n",
      "Loss in epoch 195: [100.87697]\n",
      "Loss in epoch 196: [100.82127]\n",
      "Loss in epoch 197: [100.78784]\n",
      "Loss in epoch 198: [100.77594]\n",
      "Loss in epoch 199: [100.73334]\n",
      "Loss in epoch 200: [100.71513]\n",
      "Loss in epoch 201: [100.7404]\n",
      "Loss in epoch 202: [100.67799]\n",
      "Loss in epoch 203: [100.66111]\n",
      "Loss in epoch 204: [100.65132]\n",
      "Loss in epoch 205: [100.60847]\n",
      "Loss in epoch 206: [100.58412]\n",
      "Loss in epoch 207: [100.60746]\n",
      "Loss in epoch 208: [100.54944]\n",
      "Loss in epoch 209: [100.55054]\n",
      "Loss in epoch 210: [100.52824]\n",
      "Loss in epoch 211: [100.49275]\n",
      "Loss in epoch 212: [100.47542]\n",
      "Loss in epoch 213: [100.50339]\n",
      "Loss in epoch 214: [100.43633]\n",
      "Loss in epoch 215: [100.45654]\n",
      "Loss in epoch 216: [100.41255]\n",
      "Loss in epoch 217: [100.41358]\n",
      "Loss in epoch 218: [100.37456]\n",
      "Loss in epoch 219: [100.36494]\n",
      "Loss in epoch 220: [100.33443]\n",
      "Loss in epoch 221: [100.34007]\n",
      "Loss in epoch 222: [100.30575]\n",
      "Loss in epoch 223: [100.26853]\n",
      "Loss in epoch 224: [100.2755]\n",
      "Loss in epoch 225: [100.29265]\n",
      "Loss in epoch 226: [100.25492]\n",
      "Loss in epoch 227: [100.26967]\n",
      "Loss in epoch 228: [100.22099]\n",
      "Loss in epoch 229: [100.21339]\n",
      "Loss in epoch 230: [100.16357]\n",
      "Loss in epoch 231: [100.18258]\n",
      "Loss in epoch 232: [100.13763]\n",
      "Loss in epoch 233: [100.12914]\n",
      "Loss in epoch 234: [100.14575]\n",
      "Loss in epoch 235: [100.09718]\n",
      "Loss in epoch 236: [100.10948]\n",
      "Loss in epoch 237: [100.14302]\n",
      "Loss in epoch 238: [100.07729]\n",
      "Loss in epoch 239: [100.09335]\n",
      "Loss in epoch 240: [100.05976]\n",
      "Loss in epoch 241: [100.06955]\n",
      "Loss in epoch 242: [100.02184]\n",
      "Loss in epoch 243: [100.04629]\n",
      "Loss in epoch 244: [100.02393]\n",
      "Loss in epoch 245: [100.00429]\n",
      "Loss in epoch 246: [99.999191]\n",
      "Loss in epoch 247: [99.994423]\n",
      "Loss in epoch 248: [99.958206]\n",
      "Loss in epoch 249: [99.969231]\n",
      "Loss in epoch 250: [99.957314]\n",
      "Loss in epoch 251: [99.929649]\n",
      "Loss in epoch 252: [99.900223]\n",
      "Loss in epoch 253: [99.925484]\n",
      "Loss in epoch 254: [99.921028]\n",
      "Loss in epoch 255: [99.920006]\n",
      "Loss in epoch 256: [99.883102]\n",
      "Loss in epoch 257: [99.893784]\n",
      "Loss in epoch 258: [99.854538]\n",
      "Loss in epoch 259: [99.902649]\n",
      "Loss in epoch 260: [99.843613]\n",
      "Loss in epoch 261: [99.857735]\n",
      "Loss in epoch 262: [99.850601]\n",
      "Loss in epoch 263: [99.851379]\n",
      "Loss in epoch 264: [99.811935]\n",
      "Loss in epoch 265: [99.830383]\n",
      "Loss in epoch 266: [99.816757]\n",
      "Loss in epoch 267: [99.850494]\n",
      "Loss in epoch 268: [99.802353]\n",
      "Loss in epoch 269: [99.818108]\n",
      "Loss in epoch 270: [99.774582]\n",
      "Loss in epoch 271: [99.786667]\n",
      "Loss in epoch 272: [99.776169]\n",
      "Loss in epoch 273: [99.773071]\n",
      "Loss in epoch 274: [99.761887]\n",
      "Loss in epoch 275: [99.766479]\n",
      "Loss in epoch 276: [99.759689]\n",
      "Loss in epoch 277: [99.79155]\n",
      "Loss in epoch 278: [99.741241]\n",
      "Loss in epoch 279: [99.753685]\n",
      "Loss in epoch 280: [99.7145]\n",
      "Loss in epoch 281: [99.744263]\n",
      "Loss in epoch 282: [99.710381]\n",
      "Loss in epoch 283: [99.7174]\n",
      "Loss in epoch 284: [99.725044]\n",
      "Loss in epoch 285: [99.699745]\n",
      "Loss in epoch 286: [99.693687]\n",
      "Loss in epoch 287: [99.744698]\n",
      "Loss in epoch 288: [99.662918]\n",
      "Loss in epoch 289: [99.707565]\n",
      "Loss in epoch 290: [99.676422]\n",
      "Loss in epoch 291: [99.689781]\n",
      "Loss in epoch 292: [99.656265]\n",
      "Loss in epoch 293: [99.676376]\n",
      "Loss in epoch 294: [99.660416]\n",
      "Loss in epoch 295: [99.668434]\n",
      "Loss in epoch 296: [99.641518]\n",
      "Loss in epoch 297: [99.669937]\n",
      "Loss in epoch 298: [99.622177]\n",
      "Loss in epoch 299: [99.63916]\n",
      "Loss in epoch 300: [99.624405]\n",
      "Loss in epoch 301: [99.621925]\n",
      "Loss in epoch 302: [99.598495]\n",
      "Loss in epoch 303: [99.633873]\n",
      "Loss in epoch 304: [99.597595]\n",
      "Loss in epoch 305: [99.604454]\n",
      "Loss in epoch 306: [99.599411]\n",
      "Loss in epoch 307: [99.602028]\n",
      "Loss in epoch 308: [99.570885]\n",
      "Loss in epoch 309: [99.598618]\n",
      "Loss in epoch 310: [99.570328]\n",
      "Loss in epoch 311: [99.574181]\n",
      "Loss in epoch 312: [99.564919]\n",
      "Loss in epoch 313: [99.565323]\n",
      "Loss in epoch 314: [99.57341]\n",
      "Loss in epoch 315: [99.610626]\n",
      "Loss in epoch 316: [99.583359]\n",
      "Loss in epoch 317: [99.62352]\n",
      "Loss in epoch 318: [99.583511]\n",
      "Loss in epoch 319: [99.592628]\n",
      "Loss in epoch 320: [99.561371]\n",
      "Loss in epoch 321: [99.593704]\n",
      "Loss in epoch 322: [99.563988]\n",
      "Loss in epoch 323: [99.581345]\n",
      "Loss in epoch 324: [99.555412]\n",
      "Loss in epoch 325: [99.558739]\n",
      "Loss in epoch 326: [99.535172]\n",
      "Loss in epoch 327: [99.555458]\n",
      "Loss in epoch 328: [99.546326]\n",
      "Loss in epoch 329: [99.54631]\n",
      "Loss in epoch 330: [99.523827]\n",
      "Loss in epoch 331: [99.531815]\n",
      "Loss in epoch 332: [99.56102]\n",
      "Loss in epoch 333: [99.56739]\n",
      "Loss in epoch 334: [99.547432]\n",
      "Loss in epoch 335: [99.559029]\n",
      "Loss in epoch 336: [99.534309]\n",
      "Loss in epoch 337: [99.53051]\n",
      "Loss in epoch 338: [99.529686]\n",
      "Loss in epoch 339: [99.511185]\n",
      "Loss in epoch 340: [99.526314]\n",
      "Loss in epoch 341: [99.514]\n",
      "Loss in epoch 342: [99.537216]\n",
      "Loss in epoch 343: [99.526443]\n",
      "Loss in epoch 344: [99.551102]\n",
      "Loss in epoch 345: [99.526352]\n",
      "Loss in epoch 346: [99.55365]\n",
      "Loss in epoch 347: [99.493896]\n",
      "Loss in epoch 348: [99.528358]\n",
      "Loss in epoch 349: [99.505638]\n",
      "Loss in epoch 350: [99.515549]\n",
      "Loss in epoch 351: [99.509842]\n",
      "Loss in epoch 352: [99.512306]\n",
      "Loss in epoch 353: [99.541634]\n",
      "Loss in epoch 354: [99.514313]\n",
      "Loss in epoch 355: [99.529694]\n",
      "Loss in epoch 356: [99.507889]\n",
      "Loss in epoch 357: [99.526398]\n",
      "Loss in epoch 358: [99.495575]\n",
      "Loss in epoch 359: [99.501343]\n",
      "Loss in epoch 360: [99.49379]\n",
      "Loss in epoch 361: [99.478935]\n",
      "Loss in epoch 362: [99.514]\n",
      "Loss in epoch 363: [99.488983]\n",
      "Loss in epoch 364: [99.516937]\n",
      "Loss in epoch 365: [99.507622]\n",
      "Loss in epoch 366: [99.520844]\n",
      "Loss in epoch 367: [99.483826]\n",
      "Loss in epoch 368: [99.483192]\n",
      "Loss in epoch 369: [99.495384]\n",
      "Loss in epoch 370: [99.494102]\n",
      "Loss in epoch 371: [99.506149]\n",
      "Loss in epoch 372: [99.495743]\n",
      "Loss in epoch 373: [99.488182]\n",
      "Loss in epoch 374: [99.509193]\n",
      "Loss in epoch 375: [99.476242]\n",
      "Loss in epoch 376: [99.495514]\n",
      "Loss in epoch 377: [99.490433]\n",
      "Loss in epoch 378: [99.500893]\n",
      "Loss in epoch 379: [99.471321]\n",
      "Loss in epoch 380: [99.459671]\n",
      "Loss in epoch 381: [99.480156]\n",
      "Loss in epoch 382: [99.47126]\n",
      "Loss in epoch 383: [99.505676]\n",
      "Loss in epoch 384: [99.472473]\n",
      "Loss in epoch 385: [99.486465]\n",
      "Loss in epoch 386: [99.456375]\n",
      "Loss in epoch 387: [99.461563]\n",
      "Loss in epoch 388: [99.46579]\n",
      "Loss in epoch 389: [99.475571]\n",
      "Loss in epoch 390: [99.470886]\n",
      "Loss in epoch 391: [99.480362]\n",
      "Loss in epoch 392: [99.451782]\n",
      "Loss in epoch 393: [99.493439]\n",
      "Loss in epoch 394: [99.455826]\n",
      "Loss in epoch 395: [99.464043]\n",
      "Loss in epoch 396: [99.451714]\n",
      "Loss in epoch 397: [99.441261]\n",
      "Loss in epoch 398: [99.454071]\n",
      "Loss in epoch 399: [99.451431]\n",
      "Loss in epoch 400: [99.488892]\n",
      "Loss in epoch 401: [99.45488]\n",
      "Loss in epoch 402: [99.475296]\n",
      "Loss in epoch 403: [99.451035]\n",
      "Loss in epoch 404: [99.473663]\n",
      "Loss in epoch 405: [99.457863]\n",
      "Loss in epoch 406: [99.437988]\n",
      "Loss in epoch 407: [99.453506]\n",
      "Loss in epoch 408: [99.45517]\n",
      "Loss in epoch 409: [99.455124]\n",
      "Loss in epoch 410: [99.454201]\n",
      "Loss in epoch 411: [99.460739]\n",
      "Loss in epoch 412: [99.453285]\n",
      "Loss in epoch 413: [99.435585]\n",
      "Loss in epoch 414: [99.459]\n",
      "Loss in epoch 415: [99.442917]\n",
      "Loss in epoch 416: [99.428787]\n",
      "Loss in epoch 417: [99.446976]\n",
      "Loss in epoch 418: [99.433487]\n",
      "Loss in epoch 419: [99.453751]\n",
      "Loss in epoch 420: [99.425499]\n",
      "Loss in epoch 421: [99.459106]\n",
      "Loss in epoch 422: [99.424362]\n",
      "Loss in epoch 423: [99.437843]\n",
      "Loss in epoch 424: [99.415176]\n",
      "Loss in epoch 425: [99.419571]\n",
      "Loss in epoch 426: [99.432281]\n",
      "Loss in epoch 427: [99.452927]\n",
      "Loss in epoch 428: [99.426537]\n",
      "Loss in epoch 429: [99.449043]\n",
      "Loss in epoch 430: [99.40757]\n",
      "Loss in epoch 431: [99.423935]\n",
      "Loss in epoch 432: [99.409904]\n",
      "Loss in epoch 433: [99.414711]\n",
      "Loss in epoch 434: [99.419235]\n",
      "Loss in epoch 435: [99.421448]\n",
      "Loss in epoch 436: [99.427353]\n",
      "Loss in epoch 437: [99.418053]\n",
      "Loss in epoch 438: [99.448814]\n",
      "Loss in epoch 439: [99.412926]\n",
      "Loss in epoch 440: [99.441795]\n",
      "Loss in epoch 441: [99.407364]\n",
      "Loss in epoch 442: [99.390396]\n",
      "Loss in epoch 443: [99.425621]\n",
      "Loss in epoch 444: [99.423256]\n",
      "Loss in epoch 445: [99.417152]\n",
      "Loss in epoch 446: [99.422081]\n",
      "Loss in epoch 447: [99.409866]\n",
      "Loss in epoch 448: [99.404259]\n",
      "Loss in epoch 449: [99.417801]\n",
      "Loss in epoch 450: [99.411453]\n",
      "Loss in epoch 451: [99.406326]\n",
      "Loss in epoch 452: [99.402458]\n",
      "Loss in epoch 453: [99.39489]\n",
      "Loss in epoch 454: [99.394936]\n",
      "Loss in epoch 455: [99.421516]\n",
      "Loss in epoch 456: [99.394135]\n",
      "Loss in epoch 457: [99.413071]\n",
      "Loss in epoch 458: [99.382118]\n",
      "Loss in epoch 459: [99.398788]\n",
      "Loss in epoch 460: [99.376854]\n",
      "Loss in epoch 461: [99.391983]\n",
      "Loss in epoch 462: [99.389137]\n",
      "Loss in epoch 463: [99.406586]\n",
      "Loss in epoch 464: [99.380104]\n",
      "Loss in epoch 465: [99.421219]\n",
      "Loss in epoch 466: [99.381592]\n",
      "Loss in epoch 467: [99.376312]\n",
      "Loss in epoch 468: [99.375587]\n",
      "Loss in epoch 469: [99.37677]\n",
      "Loss in epoch 470: [99.367722]\n",
      "Loss in epoch 471: [99.396004]\n",
      "Loss in epoch 472: [99.401451]\n",
      "Loss in epoch 473: [99.381203]\n",
      "Loss in epoch 474: [99.4021]\n",
      "Loss in epoch 475: [99.371887]\n",
      "Loss in epoch 476: [99.405106]\n",
      "Loss in epoch 477: [99.37117]\n",
      "Loss in epoch 478: [99.374367]\n",
      "Loss in epoch 479: [99.372108]\n",
      "Loss in epoch 480: [99.375854]\n",
      "Loss in epoch 481: [99.403412]\n",
      "Loss in epoch 482: [99.382759]\n",
      "Loss in epoch 483: [99.371666]\n",
      "Loss in epoch 484: [99.389053]\n",
      "Loss in epoch 485: [99.364883]\n",
      "Loss in epoch 486: [99.367409]\n",
      "Loss in epoch 487: [99.397263]\n",
      "Loss in epoch 488: [99.35257]\n",
      "Loss in epoch 489: [99.35408]\n",
      "Loss in epoch 490: [99.37886]\n",
      "Loss in epoch 491: [99.374069]\n",
      "Loss in epoch 492: [99.350098]\n",
      "Loss in epoch 493: [99.408257]\n",
      "Loss in epoch 494: [99.355965]\n",
      "Loss in epoch 495: [99.350632]\n",
      "Loss in epoch 496: [99.354675]\n",
      "Loss in epoch 497: [99.350403]\n",
      "Loss in epoch 498: [99.341591]\n",
      "Loss in epoch 499: [99.409798]\n",
      "[1]\n",
      "Optimal Action Squence:[[ 1.          0.99470001]\n",
      " [ 1.          0.99299997]\n",
      " [ 1.          0.99129999]\n",
      " [ 1.          0.61489999]\n",
      " [ 1.         -0.61619997]\n",
      " [ 1.          0.99299997]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 0.0511      1.        ]\n",
      " [-0.034       1.        ]\n",
      " [-0.0067      0.02      ]\n",
      " [-0.60790002 -0.2872    ]]\n",
      "Best Cost: [-81.700737]\n",
      "The last state:[ 7.40257215  7.70356226]\n",
      "Rewards each time step:[[-16.        ]\n",
      " [-14.0053091 ]\n",
      " [-12.01228619]\n",
      " [-10.02093887]\n",
      " [ -8.40606213]\n",
      " [ -8.02222061]\n",
      " [ -6.02920532]\n",
      " [ -4.02920532]\n",
      " [ -2.02920532]\n",
      " [ -1.08029079]\n",
      " [ -0.04633045]\n",
      " [ -0.0196805 ]]\n",
      "Intermediate states:[[ 1.          0.99469066]\n",
      " [ 2.          1.98771358]\n",
      " [ 3.          2.97906113]\n",
      " [ 4.          3.59393764]\n",
      " [ 5.          2.97777939]\n",
      " [ 6.          3.97079444]\n",
      " [ 7.          4.97079468]\n",
      " [ 8.          5.97079468]\n",
      " [ 8.05108547  6.97079468]\n",
      " [ 8.01712513  7.97079468]\n",
      " [ 8.01042557  7.99074507]\n",
      " [ 7.40257215  7.70356226]]\n",
      "progress shape:(50, 12, 2)\n"
     ]
    }
   ],
   "source": [
    "rnn_inst.Optimize(500,show_progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
