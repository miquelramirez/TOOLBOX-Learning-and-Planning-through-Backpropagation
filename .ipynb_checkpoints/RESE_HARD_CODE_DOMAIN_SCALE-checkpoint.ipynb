{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "#Functional coding\n",
    "import functools\n",
    "from functools import partial\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Path..\n",
    "Datapath=\"DATA/Reservoir/Reservoir_Data.txt\"\n",
    "Labelpath=\"DATA/Reservoir/Reservoir_Label.txt\"\n",
    "Rewardpath=\"DATA/Reservoir/Reservoir_Reward.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given local path, find full path\n",
    "def PathFinder(path):\n",
    "    #python 2\n",
    "    #script_dir = os.path.dirname('__file__')\n",
    "    #fullpath = os.path.join(script_dir,path)\n",
    "    #python 3\n",
    "    fullpath=os.path.abspath(path)\n",
    "    print(fullpath)\n",
    "    return fullpath\n",
    "\n",
    "#Read Data for Deep Learning\n",
    "def ReadData(path):\n",
    "    fullpath=PathFinder(path)\n",
    "    return pd.read_csv(fullpath, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Reservoir/Reservoir_Data.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Reservoir/Reservoir_Label.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Reservoir/Reservoir_Reward.txt\n"
     ]
    }
   ],
   "source": [
    "S_A_pd = ReadData(Datapath)\n",
    "SP_pd = ReadData(Labelpath)\n",
    "R_pd = ReadData(Rewardpath)\n",
    "S_A_matrix=S_A_pd.as_matrix()\n",
    "SP_matrix=SP_pd.as_matrix()\n",
    "R_matrix=R_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_settings = {\n",
    "    \"max_cap\"          : [100,100,100,100,100,100,100,100,200,200,200,200,200,200,400,400,400,500,500,1000],\n",
    "    \"high_bound\"       : [80,80,80,80,80,80,80,80,180,180,180,180,180,180,380,380,380,480,480,980],\n",
    "    \"low_bound\"        : [20,20,20,20,20,20,20,20,30,30,30,30,30,30,40,40,40,60,60,100],\n",
    "    \"rain\"             : [5,5,5,5,5,5,5,5,10,10,10,10,10,10,20,20,20,30,30,40],\n",
    "    \"downstream\"       : [[1,9],[2,9],[3,10],[4,10],[5,11],[6,11],[7,12],[8,12],[9,15],[10,15],\\\n",
    "                          [11,16],[12,16],[13,17],[14,17],[15,18],[16,19],[17,19],[18,20],[19,20]],\n",
    "    \"downtosea\"        : [20],\n",
    "    \"biggestmaxcap\"    : 1000,\n",
    "    \"reservoirs\"    : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "    \"init_state\"       : [75,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50]\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RESERVOIR(object):\n",
    "    def __init__(self, \n",
    "                 batch_size,\n",
    "                 default_settings):\n",
    "        self.batch_size = batch_size\n",
    "        self.reservoirs = default_settings['reservoirs']\n",
    "        self.reservoir_num = len(default_settings['reservoirs'])\n",
    "        self.biggestmaxcap = tf.constant(default_settings[\"biggestmaxcap\"],dtype=tf.float32)\n",
    "        self.zero = tf.constant(0,shape=[self.batch_size,self.reservoir_num],dtype=tf.float32)\n",
    "        self._high_bounds(default_settings[\"high_bound\"])\n",
    "        self._low_bounds(default_settings[\"low_bound\"])\n",
    "        self._rains(default_settings[\"rain\"])\n",
    "        self._max_cap(default_settings[\"max_cap\"])\n",
    "        self._downstream(default_settings[\"downstream\"])\n",
    "        self._downtosea(default_settings[\"downtosea\"])\n",
    "        \n",
    "    def _max_cap(self, max_cap_list):\n",
    "        self.max_cap = tf.constant(max_cap_list,dtype=tf.float32)\n",
    "    \n",
    "    def _high_bounds(self, high_bound_list):\n",
    "        self.high_bound = tf.constant(high_bound_list,dtype=tf.float32)\n",
    "            \n",
    "    def _low_bounds(self, low_bound_list):\n",
    "        self.low_bound = tf.constant(low_bound_list,dtype=tf.float32)\n",
    "            \n",
    "    def _rains(self, rain_list):\n",
    "        self.rain = tf.constant(rain_list,dtype=tf.float32)\n",
    "        \n",
    "    def _downstream(self, downstream):\n",
    "        np_downstream = np.zeros((self.reservoir_num,self.reservoir_num))\n",
    "        for i in downstream:\n",
    "            m = self.reservoirs.index(i[0])\n",
    "            n = self.reservoirs.index(i[1])\n",
    "            np_downstream[m,n] = 1\n",
    "        self.downstream = tf.constant(np_downstream,dtype=tf.float32)\n",
    "        \n",
    "    def _downtosea(self, downtosea):\n",
    "        np_downtosea = np.zeros((self.reservoir_num,))\n",
    "        for i in downtosea:\n",
    "            m = self.reservoirs.index(i)\n",
    "            np_downtosea[m] = 1\n",
    "        self.downtosea =  tf.constant(np_downtosea,dtype=tf.float32)\n",
    "            \n",
    "    def MAXCAP(self):\n",
    "        return self.max_cap\n",
    "    \n",
    "    def HIGH_BOUND(self):\n",
    "        return self.high_bound\n",
    "    \n",
    "    def LOW_BOUND(self):\n",
    "        return self.low_bound\n",
    "    \n",
    "    def RAIN(self):\n",
    "        return self.rain\n",
    "    \n",
    "    def DOWNSTREAM(self):\n",
    "        return self.downstream\n",
    "    \n",
    "    def DOWNTOSEA(self):\n",
    "        return self.downtosea\n",
    "        \n",
    "    def BIGGESTMAXCAP(self):\n",
    "        return self.biggestmaxcap\n",
    "    \n",
    "    def Transition(self, states, actions):\n",
    "        previous_state = states\n",
    "        vaporated = 0.5*tf.sin(previous_state/self.BIGGESTMAXCAP())*previous_state\n",
    "        upstreamflow = tf.transpose(tf.matmul(tf.transpose(self.DOWNSTREAM()),tf.transpose(actions)))\n",
    "        new_state = previous_state + self.RAIN()-vaporated-actions+ upstreamflow                        \n",
    "        return new_state\n",
    "    \n",
    "    #Reward for Reservoir is computed on 'Next State'\n",
    "    def Reward(self, states):\n",
    "        new_rewards = tf.select(tf.logical_and(tf.greater_equal(states,self.LOW_BOUND()),tf.less_equal(states,self.HIGH_BOUND())),\\\n",
    "                                 self.zero,\\\n",
    "                                tf.select(tf.less(states,self.LOW_BOUND()),\\\n",
    "                                          -5*(self.LOW_BOUND()-states),\\\n",
    "                                         -100*(states-self.HIGH_BOUND()))\\\n",
    "                               )\n",
    "        new_rewards+=tf.abs(((self.HIGH_BOUND()+self.LOW_BOUND())/2.0)-states)*(-0.1)\n",
    "        return tf.reduce_sum(new_rewards,1,keep_dims=True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# States\n",
    "states = tf.placeholder(tf.float32,[10, 20],name=\"States\")\n",
    "\n",
    "# Actions\n",
    "actions = tf.placeholder(tf.float32,[10, 20],name=\"Actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RESERVOIRCell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, batch_size,default_settings):\n",
    "        self._num_state_units = len(default_settings[\"reservoirs\"])\n",
    "        self._num_reward_units = self._num_state_units +1\n",
    "        self.reservoir = RESERVOIR(batch_size,default_settings)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_state_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_reward_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        next_state =  self.reservoir.Transition(state, inputs)\n",
    "        reward = self.reservoir.Reward(next_state)   \n",
    "        return tf.concat(1,[reward,next_state]), next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    def __init__(self,\n",
    "                a, # Actions\n",
    "                num_step, # Number of RNN step, this is a fixed step RNN sequence, 12 for navigation\n",
    "                num_act, # Number of actions\n",
    "                batch_size, #Batch Size\n",
    "                learning_rate=0.1): \n",
    "        self.action = a\n",
    "        print(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_step = num_step\n",
    "        self.learning_rate = learning_rate\n",
    "        self._p_create_rnn_graph()\n",
    "        self._p_Q_loss()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _p_create_rnn_graph(self):\n",
    "        cell = RESERVOIRCell(self.batch_size, default_settings)\n",
    "        initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)+tf.constant([default_settings[\"init_state\"]],dtype=tf.float32)\n",
    "        print('action batch size:{0}'.format(array_ops.shape(self.action)[0]))\n",
    "        print('Initial_state shape:{0}'.format(initial_state))\n",
    "        rnn_outputs, state = tf.nn.dynamic_rnn(cell, self.action, dtype=tf.float32,initial_state=initial_state)\n",
    "        #need output intermediate states as well\n",
    "        concated = tf.concat(0,rnn_outputs)\n",
    "        print('concated shape:{0}'.format(concated.get_shape()))\n",
    "        something_unpacked =  tf.unpack(concated, axis=2)\n",
    "        self.outputs = tf.reshape(something_unpacked[0],[-1,self.num_step,1])\n",
    "        print(' self.outputs:{0}'.format(self.outputs.get_shape()))\n",
    "        self.intern_states = tf.pack([something_unpacked[i+1] for i in range(len(default_settings[\"reservoirs\"]))], axis=2)\n",
    "        self.last_state = state\n",
    "        self.pred = tf.reduce_sum(self.outputs,1)\n",
    "        self.average_pred = tf.reduce_mean(self.pred)\n",
    "        print(\"self.pred:{0}\".format(self.pred))\n",
    "            \n",
    "    def _p_create_loss(self):\n",
    "\n",
    "        objective = tf.reduce_mean(self.pred) \n",
    "        self.loss = -objective\n",
    "        print(self.loss.get_shape())\n",
    "        #self.loss = -objective\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def _p_Q_loss(self):\n",
    "        objective = tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "        for i in range(self.num_step):\n",
    "            Rt = self.outputs[:,i]\n",
    "            SumRj=tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "            #SumRk=tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "            if i<(self.num_step-1):\n",
    "                j = i+1\n",
    "                SumRj = tf.reduce_sum(self.outputs[:,j:],1)\n",
    "            #if i<(self.num_step-1):\n",
    "                #k= i+1\n",
    "                #SumRk = tf.reduce_sum(self.outputs[:,k:],1)\n",
    "            objective+=(Rt*SumRj+tf.square(Rt))*(self.num_step-i)/np.square(self.num_step)\n",
    "        self.loss = tf.log(tf.reduce_mean(tf.square(objective)))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def Optimize(self,epoch=100):       \n",
    "        Time_Target_List = [15,30,60,120,240,480,960]\n",
    "        Target = Time_Target_List[0]\n",
    "        counter = 0\n",
    "        new_loss = self.sess.run([self.average_pred])\n",
    "        print('Loss in epoch {0}: {1}'.format(\"Initial\", new_loss)) \n",
    "        print('Compile to backend complete!') \n",
    "        start = time.time()\n",
    "        while True:\n",
    "            training = self.sess.run([self.optimizer])\n",
    "            action_upperbound=self.sess.run(self.intern_states)\n",
    "            self.sess.run(tf.assign(self.action, tf.clip_by_value(self.action, 0, action_upperbound)))\n",
    "            end = time.time()\n",
    "            if end-start>=Target:\n",
    "                print('Time: {0}'.format(Target))\n",
    "                pred_list = self.sess.run(self.pred)\n",
    "                pred_list=np.sort(pred_list.flatten())[::-1]\n",
    "                pred_list=pred_list[:5]\n",
    "                pred_mean = np.mean(pred_list)\n",
    "                pred_std = np.std(pred_list)\n",
    "                print('Best Cost: {0}'.format(pred_list[0]))\n",
    "                print('MEAN: {0}, STD:{1}'.format(pred_mean,pred_std))\n",
    "                counter = counter+1\n",
    "                if counter == len(Time_Target_List):\n",
    "                    print(\"Done!\")\n",
    "                    break\n",
    "                else:\n",
    "                    Target = Time_Target_List[counter]\n",
    "#         minimum_costs_id=self.sess.run(tf.argmax(self.pred,0))\n",
    "#         print(minimum_costs_id)\n",
    "#         best_action = self.sess.run(self.action)[minimum_costs_id[0]]\n",
    "#         print('Optimal Action Squence:{0}'.format(best_action))\n",
    "#         pred_list = self.sess.run(self.pred)\n",
    "#         pred_list=np.sort(pred_list.flatten())[::-1]\n",
    "#         pred_list=pred_list[:5]\n",
    "#         pred_mean = np.mean(pred_list)\n",
    "#         pred_std = np.std(pred_list)\n",
    "#         print('Best Cost: {0}'.format(pred_list[0]))\n",
    "#         print('Sorted Costs:{0}'.format(pred_list))\n",
    "#         print('MEAN: {0}, STD:{1}'.format(pred_mean,pred_std))\n",
    "#         print('The last state:{0}'.format(self.sess.run(self.last_state)))\n",
    "#         print('The last state:{0}'.format(self.sess.run(self.last_state)[minimum_costs_id[0]]))\n",
    "#         print('Rewards each time step:{0}'.format(self.sess.run(self.outputs)[minimum_costs_id[0]]))\n",
    "#         print('Intermediate states:{0}'.format(self.sess.run(self.intern_states)[minimum_costs_id[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"action/read:0\", shape=(10, 120, 20), dtype=float32)\n",
      "action batch size:Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Initial_state shape:Tensor(\"add:0\", shape=(10, 20), dtype=float32)\n",
      "concated shape:(10, 120, 21)\n",
      " self.outputs:(10, 120, 1)\n",
      "self.pred:Tensor(\"Sum:0\", shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.truncated_normal(shape=[10,120,20],mean=0.0, stddev=0.5),name=\"action\")\n",
    "rnn_inst = ActionOptimizer(a, 120,20,10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch Initial: [-1634377.2]\n",
      "Compile to backend complete!\n",
      "Time: 15\n",
      "Best Cost: -1456042.875\n",
      "MEAN: -1462760.375, STD:3628.82836914\n",
      "Time: 30\n",
      "Best Cost: -1395868.5\n",
      "MEAN: -1402154.5, STD:3385.52758789\n",
      "Time: 60\n",
      "Best Cost: -9063.80175781\n",
      "MEAN: -9065.50390625, STD:1.68165886402\n",
      "Time: 120\n",
      "Best Cost: -3462.32104492\n",
      "MEAN: -3487.53979492, STD:12.7162065506\n",
      "Time: 240\n",
      "Best Cost: -2393.84277344\n",
      "MEAN: -2407.97119141, STD:11.6076364517\n",
      "Time: 480\n",
      "Best Cost: -2387.54394531\n",
      "MEAN: -2399.640625, STD:10.9062280655\n",
      "Time: 960\n",
      "Best Cost: -2394.15014648\n",
      "MEAN: -2402.01806641, STD:7.24342679977\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "rnn_inst.Optimize(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
