{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NAVI_HARD_CODE_DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "#Functional coding\n",
    "import functools\n",
    "from functools import partial\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Path..\n",
    "Datapath=\"DATA/Navigation/linear/Navigation_Data.txt\"\n",
    "Labelpath=\"DATA/Navigation/linear/Navigation_Label.txt\"\n",
    "Rewardpath=\"DATA/Navigation/linear/Navigation_Reward.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given local path, find full path\n",
    "def PathFinder(path):\n",
    "    #python 2\n",
    "    #script_dir = os.path.dirname('__file__')\n",
    "    #fullpath = os.path.join(script_dir,path)\n",
    "    #python 3\n",
    "    fullpath=os.path.abspath(path)\n",
    "    print(fullpath)\n",
    "    return fullpath\n",
    "\n",
    "#Read Data for Deep Learning\n",
    "def ReadData(path):\n",
    "    fullpath=PathFinder(path)\n",
    "    return pd.read_csv(fullpath, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Data.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Label.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Reward.txt\n"
     ]
    }
   ],
   "source": [
    "S_A_pd = ReadData(Datapath)\n",
    "SP_pd = ReadData(Labelpath)\n",
    "R_pd = ReadData(Rewardpath)\n",
    "S_A_matrix=S_A_pd.as_matrix()\n",
    "SP_matrix=SP_pd.as_matrix()\n",
    "R_matrix=R_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_settings = {\n",
    "    \"dims\"          : 2,\n",
    "    \"min_maze_bound\": tf.constant(0.0,dtype=tf.float32), \n",
    "    \"max_maze_bound\": tf.constant(10.0,dtype=tf.float32), \n",
    "    \"min_act_bound\": tf.constant(-1.0,dtype=tf.float32), \n",
    "    \"max_act_bound\": tf.constant(1.0,dtype=tf.float32), \n",
    "    \"goal\"    : tf.constant(8.0,dtype=tf.float32),\n",
    "    \"penalty\" : tf.constant(1000000.0,dtype=tf.float32),\n",
    "    \"centre\"  : tf.constant(5.0,dtype=tf.float32)\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NAVI(object):\n",
    "    def __init__(self, \n",
    "                 default_settings):\n",
    "        self.__dict__.update(default_settings)\n",
    "        self.zero = tf.constant(0,dtype=tf.float32)\n",
    "        self.two = tf.constant(2.0,dtype=tf.float32)\n",
    "        self.one = tf.constant(1.0,dtype=tf.float32)\n",
    "        self.onedsix = tf.constant(1.6,dtype=tf.float32)\n",
    "        self.onedtwo = tf.constant(1.2,dtype=tf.float32)\n",
    "        self.doteight = tf.constant(0.8,dtype=tf.float32)\n",
    "        self.dotfour = tf.constant(0.4,dtype=tf.float32)\n",
    "        self.dotoofive = tf.constant(0.005,dtype = tf.float32)\n",
    "    \n",
    "    def MINMAZEBOUND(self, dim):\n",
    "        return self.min_maze_bound\n",
    "    \n",
    "    def MAXMAZEBOUND(self, dim):\n",
    "        return self.max_maze_bound\n",
    "    \n",
    "    def MINACTIONBOUND(self, dim):\n",
    "        return self.min_act_bound\n",
    "    \n",
    "    def MAXACTIONBOUND(self, dim):\n",
    "        return self.max_act_bound\n",
    "    \n",
    "    def GOAL(self, dim):\n",
    "        return self.goal\n",
    "    \n",
    "    def CENTER(self, dim):\n",
    "        return self.centre\n",
    "    \n",
    "    def PENALTY(self):\n",
    "        return self.penalty\n",
    "    \n",
    "    def _transition(self, dim, states_packed, actions_packed):\n",
    "        \n",
    "        states = tf.unpack(states_packed)\n",
    "        actions = tf.unpack(actions_packed)\n",
    "        \n",
    "        previous_state = states[dim]\n",
    "        \n",
    "        #distance to centre Manhattan\n",
    "        distance = self.zero\n",
    "        for i in range(len(states)):\n",
    "            distance+=tf.abs(states[i]-self.CENTER(i))\n",
    "        \n",
    "        #scale factor\n",
    "        #scalefactor = tf.cond(distance<self.two, lambda: distance/self.two, lambda: self.one)\n",
    "        scalefactor = tf.cond(tf.logical_and(distance<=self.two, distance>self.onedsix),\n",
    "                        lambda: self.onedsix/self.two,\n",
    "                        lambda: tf.cond(tf.logical_and(distance<=self.onedsix, distance>self.onedtwo),\n",
    "                                lambda: self.onedtwo/self.two,\n",
    "                                lambda: tf.cond(tf.logical_and(distance<=self.onedtwo, distance>self.doteight),\n",
    "                                        lambda: self.doteight/self.two,\n",
    "                                        lambda: tf.cond(tf.logical_and(distance<=self.doteight, distance>self.dotfour),\n",
    "                                                lambda: self.dotfour/self.two,   \n",
    "                                                lambda: tf.cond(distance<=self.dotfour,\n",
    "                                                        lambda: self.dotoofive,\n",
    "                                                        lambda: self.one\n",
    "                                                        )      \n",
    "                                                       \n",
    "                                                )\n",
    "                                        )  \n",
    "                                )\n",
    "                        )\n",
    "        \n",
    "        #proposed location\n",
    "        proposedLoc = previous_state + actions[dim]*scalefactor\n",
    "        \n",
    "        #new state\n",
    "        new_state = tf.cond(tf.logical_and(proposedLoc <= self.MAXMAZEBOUND(dim), proposedLoc >= self.MINMAZEBOUND(dim)), \\\n",
    "                            lambda: proposedLoc, \\\n",
    "                            lambda: tf.cond(proposedLoc >self.MAXMAZEBOUND(dim), lambda:self.MAXMAZEBOUND(dim), lambda:self.MINMAZEBOUND(dim) ) \\\n",
    "                           )\n",
    "        \n",
    "        return new_state\n",
    "    \n",
    "    # For single data point\n",
    "    def _vector_trans(self, state_size, states_packed, actions_packed):\n",
    "        new_states=[]\n",
    "        for i in range(state_size):\n",
    "            new_states.append(self._transition(i,states_packed,actions_packed))\n",
    "        return tf.pack(new_states)\n",
    "    \n",
    "    def Transition(self, states, actions):\n",
    "        new_states = []\n",
    "        batch_size,state_size = states.get_shape()\n",
    "        states_list = tf.unpack(states)\n",
    "        actions_list = tf.unpack(actions)\n",
    "        for i in range(batch_size):\n",
    "            new_states.append(self._vector_trans(state_size,states_list[i],actions_list[i]))\n",
    "        return tf.pack(new_states)\n",
    "    \n",
    "    def _reward(self, state_size, states_packed, actions_packed):\n",
    "        reward = self.zero\n",
    "        states = tf.unpack(states_packed)\n",
    "        actions = tf.unpack(actions_packed)\n",
    "        \n",
    "        for i in range(state_size):\n",
    "            reward -= tf.abs(states[i]-self.GOAL(i))\n",
    "        return tf.pack([reward])\n",
    "    \n",
    "    def Reward(self, states,actions):\n",
    "        new_rewards = []\n",
    "        batch_size,state_size = states.get_shape()\n",
    "        states_list = tf.unpack(states)\n",
    "        actions_list = tf.unpack(actions)\n",
    "        for i in range(batch_size):\n",
    "            new_rewards.append(self._reward(state_size,states_list[i],actions_list[i]))\n",
    "        return tf.pack(new_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# States\n",
    "states = tf.placeholder(tf.float32,[30, 2],name=\"States\")\n",
    "\n",
    "# Actions\n",
    "actions = tf.placeholder(tf.float32,[30, 2],name=\"Actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "navi_inst = NAVI(default_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.2864389]\n",
      "[array([ 3.58680534,  4.28643894], dtype=float32)]\n",
      "[array([ 3.58680534,  4.28643894], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "states_list=tf.unpack(states)\n",
    "actions_list = tf.unpack(actions)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict={states:S_A_matrix[:30,2:], actions:S_A_matrix[:30,:2]}\n",
    "new_state = navi_inst._transition(1,states_list[8],actions_list[8])\n",
    "print(sess.run([new_state], feed_dict=feed_dict))\n",
    "print(sess.run([states_list[9]], feed_dict=feed_dict))\n",
    "print(sess.run([states_list[9]], feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rewards = navi_inst.Reward(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-16.        ],\n",
       "       [-15.0486908 ],\n",
       "       [-14.45474052],\n",
       "       [-14.18281746],\n",
       "       [-13.06755447],\n",
       "       [-13.33346748],\n",
       "       [-12.04419136],\n",
       "       [-10.35970879],\n",
       "       [ -9.259552  ],\n",
       "       [ -8.12675571],\n",
       "       [ -7.76328278],\n",
       "       [ -6.64727402],\n",
       "       [-16.        ],\n",
       "       [-16.        ],\n",
       "       [-15.79463196],\n",
       "       [-15.29909897],\n",
       "       [-15.39221764],\n",
       "       [-14.86650181],\n",
       "       [-13.72164154],\n",
       "       [-14.40450287],\n",
       "       [-14.44963074],\n",
       "       [-13.74181557],\n",
       "       [-13.46664429],\n",
       "       [-12.79545021],\n",
       "       [-16.        ],\n",
       "       [-14.33040237],\n",
       "       [-14.17589855],\n",
       "       [-12.88618851],\n",
       "       [-11.73449802],\n",
       "       [-10.57782745]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict={states:S_A_matrix[:30,2:], actions:S_A_matrix[:30,:2]}\n",
    "sess.run(new_rewards,feed_dict=feed_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAVICell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, default_settings):\n",
    "        self._num_state_units = 2\n",
    "        self._num_reward_units = 3\n",
    "        self.navi = NAVI(default_settings)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_state_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_reward_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        next_state =  self.navi.Transition(state, inputs)\n",
    "        reward = self.navi.Reward(state, inputs)      \n",
    "        return tf.concat(1,[reward,next_state]), next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    def __init__(self,\n",
    "                a, # Actions\n",
    "                num_step, # Number of RNN step, this is a fixed step RNN sequence, 12 for navigation\n",
    "                num_act, # Number of actions\n",
    "                batch_size, #Batch Size\n",
    "                learning_rate=0.01): \n",
    "        self.action = tf.reshape(a,[-1,num_step,num_act]) #Reshape rewards\n",
    "        print(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_step = num_step\n",
    "        self.learning_rate = learning_rate\n",
    "        self._p_create_rnn_graph()\n",
    "        self._p_create_loss()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _p_create_rnn_graph(self):\n",
    "        cell = NAVICell(default_settings)\n",
    "        initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        print('action batch size:{0}'.format(array_ops.shape(self.action)[0]))\n",
    "        print('Initial_state shape:{0}'.format(initial_state))\n",
    "        rnn_outputs, state = tf.nn.dynamic_rnn(cell, self.action, dtype=tf.float32,initial_state=initial_state)\n",
    "        #need output intermediate states as well\n",
    "        concated = tf.concat(0,rnn_outputs)\n",
    "        print('concated shape:{0}'.format(concated.get_shape()))\n",
    "        something_unpacked =  tf.unpack(concated, axis=2)\n",
    "        self.outputs = tf.reshape(something_unpacked[0],[-1,self.num_step,1])\n",
    "        print(' self.outputs:{0}'.format(self.outputs.get_shape()))\n",
    "        self.intern_states = tf.pack([something_unpacked[1],something_unpacked[2]], axis=2)\n",
    "        self.last_state = state\n",
    "        self.pred = tf.reduce_sum(self.outputs,1)\n",
    "        print(\"self.pred:{0}\".format(self.pred))\n",
    "            \n",
    "    def _p_create_loss(self):\n",
    "\n",
    "        objective = tf.reduce_mean(self.pred) \n",
    "        self.loss = -objective\n",
    "        print(self.loss.get_shape())\n",
    "        #self.loss = -objective\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def Optimize(self,epoch=100):\n",
    "        \n",
    "        new_loss = self.sess.run([self.loss])\n",
    "        print('Loss in epoch {0}: {1}'.format(\"Initial\", new_loss)) \n",
    "        for epoch in range(epoch):\n",
    "            training = self.sess.run([self.optimizer])\n",
    "            self.sess.run(tf.assign(a, tf.clip_by_value(a, -1, 1)))\n",
    "            if True:\n",
    "                new_loss = self.sess.run([self.loss])\n",
    "                print('Loss in epoch {0}: {1}'.format(epoch, new_loss))  \n",
    "        minimum_costs_id=self.sess.run(tf.argmax(self.pred,0))\n",
    "        print(minimum_costs_id)\n",
    "        best_action = np.round(self.sess.run(self.action)[minimum_costs_id[0]],4)\n",
    "        print('Optimal Action Squence:{0}'.format(best_action))\n",
    "        print('Best Cost: {0}'.format(self.sess.run(self.pred)[minimum_costs_id[0]]))\n",
    "        print('The last state:{0}'.format(self.sess.run(self.last_state)[minimum_costs_id[0]]))\n",
    "        print('Rewards each time step:{0}'.format(self.sess.run(self.outputs)[minimum_costs_id[0]]))\n",
    "        print('Intermediate states:{0}'.format(self.sess.run(self.intern_states)[minimum_costs_id[0]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(10, 12, 2), dtype=float32)\n",
      "action batch size:Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Initial_state shape:Tensor(\"zeros:0\", shape=(10, 2), dtype=float32)\n",
      "concated shape:(10, 12, 3)\n",
      " self.outputs:(10, 12, 1)\n",
      "self.pred:Tensor(\"Sum:0\", shape=(10, 1), dtype=float32)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.truncated_normal(shape=[240],mean=0.0, stddev=0.5),name=\"action\")\n",
    "rnn_inst = ActionOptimizer(a, 12,2,10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch Initial: [171.2999]\n",
      "Loss in epoch 0: [170.77216]\n",
      "Loss in epoch 1: [170.23331]\n",
      "Loss in epoch 2: [169.65598]\n",
      "Loss in epoch 3: [169.03743]\n",
      "Loss in epoch 4: [168.40915]\n",
      "Loss in epoch 5: [167.77609]\n",
      "Loss in epoch 6: [167.13425]\n",
      "Loss in epoch 7: [166.48372]\n",
      "Loss in epoch 8: [165.82272]\n",
      "Loss in epoch 9: [165.13849]\n",
      "Loss in epoch 10: [164.44594]\n",
      "Loss in epoch 11: [163.74521]\n",
      "Loss in epoch 12: [163.03648]\n",
      "Loss in epoch 13: [162.32117]\n",
      "Loss in epoch 14: [161.55301]\n",
      "Loss in epoch 15: [160.7933]\n",
      "Loss in epoch 16: [160.03813]\n",
      "Loss in epoch 17: [159.28128]\n",
      "Loss in epoch 18: [158.51956]\n",
      "Loss in epoch 19: [157.72458]\n",
      "Loss in epoch 20: [156.93062]\n",
      "Loss in epoch 21: [156.1312]\n",
      "Loss in epoch 22: [155.32829]\n",
      "Loss in epoch 23: [154.52007]\n",
      "Loss in epoch 24: [153.70039]\n",
      "Loss in epoch 25: [152.8085]\n",
      "Loss in epoch 26: [151.84177]\n",
      "Loss in epoch 27: [150.89474]\n",
      "Loss in epoch 28: [149.95572]\n",
      "Loss in epoch 29: [148.95035]\n",
      "Loss in epoch 30: [147.95227]\n",
      "Loss in epoch 31: [146.98181]\n",
      "Loss in epoch 32: [146.01529]\n",
      "Loss in epoch 33: [145.07675]\n",
      "Loss in epoch 34: [144.13611]\n",
      "Loss in epoch 35: [143.23828]\n",
      "Loss in epoch 36: [142.32864]\n",
      "Loss in epoch 37: [141.42802]\n",
      "Loss in epoch 38: [140.61368]\n",
      "Loss in epoch 39: [139.7496]\n",
      "Loss in epoch 40: [138.89034]\n",
      "Loss in epoch 41: [138.00551]\n",
      "Loss in epoch 42: [137.19872]\n",
      "Loss in epoch 43: [136.41064]\n",
      "Loss in epoch 44: [135.55159]\n",
      "Loss in epoch 45: [134.71065]\n",
      "Loss in epoch 46: [134.04622]\n",
      "Loss in epoch 47: [133.21355]\n",
      "Loss in epoch 48: [132.40031]\n",
      "Loss in epoch 49: [131.59879]\n",
      "Loss in epoch 50: [130.85352]\n",
      "Loss in epoch 51: [130.00012]\n",
      "Loss in epoch 52: [129.2021]\n",
      "Loss in epoch 53: [128.53546]\n",
      "Loss in epoch 54: [127.7168]\n",
      "Loss in epoch 55: [126.98381]\n",
      "Loss in epoch 56: [126.1815]\n",
      "Loss in epoch 57: [125.73528]\n",
      "Loss in epoch 58: [124.96886]\n",
      "Loss in epoch 59: [124.36525]\n",
      "Loss in epoch 60: [123.67836]\n",
      "Loss in epoch 61: [122.96157]\n",
      "Loss in epoch 62: [122.6013]\n",
      "Loss in epoch 63: [122.08678]\n",
      "Loss in epoch 64: [121.47832]\n",
      "Loss in epoch 65: [120.82092]\n",
      "Loss in epoch 66: [120.29643]\n",
      "Loss in epoch 67: [119.66978]\n",
      "Loss in epoch 68: [119.5183]\n",
      "Loss in epoch 69: [119.02455]\n",
      "Loss in epoch 70: [118.40422]\n",
      "Loss in epoch 71: [117.9901]\n",
      "Loss in epoch 72: [117.52114]\n",
      "Loss in epoch 73: [116.92114]\n",
      "Loss in epoch 74: [116.34444]\n",
      "Loss in epoch 75: [116.08429]\n",
      "Loss in epoch 76: [115.53606]\n",
      "Loss in epoch 77: [114.97439]\n",
      "Loss in epoch 78: [114.63351]\n",
      "Loss in epoch 79: [114.01733]\n",
      "Loss in epoch 80: [113.83795]\n",
      "Loss in epoch 81: [113.31294]\n",
      "Loss in epoch 82: [113.04317]\n",
      "Loss in epoch 83: [112.9931]\n",
      "Loss in epoch 84: [112.70084]\n",
      "Loss in epoch 85: [112.05629]\n",
      "Loss in epoch 86: [111.59715]\n",
      "Loss in epoch 87: [111.11943]\n",
      "Loss in epoch 88: [110.66721]\n",
      "Loss in epoch 89: [110.49723]\n",
      "Loss in epoch 90: [109.9192]\n",
      "Loss in epoch 91: [109.85045]\n",
      "Loss in epoch 92: [109.65782]\n",
      "Loss in epoch 93: [109.2429]\n",
      "Loss in epoch 94: [108.69208]\n",
      "Loss in epoch 95: [108.13558]\n",
      "Loss in epoch 96: [107.97805]\n",
      "Loss in epoch 97: [107.9893]\n",
      "Loss in epoch 98: [107.85384]\n",
      "Loss in epoch 99: [107.41785]\n",
      "Loss in epoch 100: [106.97909]\n",
      "Loss in epoch 101: [106.66924]\n",
      "Loss in epoch 102: [106.4894]\n",
      "Loss in epoch 103: [106.14048]\n",
      "Loss in epoch 104: [106.41866]\n",
      "Loss in epoch 105: [106.61847]\n",
      "Loss in epoch 106: [106.27763]\n",
      "Loss in epoch 107: [106.37953]\n",
      "Loss in epoch 108: [105.96834]\n",
      "Loss in epoch 109: [105.84402]\n",
      "Loss in epoch 110: [105.46658]\n",
      "Loss in epoch 111: [105.1832]\n",
      "Loss in epoch 112: [104.79359]\n",
      "Loss in epoch 113: [104.48308]\n",
      "Loss in epoch 114: [104.24075]\n",
      "Loss in epoch 115: [104.1859]\n",
      "Loss in epoch 116: [104.6814]\n",
      "Loss in epoch 117: [105.58083]\n",
      "Loss in epoch 118: [104.94468]\n",
      "Loss in epoch 119: [104.87634]\n",
      "Loss in epoch 120: [104.55058]\n",
      "Loss in epoch 121: [104.29607]\n",
      "Loss in epoch 122: [104.25961]\n",
      "Loss in epoch 123: [103.86068]\n",
      "Loss in epoch 124: [103.63326]\n",
      "Loss in epoch 125: [103.68781]\n",
      "Loss in epoch 126: [103.76094]\n",
      "Loss in epoch 127: [103.72856]\n",
      "Loss in epoch 128: [103.88926]\n",
      "Loss in epoch 129: [104.53162]\n",
      "Loss in epoch 130: [104.34592]\n",
      "Loss in epoch 131: [104.09132]\n",
      "Loss in epoch 132: [104.02083]\n",
      "Loss in epoch 133: [104.33398]\n",
      "Loss in epoch 134: [104.13493]\n",
      "Loss in epoch 135: [104.40834]\n",
      "Loss in epoch 136: [104.22638]\n",
      "Loss in epoch 137: [104.64956]\n",
      "Loss in epoch 138: [104.50597]\n",
      "Loss in epoch 139: [104.37444]\n",
      "Loss in epoch 140: [104.24204]\n",
      "Loss in epoch 141: [104.11277]\n",
      "Loss in epoch 142: [103.99148]\n",
      "Loss in epoch 143: [103.87862]\n",
      "Loss in epoch 144: [103.74358]\n",
      "Loss in epoch 145: [103.64514]\n",
      "Loss in epoch 146: [103.5425]\n",
      "Loss in epoch 147: [103.44232]\n",
      "Loss in epoch 148: [103.86049]\n",
      "Loss in epoch 149: [103.76347]\n",
      "Loss in epoch 150: [104.38063]\n",
      "Loss in epoch 151: [104.28239]\n",
      "Loss in epoch 152: [104.1841]\n",
      "Loss in epoch 153: [103.9706]\n",
      "Loss in epoch 154: [104.71328]\n",
      "Loss in epoch 155: [104.59009]\n",
      "Loss in epoch 156: [104.49016]\n",
      "Loss in epoch 157: [104.95625]\n",
      "Loss in epoch 158: [104.87073]\n",
      "Loss in epoch 159: [104.79027]\n",
      "Loss in epoch 160: [104.70149]\n",
      "Loss in epoch 161: [105.09122]\n",
      "Loss in epoch 162: [105.01161]\n",
      "Loss in epoch 163: [104.93748]\n",
      "Loss in epoch 164: [104.85753]\n",
      "Loss in epoch 165: [104.79079]\n",
      "Loss in epoch 166: [105.3228]\n",
      "Loss in epoch 167: [105.25763]\n",
      "Loss in epoch 168: [105.18723]\n",
      "Loss in epoch 169: [105.08639]\n",
      "Loss in epoch 170: [105.02065]\n",
      "Loss in epoch 171: [104.92554]\n",
      "Loss in epoch 172: [104.86546]\n",
      "Loss in epoch 173: [104.81535]\n",
      "Loss in epoch 174: [104.76229]\n",
      "Loss in epoch 175: [104.71101]\n",
      "Loss in epoch 176: [104.70062]\n",
      "Loss in epoch 177: [104.6618]\n",
      "Loss in epoch 178: [104.60885]\n",
      "Loss in epoch 179: [105.56221]\n",
      "Loss in epoch 180: [105.50816]\n",
      "Loss in epoch 181: [105.46505]\n",
      "Loss in epoch 182: [105.41583]\n",
      "Loss in epoch 183: [105.37286]\n",
      "Loss in epoch 184: [105.32697]\n",
      "Loss in epoch 185: [105.28691]\n",
      "Loss in epoch 186: [105.48335]\n",
      "Loss in epoch 187: [105.61674]\n",
      "Loss in epoch 188: [105.57882]\n",
      "Loss in epoch 189: [105.53825]\n",
      "Loss in epoch 190: [105.50022]\n",
      "Loss in epoch 191: [105.46391]\n",
      "Loss in epoch 192: [105.42483]\n",
      "Loss in epoch 193: [105.38509]\n",
      "Loss in epoch 194: [105.34628]\n",
      "Loss in epoch 195: [105.31081]\n",
      "Loss in epoch 196: [105.27232]\n",
      "Loss in epoch 197: [105.23508]\n",
      "Loss in epoch 198: [105.1979]\n",
      "Loss in epoch 199: [105.16274]\n",
      "Loss in epoch 200: [105.12804]\n",
      "Loss in epoch 201: [105.09566]\n",
      "Loss in epoch 202: [105.0623]\n",
      "Loss in epoch 203: [105.02809]\n",
      "Loss in epoch 204: [104.99766]\n",
      "Loss in epoch 205: [104.96494]\n",
      "Loss in epoch 206: [104.43105]\n",
      "Loss in epoch 207: [104.39748]\n",
      "Loss in epoch 208: [104.36289]\n",
      "Loss in epoch 209: [104.33144]\n",
      "Loss in epoch 210: [104.30141]\n",
      "Loss in epoch 211: [104.28206]\n",
      "Loss in epoch 212: [104.25877]\n",
      "Loss in epoch 213: [104.23833]\n",
      "Loss in epoch 214: [104.2112]\n",
      "Loss in epoch 215: [104.19228]\n",
      "Loss in epoch 216: [104.16968]\n",
      "Loss in epoch 217: [104.14922]\n",
      "Loss in epoch 218: [104.12242]\n",
      "Loss in epoch 219: [104.10284]\n",
      "Loss in epoch 220: [104.08144]\n",
      "Loss in epoch 221: [104.06042]\n",
      "Loss in epoch 222: [104.03426]\n",
      "Loss in epoch 223: [104.01378]\n",
      "Loss in epoch 224: [103.99387]\n",
      "Loss in epoch 225: [103.97168]\n",
      "Loss in epoch 226: [103.94906]\n",
      "Loss in epoch 227: [103.9259]\n",
      "Loss in epoch 228: [103.90963]\n",
      "Loss in epoch 229: [103.88652]\n",
      "Loss in epoch 230: [104.09816]\n",
      "Loss in epoch 231: [104.08717]\n",
      "Loss in epoch 232: [104.06804]\n",
      "Loss in epoch 233: [104.0557]\n",
      "Loss in epoch 234: [104.04124]\n",
      "Loss in epoch 235: [104.02401]\n",
      "Loss in epoch 236: [104.01003]\n",
      "Loss in epoch 237: [103.99786]\n",
      "Loss in epoch 238: [103.98059]\n",
      "Loss in epoch 239: [103.96716]\n",
      "Loss in epoch 240: [103.95358]\n",
      "Loss in epoch 241: [103.94923]\n",
      "Loss in epoch 242: [103.93888]\n",
      "Loss in epoch 243: [103.40907]\n",
      "Loss in epoch 244: [103.40092]\n",
      "Loss in epoch 245: [103.39256]\n",
      "Loss in epoch 246: [103.38078]\n",
      "Loss in epoch 247: [103.37478]\n",
      "Loss in epoch 248: [103.36226]\n",
      "Loss in epoch 249: [103.35347]\n",
      "Loss in epoch 250: [103.34904]\n",
      "Loss in epoch 251: [103.34056]\n",
      "Loss in epoch 252: [103.33403]\n",
      "Loss in epoch 253: [103.32109]\n",
      "Loss in epoch 254: [103.31895]\n",
      "Loss in epoch 255: [103.30684]\n",
      "Loss in epoch 256: [103.30103]\n",
      "Loss in epoch 257: [103.28922]\n",
      "Loss in epoch 258: [103.28705]\n",
      "Loss in epoch 259: [103.27476]\n",
      "Loss in epoch 260: [103.26882]\n",
      "Loss in epoch 261: [103.25696]\n",
      "Loss in epoch 262: [103.25487]\n",
      "Loss in epoch 263: [103.24229]\n",
      "Loss in epoch 264: [103.23631]\n",
      "Loss in epoch 265: [103.22642]\n",
      "Loss in epoch 266: [103.23134]\n",
      "Loss in epoch 267: [103.22523]\n",
      "Loss in epoch 268: [103.22614]\n",
      "Loss in epoch 269: [103.22084]\n",
      "Loss in epoch 270: [103.22606]\n",
      "Loss in epoch 271: [103.21968]\n",
      "Loss in epoch 272: [103.22072]\n",
      "Loss in epoch 273: [103.21529]\n",
      "Loss in epoch 274: [103.22084]\n",
      "Loss in epoch 275: [103.21413]\n",
      "Loss in epoch 276: [103.21542]\n",
      "Loss in epoch 277: [103.21295]\n",
      "Loss in epoch 278: [103.2126]\n",
      "Loss in epoch 279: [103.20951]\n",
      "Loss in epoch 280: [103.2114]\n",
      "Loss in epoch 281: [103.20504]\n",
      "Loss in epoch 282: [103.20714]\n",
      "Loss in epoch 283: [103.20447]\n",
      "Loss in epoch 284: [103.20598]\n",
      "Loss in epoch 285: [103.19978]\n",
      "Loss in epoch 286: [103.20172]\n",
      "Loss in epoch 287: [103.19946]\n",
      "Loss in epoch 288: [103.20062]\n",
      "Loss in epoch 289: [103.1946]\n",
      "Loss in epoch 290: [103.19639]\n",
      "Loss in epoch 291: [103.19248]\n",
      "Loss in epoch 292: [103.19524]\n",
      "Loss in epoch 293: [103.19139]\n",
      "Loss in epoch 294: [103.19118]\n",
      "Loss in epoch 295: [103.1871]\n",
      "Loss in epoch 296: [103.19028]\n",
      "Loss in epoch 297: [103.18606]\n",
      "Loss in epoch 298: [103.18607]\n",
      "Loss in epoch 299: [103.18179]\n",
      "Loss in epoch 300: [103.18536]\n",
      "Loss in epoch 301: [103.18076]\n",
      "Loss in epoch 302: [103.18097]\n",
      "Loss in epoch 303: [103.17649]\n",
      "Loss in epoch 304: [103.18047]\n",
      "Loss in epoch 305: [103.17546]\n",
      "Loss in epoch 306: [103.17589]\n",
      "Loss in epoch 307: [103.17122]\n",
      "Loss in epoch 308: [103.17556]\n",
      "Loss in epoch 309: [103.1702]\n",
      "Loss in epoch 310: [103.17081]\n",
      "Loss in epoch 311: [103.16594]\n",
      "Loss in epoch 312: [103.17068]\n",
      "Loss in epoch 313: [103.16494]\n",
      "Loss in epoch 314: [103.16575]\n",
      "Loss in epoch 315: [103.16071]\n",
      "Loss in epoch 316: [103.16581]\n",
      "Loss in epoch 317: [103.15971]\n",
      "Loss in epoch 318: [103.16069]\n",
      "Loss in epoch 319: [103.15547]\n",
      "Loss in epoch 320: [103.16093]\n",
      "Loss in epoch 321: [103.15446]\n",
      "Loss in epoch 322: [103.15758]\n",
      "Loss in epoch 323: [103.15542]\n",
      "Loss in epoch 324: [103.15701]\n",
      "Loss in epoch 325: [103.15403]\n",
      "Loss in epoch 326: [103.15797]\n",
      "Loss in epoch 327: [103.15176]\n",
      "Loss in epoch 328: [103.15585]\n",
      "Loss in epoch 329: [103.15326]\n",
      "Loss in epoch 330: [103.15682]\n",
      "Loss in epoch 331: [103.15074]\n",
      "Loss in epoch 332: [103.15468]\n",
      "Loss in epoch 333: [103.15245]\n",
      "Loss in epoch 334: [103.15565]\n",
      "Loss in epoch 335: [103.14972]\n",
      "Loss in epoch 336: [103.15347]\n",
      "Loss in epoch 337: [103.15163]\n",
      "Loss in epoch 338: [103.15446]\n",
      "Loss in epoch 339: [103.14868]\n",
      "Loss in epoch 340: [103.15242]\n",
      "Loss in epoch 341: [103.14854]\n",
      "Loss in epoch 342: [103.15348]\n",
      "Loss in epoch 343: [103.14954]\n",
      "Loss in epoch 344: [103.15134]\n",
      "Loss in epoch 345: [103.14725]\n",
      "Loss in epoch 346: [103.1526]\n",
      "Loss in epoch 347: [103.14824]\n",
      "Loss in epoch 348: [103.15028]\n",
      "Loss in epoch 349: [103.14597]\n",
      "Loss in epoch 350: [103.15172]\n",
      "Loss in epoch 351: [103.14697]\n",
      "Loss in epoch 352: [103.14922]\n",
      "Loss in epoch 353: [103.14471]\n",
      "Loss in epoch 354: [103.15086]\n",
      "Loss in epoch 355: [103.14571]\n",
      "Loss in epoch 356: [103.14816]\n",
      "Loss in epoch 357: [103.14346]\n",
      "Loss in epoch 358: [103.14998]\n",
      "Loss in epoch 359: [103.14445]\n",
      "Loss in epoch 360: [103.14709]\n",
      "Loss in epoch 361: [103.14221]\n",
      "Loss in epoch 362: [103.14909]\n",
      "Loss in epoch 363: [103.14321]\n",
      "Loss in epoch 364: [103.14604]\n",
      "Loss in epoch 365: [103.14097]\n",
      "Loss in epoch 366: [103.14822]\n",
      "Loss in epoch 367: [103.14197]\n",
      "Loss in epoch 368: [103.14498]\n",
      "Loss in epoch 369: [103.13973]\n",
      "Loss in epoch 370: [103.14735]\n",
      "Loss in epoch 371: [103.14072]\n",
      "Loss in epoch 372: [103.14408]\n",
      "Loss in epoch 373: [103.14169]\n",
      "Loss in epoch 374: [103.14326]\n",
      "Loss in epoch 375: [103.14053]\n",
      "Loss in epoch 376: [103.14424]\n",
      "Loss in epoch 377: [103.13814]\n",
      "Loss in epoch 378: [103.14211]\n",
      "Loss in epoch 379: [103.13977]\n",
      "Loss in epoch 380: [103.14309]\n",
      "Loss in epoch 381: [103.13712]\n",
      "Loss in epoch 382: [103.14093]\n",
      "Loss in epoch 383: [103.13896]\n",
      "Loss in epoch 384: [103.14192]\n",
      "Loss in epoch 385: [103.13609]\n",
      "Loss in epoch 386: [103.13985]\n",
      "Loss in epoch 387: [103.13601]\n",
      "Loss in epoch 388: [103.14085]\n",
      "Loss in epoch 389: [103.137]\n",
      "Loss in epoch 390: [103.13878]\n",
      "Loss in epoch 391: [103.13473]\n",
      "Loss in epoch 392: [103.13998]\n",
      "Loss in epoch 393: [103.13572]\n",
      "Loss in epoch 394: [103.13772]\n",
      "Loss in epoch 395: [103.13346]\n",
      "Loss in epoch 396: [103.13911]\n",
      "Loss in epoch 397: [103.13445]\n",
      "Loss in epoch 398: [103.13666]\n",
      "Loss in epoch 399: [103.1322]\n",
      "Loss in epoch 400: [103.13823]\n",
      "Loss in epoch 401: [103.13319]\n",
      "Loss in epoch 402: [103.1356]\n",
      "Loss in epoch 403: [103.13096]\n",
      "Loss in epoch 404: [103.13737]\n",
      "Loss in epoch 405: [103.13194]\n",
      "Loss in epoch 406: [103.13454]\n",
      "Loss in epoch 407: [103.12971]\n",
      "Loss in epoch 408: [103.13649]\n",
      "Loss in epoch 409: [103.1307]\n",
      "Loss in epoch 410: [103.13348]\n",
      "Loss in epoch 411: [103.12846]\n",
      "Loss in epoch 412: [103.13562]\n",
      "Loss in epoch 413: [103.12946]\n",
      "Loss in epoch 414: [103.13243]\n",
      "Loss in epoch 415: [103.12722]\n",
      "Loss in epoch 416: [103.13474]\n",
      "Loss in epoch 417: [103.12822]\n",
      "Loss in epoch 418: [103.13147]\n",
      "Loss in epoch 419: [103.12919]\n",
      "Loss in epoch 420: [103.13076]\n",
      "Loss in epoch 421: [103.12793]\n",
      "Loss in epoch 422: [103.13173]\n",
      "Loss in epoch 423: [103.12559]\n",
      "Loss in epoch 424: [103.12962]\n",
      "Loss in epoch 425: [103.12716]\n",
      "Loss in epoch 426: [103.13059]\n",
      "Loss in epoch 427: [103.12457]\n",
      "Loss in epoch 428: [103.12843]\n",
      "Loss in epoch 429: [103.12636]\n",
      "Loss in epoch 430: [103.12942]\n",
      "Loss in epoch 431: [103.12355]\n",
      "Loss in epoch 432: [103.12728]\n",
      "Loss in epoch 433: [103.12351]\n",
      "Loss in epoch 434: [103.12823]\n",
      "Loss in epoch 435: [103.1245]\n",
      "Loss in epoch 436: [103.12622]\n",
      "Loss in epoch 437: [103.12222]\n",
      "Loss in epoch 438: [103.12737]\n",
      "Loss in epoch 439: [103.12321]\n",
      "Loss in epoch 440: [103.12516]\n",
      "Loss in epoch 441: [103.12096]\n",
      "Loss in epoch 442: [103.1265]\n",
      "Loss in epoch 443: [103.12195]\n",
      "Loss in epoch 444: [103.1241]\n",
      "Loss in epoch 445: [103.11971]\n",
      "Loss in epoch 446: [103.12563]\n",
      "Loss in epoch 447: [103.12069]\n",
      "Loss in epoch 448: [103.12305]\n",
      "Loss in epoch 449: [103.11845]\n",
      "Loss in epoch 450: [103.12476]\n",
      "Loss in epoch 451: [103.11945]\n",
      "Loss in epoch 452: [103.12199]\n",
      "Loss in epoch 453: [103.1172]\n",
      "Loss in epoch 454: [103.12388]\n",
      "Loss in epoch 455: [103.1182]\n",
      "Loss in epoch 456: [103.12093]\n",
      "Loss in epoch 457: [103.11595]\n",
      "Loss in epoch 458: [103.12301]\n",
      "Loss in epoch 459: [103.11696]\n",
      "Loss in epoch 460: [103.11987]\n",
      "Loss in epoch 461: [103.11472]\n",
      "Loss in epoch 462: [103.12213]\n",
      "Loss in epoch 463: [103.11503]\n",
      "Loss in epoch 464: [103.12425]\n",
      "Loss in epoch 465: [103.11499]\n",
      "Loss in epoch 466: [103.1239]\n",
      "Loss in epoch 467: [103.11497]\n",
      "Loss in epoch 468: [103.12366]\n",
      "Loss in epoch 469: [103.11495]\n",
      "Loss in epoch 470: [103.12347]\n",
      "Loss in epoch 471: [103.11494]\n",
      "Loss in epoch 472: [103.12335]\n",
      "Loss in epoch 473: [103.11493]\n",
      "Loss in epoch 474: [103.12325]\n",
      "Loss in epoch 475: [103.11491]\n",
      "Loss in epoch 476: [103.12318]\n",
      "Loss in epoch 477: [103.11491]\n",
      "Loss in epoch 478: [103.12312]\n",
      "Loss in epoch 479: [103.11491]\n",
      "Loss in epoch 480: [103.12307]\n",
      "Loss in epoch 481: [103.11491]\n",
      "Loss in epoch 482: [103.12303]\n",
      "Loss in epoch 483: [103.11491]\n",
      "Loss in epoch 484: [103.12301]\n",
      "Loss in epoch 485: [103.11491]\n",
      "Loss in epoch 486: [103.12299]\n",
      "Loss in epoch 487: [103.11489]\n",
      "Loss in epoch 488: [103.12297]\n",
      "Loss in epoch 489: [103.11489]\n",
      "Loss in epoch 490: [103.12295]\n",
      "Loss in epoch 491: [103.11489]\n",
      "Loss in epoch 492: [103.12295]\n",
      "Loss in epoch 493: [103.11489]\n",
      "Loss in epoch 494: [103.12294]\n",
      "Loss in epoch 495: [103.11489]\n",
      "Loss in epoch 496: [103.12292]\n",
      "Loss in epoch 497: [103.11489]\n",
      "Loss in epoch 498: [103.12292]\n",
      "Loss in epoch 499: [103.11489]\n",
      "[5]\n",
      "Optimal Action Squence:[[ 1.         -0.20110001]\n",
      " [ 1.         -0.16949999]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 0.40720001  1.        ]\n",
      " [ 0.0057      1.        ]\n",
      " [-0.005       0.39660001]\n",
      " [ 0.44459999 -0.44800001]]\n",
      "Best Cost: [-90.4315033]\n",
      "The last state:[ 8.45248795  7.5485754 ]\n",
      "Rewards each time step:[[ -1.60000000e+01]\n",
      " [ -1.50000000e+01]\n",
      " [ -1.40000000e+01]\n",
      " [ -1.20000000e+01]\n",
      " [ -1.00000000e+01]\n",
      " [ -8.00000000e+00]\n",
      " [ -6.39999962e+00]\n",
      " [ -4.39999962e+00]\n",
      " [ -2.79999924e+00]\n",
      " [ -1.40723801e+00]\n",
      " [ -4.12953377e-01]\n",
      " [ -1.13134384e-02]]\n",
      "Intermediate states:[[ 1.          0.        ]\n",
      " [ 2.          0.        ]\n",
      " [ 3.          1.        ]\n",
      " [ 4.          2.        ]\n",
      " [ 5.          3.        ]\n",
      " [ 5.80000019  3.79999995]\n",
      " [ 6.80000019  4.80000019]\n",
      " [ 7.60000038  5.60000038]\n",
      " [ 8.00723839  6.60000038]\n",
      " [ 8.01295376  7.60000038]\n",
      " [ 8.00792217  7.99660873]\n",
      " [ 8.45248795  7.5485754 ]]\n"
     ]
    }
   ],
   "source": [
    "rnn_inst.Optimize(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
