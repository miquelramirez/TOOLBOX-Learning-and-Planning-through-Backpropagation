{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NAVI_HARD_CODE_DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "#Functional coding\n",
    "import functools\n",
    "from functools import partial\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Path..\n",
    "Datapath=\"DATA/Navigation/Navigation_Data.txt\"\n",
    "Labelpath=\"DATA/Navigation/Navigation_Label.txt\"\n",
    "Rewardpath=\"DATA/Navigation/Navigation_Reward.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given local path, find full path\n",
    "def PathFinder(path):\n",
    "    #python 2\n",
    "    #script_dir = os.path.dirname('__file__')\n",
    "    #fullpath = os.path.join(script_dir,path)\n",
    "    #python 3\n",
    "    fullpath=os.path.abspath(path)\n",
    "    print(fullpath)\n",
    "    return fullpath\n",
    "\n",
    "#Read Data for Deep Learning\n",
    "def ReadData(path):\n",
    "    fullpath=PathFinder(path)\n",
    "    return pd.read_csv(fullpath, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/Navigation_Data.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/Navigation_Label.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/Navigation_Reward.txt\n"
     ]
    }
   ],
   "source": [
    "S_A_pd = ReadData(Datapath)\n",
    "SP_pd = ReadData(Labelpath)\n",
    "R_pd = ReadData(Rewardpath)\n",
    "S_A_matrix=S_A_pd.as_matrix()\n",
    "SP_matrix=SP_pd.as_matrix()\n",
    "R_matrix=R_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_settings = {\n",
    "    \"dims\"          : 2,\n",
    "    \"min_maze_bound\": tf.constant(0.0,dtype=tf.float32), \n",
    "    \"max_maze_bound\": tf.constant(10.0,dtype=tf.float32), \n",
    "    \"min_act_bound\": tf.constant(-1.0,dtype=tf.float32), \n",
    "    \"max_act_bound\": tf.constant(1.0,dtype=tf.float32), \n",
    "    \"goal\"    : tf.constant(8.0,dtype=tf.float32),\n",
    "    \"penalty\" : tf.constant(1000000.0,dtype=tf.float32),\n",
    "    \"centre\"  : tf.constant(5.0,dtype=tf.float32)\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAVI(object):\n",
    "    def __init__(self, \n",
    "                 default_settings):\n",
    "        self.__dict__.update(default_settings)\n",
    "        self.zero = tf.constant(0,dtype=tf.float32)\n",
    "        self.two = tf.constant(2.0,dtype=tf.float32)\n",
    "        self.one = tf.constant(1.0,dtype=tf.float32)\n",
    "        self.lessone = tf.constant(0.99,dtype=tf.float32)\n",
    "    \n",
    "    def MINMAZEBOUND(self, dim):\n",
    "        return self.min_maze_bound\n",
    "    \n",
    "    def MAXMAZEBOUND(self, dim):\n",
    "        return self.max_maze_bound\n",
    "    \n",
    "    def MINACTIONBOUND(self, dim):\n",
    "        return self.min_act_bound\n",
    "    \n",
    "    def MAXACTIONBOUND(self, dim):\n",
    "        return self.max_act_bound\n",
    "    \n",
    "    def GOAL(self, dim):\n",
    "        return self.goal\n",
    "    \n",
    "    def CENTER(self, dim):\n",
    "        return self.centre\n",
    "    \n",
    "    def PENALTY(self):\n",
    "        return self.penalty\n",
    "    \n",
    "    def _transition(self, dim, states_packed, actions_packed):\n",
    "        \n",
    "        #distance to centre Manhattan\n",
    "        #distance = tf.abs(previous_state-self.CENTER(dim))\n",
    "        distance = tf.sqrt(tf.reduce_sum(tf.square(states_packed-tf.pack([self.CENTER(i) for i in range(self.dims)]))))\n",
    "        states = tf.unpack(states_packed)\n",
    "        actions = tf.unpack(actions_packed)\n",
    "        \n",
    "        previous_state = states[dim]\n",
    "        \n",
    "        #scale factor\n",
    "        scalefactor = self.two/(self.one+tf.exp(-self.two*distance))-self.lessone\n",
    "        \n",
    "        #proposed location\n",
    "        proposedLoc = previous_state + actions[dim]*scalefactor\n",
    "        \n",
    "        #new state\n",
    "        new_state = tf.cond(tf.logical_and(proposedLoc <= self.MAXMAZEBOUND(dim), proposedLoc >= self.MINMAZEBOUND(dim)), \\\n",
    "                            lambda: proposedLoc, \\\n",
    "                            lambda: tf.cond(proposedLoc >self.MAXMAZEBOUND(dim), lambda:self.MAXMAZEBOUND(dim), lambda:self.MINMAZEBOUND(dim) ) \\\n",
    "                           )\n",
    "        \n",
    "        return new_state\n",
    "    \n",
    "    # For single data point\n",
    "    def _vector_trans(self, state_size, states_packed, actions_packed):\n",
    "        new_states=[]\n",
    "        for i in range(state_size):\n",
    "            new_states.append(self._transition(i,states_packed,actions_packed))\n",
    "        return tf.pack(new_states)\n",
    "    \n",
    "    def Transition(self, states, actions):\n",
    "        new_states = []\n",
    "        batch_size,state_size = states.get_shape()\n",
    "        states_list = tf.unpack(states)\n",
    "        actions_list = tf.unpack(actions)\n",
    "        for i in range(batch_size):\n",
    "            new_states.append(self._vector_trans(state_size,states_list[i],actions_list[i]))\n",
    "        return tf.pack(new_states)\n",
    "    \n",
    "    def _reward(self, state_size, states_packed, actions_packed):\n",
    "        reward = self.zero\n",
    "        states = tf.unpack(states_packed)\n",
    "        actions = tf.unpack(actions_packed)\n",
    "        \n",
    "        for i in range(state_size):\n",
    "            reward -= tf.abs(states[i]-self.GOAL(i))\n",
    "        return tf.pack([reward])\n",
    "    \n",
    "    def Reward(self, states,actions):\n",
    "        new_rewards = []\n",
    "        batch_size,state_size = states.get_shape()\n",
    "        states_list = tf.unpack(states)\n",
    "        actions_list = tf.unpack(actions)\n",
    "        for i in range(batch_size):\n",
    "            new_rewards.append(self._reward(state_size,states_list[i],actions_list[i]))\n",
    "        return tf.pack(new_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# States\n",
    "states = tf.placeholder(tf.float32,[10, 2],name=\"States\")\n",
    "\n",
    "# Actions\n",
    "actions = tf.placeholder(tf.float32,[10, 2],name=\"Actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "navi_inst = NAVI(default_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n",
      "[array([ 0.5490576,  0.       ], dtype=float32)]\n",
      "[array([ 0.5490576,  0.       ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "states_list=tf.unpack(states)\n",
    "actions_list = tf.unpack(actions)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict={states:S_A_matrix[:10,2:], actions:S_A_matrix[:10,:2]}\n",
    "new_state = navi_inst._transition(1,states_list[0],actions_list[0])\n",
    "print(sess.run([new_state], feed_dict=feed_dict))\n",
    "print(sess.run([states_list[1]], feed_dict=feed_dict))\n",
    "print(sess.run([states_list[1]], feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rewards = navi_inst.Reward(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-16.        ],\n",
       "       [-15.45094299],\n",
       "       [-15.34755898],\n",
       "       [-14.92229462],\n",
       "       [-13.87710953],\n",
       "       [-12.88495827],\n",
       "       [-11.48205948],\n",
       "       [-13.36318207],\n",
       "       [-13.89812565],\n",
       "       [-12.66272354]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict={states:S_A_matrix[:10,2:], actions:S_A_matrix[:10,:2]}\n",
    "sess.run(new_rewards,feed_dict=feed_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAVICell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, default_settings):\n",
    "        self._num_state_units = 2\n",
    "        self._num_reward_units = 3\n",
    "        self.navi = NAVI(default_settings)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_state_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_reward_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        next_state =  self.navi.Transition(state, inputs)\n",
    "        reward = self.navi.Reward(state, inputs)      \n",
    "        return tf.concat(1,[reward,next_state]), next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    def __init__(self,\n",
    "                a, # Actions\n",
    "                num_step, # Number of RNN step, this is a fixed step RNN sequence, 12 for navigation\n",
    "                num_act, # Number of actions\n",
    "                batch_size, #Batch Size\n",
    "                learning_rate=0.01): \n",
    "        self.action = tf.reshape(a,[-1,num_step,num_act]) #Reshape rewards\n",
    "        print(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_step = num_step\n",
    "        self.learning_rate = learning_rate\n",
    "        self._p_create_rnn_graph()\n",
    "        self._p_create_loss()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _p_create_rnn_graph(self):\n",
    "        cell = NAVICell(default_settings)\n",
    "        initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        print('action batch size:{0}'.format(array_ops.shape(self.action)[0]))\n",
    "        print('Initial_state shape:{0}'.format(initial_state))\n",
    "        rnn_outputs, state = tf.nn.dynamic_rnn(cell, self.action, dtype=tf.float32,initial_state=initial_state)\n",
    "        #need output intermediate states as well\n",
    "        concated = tf.concat(0,rnn_outputs)\n",
    "        print('concated shape:{0}'.format(concated.get_shape()))\n",
    "        something_unpacked =  tf.unpack(concated, axis=2)\n",
    "        self.outputs = tf.reshape(something_unpacked[0],[-1,self.num_step,1])\n",
    "        print(' self.outputs:{0}'.format(self.outputs.get_shape()))\n",
    "        self.intern_states = tf.pack([something_unpacked[1],something_unpacked[2]], axis=2)\n",
    "        self.last_state = state\n",
    "        self.pred = tf.reduce_sum(self.outputs,1)\n",
    "        print(\"self.pred:{0}\".format(self.pred))\n",
    "            \n",
    "    def _p_create_loss(self):\n",
    "\n",
    "        objective = tf.reduce_mean(self.pred) \n",
    "        self.loss = -objective\n",
    "        print(self.loss.get_shape())\n",
    "        #self.loss = -objective\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def Optimize(self,epoch=100):\n",
    "        \n",
    "        new_loss = self.sess.run([self.loss])\n",
    "        print('Loss in epoch {0}: {1}'.format(\"Initial\", new_loss)) \n",
    "        for epoch in range(epoch):\n",
    "            training = self.sess.run([self.optimizer])\n",
    "            self.sess.run(tf.assign(a, tf.clip_by_value(a, -1, 1)))\n",
    "            if True:\n",
    "                new_loss = self.sess.run([self.loss])\n",
    "                print('Loss in epoch {0}: {1}'.format(epoch, new_loss))  \n",
    "        minimum_costs_id=self.sess.run(tf.argmax(self.pred,0))\n",
    "        print(minimum_costs_id)\n",
    "        best_action = np.round(self.sess.run(self.action)[minimum_costs_id[0]],4)\n",
    "        print('Optimal Action Squence:{0}'.format(best_action))\n",
    "        print('The last state:{0}'.format(self.sess.run(self.last_state)[minimum_costs_id[0]]))\n",
    "        print('Rewards each time step:{0}'.format(self.sess.run(self.outputs)[minimum_costs_id[0]]))\n",
    "        print('Intermediate states:{0}'.format(self.sess.run(self.intern_states)[minimum_costs_id[0]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(10, 12, 2), dtype=float32)\n",
      "action batch size:Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Initial_state shape:Tensor(\"zeros:0\", shape=(10, 2), dtype=float32)\n",
      "concated shape:(10, 12, 3)\n",
      " self.outputs:(10, 12, 1)\n",
      "self.pred:Tensor(\"Sum_1:0\", shape=(10, 1), dtype=float32)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.truncated_normal(shape=[240],mean=0.0, stddev=0.5),name=\"action\")\n",
    "rnn_inst = ActionOptimizer(a, 12,2,10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch Initial: [180.41524]\n",
      "Loss in epoch 0: [180.08914]\n",
      "Loss in epoch 1: [179.75607]\n",
      "Loss in epoch 2: [179.38736]\n",
      "Loss in epoch 3: [179.00287]\n",
      "Loss in epoch 4: [178.60275]\n",
      "Loss in epoch 5: [178.19333]\n",
      "Loss in epoch 6: [177.76956]\n",
      "Loss in epoch 7: [177.27542]\n",
      "Loss in epoch 8: [176.7803]\n",
      "Loss in epoch 9: [176.27917]\n",
      "Loss in epoch 10: [175.76785]\n",
      "Loss in epoch 11: [175.21066]\n",
      "Loss in epoch 12: [174.60406]\n",
      "Loss in epoch 13: [173.95772]\n",
      "Loss in epoch 14: [173.30959]\n",
      "Loss in epoch 15: [172.65219]\n",
      "Loss in epoch 16: [171.96379]\n",
      "Loss in epoch 17: [171.25711]\n",
      "Loss in epoch 18: [170.54021]\n",
      "Loss in epoch 19: [169.80936]\n",
      "Loss in epoch 20: [169.07755]\n",
      "Loss in epoch 21: [168.3363]\n",
      "Loss in epoch 22: [167.56424]\n",
      "Loss in epoch 23: [166.8056]\n",
      "Loss in epoch 24: [166.04466]\n",
      "Loss in epoch 25: [165.23904]\n",
      "Loss in epoch 26: [164.44693]\n",
      "Loss in epoch 27: [163.66357]\n",
      "Loss in epoch 28: [162.88058]\n",
      "Loss in epoch 29: [162.07864]\n",
      "Loss in epoch 30: [161.27948]\n",
      "Loss in epoch 31: [160.48137]\n",
      "Loss in epoch 32: [159.67453]\n",
      "Loss in epoch 33: [158.84746]\n",
      "Loss in epoch 34: [158.00938]\n",
      "Loss in epoch 35: [157.10963]\n",
      "Loss in epoch 36: [156.22792]\n",
      "Loss in epoch 37: [155.34407]\n",
      "Loss in epoch 38: [154.4686]\n",
      "Loss in epoch 39: [153.59924]\n",
      "Loss in epoch 40: [152.73413]\n",
      "Loss in epoch 41: [151.88016]\n",
      "Loss in epoch 42: [151.01741]\n",
      "Loss in epoch 43: [150.05344]\n",
      "Loss in epoch 44: [149.09741]\n",
      "Loss in epoch 45: [148.07822]\n",
      "Loss in epoch 46: [147.09909]\n",
      "Loss in epoch 47: [146.11868]\n",
      "Loss in epoch 48: [145.07404]\n",
      "Loss in epoch 49: [144.0851]\n",
      "Loss in epoch 50: [143.1311]\n",
      "Loss in epoch 51: [142.17619]\n",
      "Loss in epoch 52: [141.24121]\n",
      "Loss in epoch 53: [140.32034]\n",
      "Loss in epoch 54: [139.4157]\n",
      "Loss in epoch 55: [138.52377]\n",
      "Loss in epoch 56: [137.64355]\n",
      "Loss in epoch 57: [136.77608]\n",
      "Loss in epoch 58: [135.91898]\n",
      "Loss in epoch 59: [135.09488]\n",
      "Loss in epoch 60: [134.30029]\n",
      "Loss in epoch 61: [133.51985]\n",
      "Loss in epoch 62: [132.74872]\n",
      "Loss in epoch 63: [131.98752]\n",
      "Loss in epoch 64: [131.25516]\n",
      "Loss in epoch 65: [130.50757]\n",
      "Loss in epoch 66: [129.69421]\n",
      "Loss in epoch 67: [128.91814]\n",
      "Loss in epoch 68: [128.16006]\n",
      "Loss in epoch 69: [127.37109]\n",
      "Loss in epoch 70: [126.62276]\n",
      "Loss in epoch 71: [125.9016]\n",
      "Loss in epoch 72: [125.19997]\n",
      "Loss in epoch 73: [124.52278]\n",
      "Loss in epoch 74: [123.8606]\n",
      "Loss in epoch 75: [123.20886]\n",
      "Loss in epoch 76: [122.4801]\n",
      "Loss in epoch 77: [121.76904]\n",
      "Loss in epoch 78: [121.0827]\n",
      "Loss in epoch 79: [120.40486]\n",
      "Loss in epoch 80: [119.72456]\n",
      "Loss in epoch 81: [119.0032]\n",
      "Loss in epoch 82: [118.30032]\n",
      "Loss in epoch 83: [117.60658]\n",
      "Loss in epoch 84: [116.9196]\n",
      "Loss in epoch 85: [116.24398]\n",
      "Loss in epoch 86: [115.58879]\n",
      "Loss in epoch 87: [114.94269]\n",
      "Loss in epoch 88: [114.29824]\n",
      "Loss in epoch 89: [113.66606]\n",
      "Loss in epoch 90: [113.04755]\n",
      "Loss in epoch 91: [112.45012]\n",
      "Loss in epoch 92: [111.85738]\n",
      "Loss in epoch 93: [111.27422]\n",
      "Loss in epoch 94: [110.70705]\n",
      "Loss in epoch 95: [110.14158]\n",
      "Loss in epoch 96: [109.57471]\n",
      "Loss in epoch 97: [109.0155]\n",
      "Loss in epoch 98: [108.46311]\n",
      "Loss in epoch 99: [107.92378]\n",
      "Loss in epoch 100: [107.3878]\n",
      "Loss in epoch 101: [106.87016]\n",
      "Loss in epoch 102: [106.36954]\n",
      "Loss in epoch 103: [105.88286]\n",
      "Loss in epoch 104: [105.40778]\n",
      "Loss in epoch 105: [104.97383]\n",
      "Loss in epoch 106: [104.54995]\n",
      "Loss in epoch 107: [104.12473]\n",
      "Loss in epoch 108: [103.60913]\n",
      "Loss in epoch 109: [103.13273]\n",
      "Loss in epoch 110: [102.67776]\n",
      "Loss in epoch 111: [102.23445]\n",
      "Loss in epoch 112: [101.80055]\n",
      "Loss in epoch 113: [101.38104]\n",
      "Loss in epoch 114: [100.98003]\n",
      "Loss in epoch 115: [100.57724]\n",
      "Loss in epoch 116: [100.19621]\n",
      "Loss in epoch 117: [99.816444]\n",
      "Loss in epoch 118: [99.436905]\n",
      "Loss in epoch 119: [99.063828]\n",
      "Loss in epoch 120: [98.704903]\n",
      "Loss in epoch 121: [98.338257]\n",
      "Loss in epoch 122: [97.98613]\n",
      "Loss in epoch 123: [97.67942]\n",
      "Loss in epoch 124: [97.363426]\n",
      "Loss in epoch 125: [97.0606]\n",
      "Loss in epoch 126: [96.749649]\n",
      "Loss in epoch 127: [96.447464]\n",
      "Loss in epoch 128: [96.161728]\n",
      "Loss in epoch 129: [95.89608]\n",
      "Loss in epoch 130: [95.637901]\n",
      "Loss in epoch 131: [95.393616]\n",
      "Loss in epoch 132: [95.150246]\n",
      "Loss in epoch 133: [94.920082]\n",
      "Loss in epoch 134: [94.693161]\n",
      "Loss in epoch 135: [94.47493]\n",
      "Loss in epoch 136: [94.23716]\n",
      "Loss in epoch 137: [94.012794]\n",
      "Loss in epoch 138: [93.7957]\n",
      "Loss in epoch 139: [93.574036]\n",
      "Loss in epoch 140: [93.350624]\n",
      "Loss in epoch 141: [93.13665]\n",
      "Loss in epoch 142: [92.927055]\n",
      "Loss in epoch 143: [92.716446]\n",
      "Loss in epoch 144: [92.509705]\n",
      "Loss in epoch 145: [92.314804]\n",
      "Loss in epoch 146: [92.106972]\n",
      "Loss in epoch 147: [91.89695]\n",
      "Loss in epoch 148: [91.697113]\n",
      "Loss in epoch 149: [91.482666]\n",
      "Loss in epoch 150: [91.279991]\n",
      "Loss in epoch 151: [91.084854]\n",
      "Loss in epoch 152: [90.909668]\n",
      "Loss in epoch 153: [90.734322]\n",
      "Loss in epoch 154: [90.558884]\n",
      "Loss in epoch 155: [90.385681]\n",
      "Loss in epoch 156: [90.226357]\n",
      "Loss in epoch 157: [90.058701]\n",
      "Loss in epoch 158: [89.885399]\n",
      "Loss in epoch 159: [89.720444]\n",
      "Loss in epoch 160: [89.574661]\n",
      "Loss in epoch 161: [89.457802]\n",
      "Loss in epoch 162: [89.328117]\n",
      "Loss in epoch 163: [89.193634]\n",
      "Loss in epoch 164: [89.088211]\n",
      "Loss in epoch 165: [88.981522]\n",
      "Loss in epoch 166: [88.850937]\n",
      "Loss in epoch 167: [88.737488]\n",
      "Loss in epoch 168: [88.645935]\n",
      "Loss in epoch 169: [88.529968]\n",
      "Loss in epoch 170: [88.426956]\n",
      "Loss in epoch 171: [88.324142]\n",
      "Loss in epoch 172: [88.224396]\n",
      "Loss in epoch 173: [88.112289]\n",
      "Loss in epoch 174: [88.017189]\n",
      "Loss in epoch 175: [87.905136]\n",
      "Loss in epoch 176: [87.810349]\n",
      "Loss in epoch 177: [87.725967]\n",
      "Loss in epoch 178: [87.64048]\n",
      "Loss in epoch 179: [87.538712]\n",
      "Loss in epoch 180: [87.460663]\n",
      "Loss in epoch 181: [87.355881]\n",
      "Loss in epoch 182: [87.272079]\n",
      "Loss in epoch 183: [87.188812]\n",
      "Loss in epoch 184: [87.119644]\n",
      "Loss in epoch 185: [87.027489]\n",
      "Loss in epoch 186: [86.96199]\n",
      "Loss in epoch 187: [86.880226]\n",
      "Loss in epoch 188: [86.790527]\n",
      "Loss in epoch 189: [86.721436]\n",
      "Loss in epoch 190: [86.645103]\n",
      "Loss in epoch 191: [86.592293]\n",
      "Loss in epoch 192: [86.509621]\n",
      "Loss in epoch 193: [86.458206]\n",
      "Loss in epoch 194: [86.391541]\n",
      "Loss in epoch 195: [86.315369]\n",
      "Loss in epoch 196: [86.256477]\n",
      "Loss in epoch 197: [86.205803]\n",
      "Loss in epoch 198: [86.151337]\n",
      "Loss in epoch 199: [86.095932]\n",
      "Loss in epoch 200: [86.028503]\n",
      "Loss in epoch 201: [86.002937]\n",
      "Loss in epoch 202: [85.947601]\n",
      "Loss in epoch 203: [85.925613]\n",
      "Loss in epoch 204: [85.868942]\n",
      "Loss in epoch 205: [85.843788]\n",
      "Loss in epoch 206: [85.802032]\n",
      "Loss in epoch 207: [85.75576]\n",
      "Loss in epoch 208: [85.728737]\n",
      "Loss in epoch 209: [85.704979]\n",
      "Loss in epoch 210: [85.650803]\n",
      "Loss in epoch 211: [85.623924]\n",
      "Loss in epoch 212: [85.592514]\n",
      "Loss in epoch 213: [85.557846]\n",
      "Loss in epoch 214: [85.541214]\n",
      "Loss in epoch 215: [85.504189]\n",
      "Loss in epoch 216: [85.474243]\n",
      "Loss in epoch 217: [85.459145]\n",
      "Loss in epoch 218: [85.420364]\n",
      "Loss in epoch 219: [85.412338]\n",
      "Loss in epoch 220: [85.362038]\n",
      "Loss in epoch 221: [85.34845]\n",
      "Loss in epoch 222: [85.319077]\n",
      "Loss in epoch 223: [85.297363]\n",
      "Loss in epoch 224: [85.266098]\n",
      "Loss in epoch 225: [85.225975]\n",
      "Loss in epoch 226: [85.219482]\n",
      "Loss in epoch 227: [85.190529]\n",
      "Loss in epoch 228: [85.151718]\n",
      "Loss in epoch 229: [85.125359]\n",
      "Loss in epoch 230: [85.099464]\n",
      "Loss in epoch 231: [85.085716]\n",
      "Loss in epoch 232: [85.050278]\n",
      "Loss in epoch 233: [85.051102]\n",
      "Loss in epoch 234: [85.037239]\n",
      "Loss in epoch 235: [85.030861]\n",
      "Loss in epoch 236: [84.998192]\n",
      "Loss in epoch 237: [84.985435]\n",
      "Loss in epoch 238: [84.969032]\n",
      "Loss in epoch 239: [84.969254]\n",
      "Loss in epoch 240: [84.934677]\n",
      "Loss in epoch 241: [84.934128]\n",
      "Loss in epoch 242: [84.924606]\n",
      "Loss in epoch 243: [84.921654]\n",
      "Loss in epoch 244: [84.900589]\n",
      "Loss in epoch 245: [84.884651]\n",
      "Loss in epoch 246: [84.854416]\n",
      "Loss in epoch 247: [84.881149]\n",
      "Loss in epoch 248: [84.850151]\n",
      "Loss in epoch 249: [84.879311]\n",
      "Loss in epoch 250: [84.845894]\n",
      "Loss in epoch 251: [84.85318]\n",
      "Loss in epoch 252: [84.832024]\n",
      "Loss in epoch 253: [84.844727]\n",
      "Loss in epoch 254: [84.821434]\n",
      "Loss in epoch 255: [84.836403]\n",
      "Loss in epoch 256: [84.81855]\n",
      "Loss in epoch 257: [84.815994]\n",
      "Loss in epoch 258: [84.80587]\n",
      "Loss in epoch 259: [84.800308]\n",
      "Loss in epoch 260: [84.798973]\n",
      "Loss in epoch 261: [84.814957]\n",
      "Loss in epoch 262: [84.798004]\n",
      "Loss in epoch 263: [84.781937]\n",
      "Loss in epoch 264: [84.792961]\n",
      "Loss in epoch 265: [84.787071]\n",
      "Loss in epoch 266: [84.758102]\n",
      "Loss in epoch 267: [84.772804]\n",
      "Loss in epoch 268: [84.805626]\n",
      "Loss in epoch 269: [84.787018]\n",
      "Loss in epoch 270: [84.781998]\n",
      "Loss in epoch 271: [84.779579]\n",
      "Loss in epoch 272: [84.768044]\n",
      "Loss in epoch 273: [84.794579]\n",
      "Loss in epoch 274: [84.764389]\n",
      "Loss in epoch 275: [84.785294]\n",
      "Loss in epoch 276: [84.762947]\n",
      "Loss in epoch 277: [84.776016]\n",
      "Loss in epoch 278: [84.768265]\n",
      "Loss in epoch 279: [84.78212]\n",
      "Loss in epoch 280: [84.761215]\n",
      "Loss in epoch 281: [84.781967]\n",
      "Loss in epoch 282: [84.75502]\n",
      "Loss in epoch 283: [84.769829]\n",
      "Loss in epoch 284: [84.757988]\n",
      "Loss in epoch 285: [84.785904]\n",
      "Loss in epoch 286: [84.750542]\n",
      "Loss in epoch 287: [84.759209]\n",
      "Loss in epoch 288: [84.753067]\n",
      "Loss in epoch 289: [84.783783]\n",
      "Loss in epoch 290: [84.75985]\n",
      "Loss in epoch 291: [84.779755]\n",
      "Loss in epoch 292: [84.759804]\n",
      "Loss in epoch 293: [84.76506]\n",
      "Loss in epoch 294: [84.758316]\n",
      "Loss in epoch 295: [84.776855]\n",
      "Loss in epoch 296: [84.753098]\n",
      "Loss in epoch 297: [84.777634]\n",
      "Loss in epoch 298: [84.757645]\n",
      "Loss in epoch 299: [84.775101]\n",
      "Loss in epoch 300: [84.757668]\n",
      "Loss in epoch 301: [84.774124]\n",
      "Loss in epoch 302: [84.742599]\n",
      "Loss in epoch 303: [84.773575]\n",
      "Loss in epoch 304: [84.754478]\n",
      "Loss in epoch 305: [84.777603]\n",
      "Loss in epoch 306: [84.754494]\n",
      "Loss in epoch 307: [84.775566]\n",
      "Loss in epoch 308: [84.754631]\n",
      "Loss in epoch 309: [84.773842]\n",
      "Loss in epoch 310: [84.754845]\n",
      "Loss in epoch 311: [84.772354]\n",
      "Loss in epoch 312: [84.7556]\n",
      "Loss in epoch 313: [84.760109]\n",
      "Loss in epoch 314: [84.746048]\n",
      "Loss in epoch 315: [84.775696]\n",
      "Loss in epoch 316: [84.753494]\n",
      "Loss in epoch 317: [84.774109]\n",
      "Loss in epoch 318: [84.753525]\n",
      "Loss in epoch 319: [84.772842]\n",
      "Loss in epoch 320: [84.753639]\n",
      "Loss in epoch 321: [84.771729]\n",
      "Loss in epoch 322: [84.753799]\n",
      "Loss in epoch 323: [84.770813]\n",
      "Loss in epoch 324: [84.74469]\n",
      "Loss in epoch 325: [84.768379]\n",
      "Loss in epoch 326: [84.752998]\n",
      "Loss in epoch 327: [84.773155]\n",
      "Loss in epoch 328: [84.744354]\n",
      "Loss in epoch 329: [84.77832]\n",
      "Loss in epoch 330: [84.747398]\n",
      "Loss in epoch 331: [84.777077]\n",
      "Loss in epoch 332: [84.747475]\n",
      "Loss in epoch 333: [84.776085]\n",
      "Loss in epoch 334: [84.747581]\n",
      "Loss in epoch 335: [84.775223]\n",
      "Loss in epoch 336: [84.747719]\n",
      "Loss in epoch 337: [84.774475]\n",
      "Loss in epoch 338: [84.747841]\n",
      "Loss in epoch 339: [84.773811]\n",
      "Loss in epoch 340: [84.74823]\n",
      "Loss in epoch 341: [84.76226]\n",
      "Loss in epoch 342: [84.754776]\n",
      "Loss in epoch 343: [84.76844]\n",
      "Loss in epoch 344: [84.754272]\n",
      "Loss in epoch 345: [84.768135]\n",
      "Loss in epoch 346: [84.753899]\n",
      "Loss in epoch 347: [84.758545]\n",
      "Loss in epoch 348: [84.751526]\n",
      "Loss in epoch 349: [84.769363]\n",
      "Loss in epoch 350: [84.753975]\n",
      "Loss in epoch 351: [84.768692]\n",
      "Loss in epoch 352: [84.753456]\n",
      "Loss in epoch 353: [84.768356]\n",
      "Loss in epoch 354: [84.753029]\n",
      "Loss in epoch 355: [84.768051]\n",
      "Loss in epoch 356: [84.752701]\n",
      "Loss in epoch 357: [84.767799]\n",
      "Loss in epoch 358: [84.752419]\n",
      "Loss in epoch 359: [84.767563]\n",
      "Loss in epoch 360: [84.752197]\n",
      "Loss in epoch 361: [84.767349]\n",
      "Loss in epoch 362: [84.75209]\n",
      "Loss in epoch 363: [84.756432]\n",
      "Loss in epoch 364: [84.7584]\n",
      "Loss in epoch 365: [84.762695]\n",
      "Loss in epoch 366: [84.757668]\n",
      "Loss in epoch 367: [84.762711]\n",
      "Loss in epoch 368: [84.757065]\n",
      "Loss in epoch 369: [84.762718]\n",
      "Loss in epoch 370: [84.756561]\n",
      "Loss in epoch 371: [84.762718]\n",
      "Loss in epoch 372: [84.756119]\n",
      "Loss in epoch 373: [84.762772]\n",
      "Loss in epoch 374: [84.755669]\n",
      "Loss in epoch 375: [84.763474]\n",
      "Loss in epoch 376: [84.75518]\n",
      "Loss in epoch 377: [84.763611]\n",
      "Loss in epoch 378: [84.74617]\n",
      "Loss in epoch 379: [84.758545]\n",
      "Loss in epoch 380: [84.748352]\n",
      "Loss in epoch 381: [84.76857]\n",
      "Loss in epoch 382: [84.751328]\n",
      "Loss in epoch 383: [84.767921]\n",
      "Loss in epoch 384: [84.751167]\n",
      "Loss in epoch 385: [84.767418]\n",
      "Loss in epoch 386: [84.751045]\n",
      "Loss in epoch 387: [84.767014]\n",
      "Loss in epoch 388: [84.750938]\n",
      "Loss in epoch 389: [84.766663]\n",
      "Loss in epoch 390: [84.750854]\n",
      "Loss in epoch 391: [84.766357]\n",
      "Loss in epoch 392: [84.750778]\n",
      "Loss in epoch 393: [84.766098]\n",
      "Loss in epoch 394: [84.75071]\n",
      "Loss in epoch 395: [84.765862]\n",
      "Loss in epoch 396: [84.750656]\n",
      "Loss in epoch 397: [84.765663]\n",
      "Loss in epoch 398: [84.750595]\n",
      "Loss in epoch 399: [84.765472]\n",
      "Loss in epoch 400: [84.750534]\n",
      "Loss in epoch 401: [84.765305]\n",
      "Loss in epoch 402: [84.739853]\n",
      "Loss in epoch 403: [84.77327]\n",
      "Loss in epoch 404: [84.74321]\n",
      "Loss in epoch 405: [84.772415]\n",
      "Loss in epoch 406: [84.743393]\n",
      "Loss in epoch 407: [84.771927]\n",
      "Loss in epoch 408: [84.743576]\n",
      "Loss in epoch 409: [84.7715]\n",
      "Loss in epoch 410: [84.743736]\n",
      "Loss in epoch 411: [84.771103]\n",
      "Loss in epoch 412: [84.743881]\n",
      "Loss in epoch 413: [84.770767]\n",
      "Loss in epoch 414: [84.744011]\n",
      "Loss in epoch 415: [84.759911]\n",
      "Loss in epoch 416: [84.75058]\n",
      "Loss in epoch 417: [84.765808]\n",
      "Loss in epoch 418: [84.750114]\n",
      "Loss in epoch 419: [84.765732]\n",
      "Loss in epoch 420: [84.749718]\n",
      "Loss in epoch 421: [84.765663]\n",
      "Loss in epoch 422: [84.74939]\n",
      "Loss in epoch 423: [84.765602]\n",
      "Loss in epoch 424: [84.7491]\n",
      "Loss in epoch 425: [84.765549]\n",
      "Loss in epoch 426: [84.74884]\n",
      "Loss in epoch 427: [84.765503]\n",
      "Loss in epoch 428: [84.748619]\n",
      "Loss in epoch 429: [84.765457]\n",
      "Loss in epoch 430: [84.748413]\n",
      "Loss in epoch 431: [84.765419]\n",
      "Loss in epoch 432: [84.74823]\n",
      "Loss in epoch 433: [84.765381]\n",
      "Loss in epoch 434: [84.748055]\n",
      "Loss in epoch 435: [84.765335]\n",
      "Loss in epoch 436: [84.747894]\n",
      "Loss in epoch 437: [84.765289]\n",
      "Loss in epoch 438: [84.747742]\n",
      "Loss in epoch 439: [84.765244]\n",
      "Loss in epoch 440: [84.747604]\n",
      "Loss in epoch 441: [84.765213]\n",
      "Loss in epoch 442: [84.747482]\n",
      "Loss in epoch 443: [84.765167]\n",
      "Loss in epoch 444: [84.74736]\n",
      "Loss in epoch 445: [84.765121]\n",
      "Loss in epoch 446: [84.747238]\n",
      "Loss in epoch 447: [84.765091]\n",
      "Loss in epoch 448: [84.747131]\n",
      "Loss in epoch 449: [84.765038]\n",
      "Loss in epoch 450: [84.747025]\n",
      "Loss in epoch 451: [84.764999]\n",
      "Loss in epoch 452: [84.746902]\n",
      "Loss in epoch 453: [84.764954]\n",
      "Loss in epoch 454: [84.746811]\n",
      "Loss in epoch 455: [84.764915]\n",
      "Loss in epoch 456: [84.746704]\n",
      "Loss in epoch 457: [84.764862]\n",
      "Loss in epoch 458: [84.746605]\n",
      "Loss in epoch 459: [84.764816]\n",
      "Loss in epoch 460: [84.746506]\n",
      "Loss in epoch 461: [84.764778]\n",
      "Loss in epoch 462: [84.746414]\n",
      "Loss in epoch 463: [84.764725]\n",
      "Loss in epoch 464: [84.746323]\n",
      "Loss in epoch 465: [84.764687]\n",
      "Loss in epoch 466: [84.746231]\n",
      "Loss in epoch 467: [84.764633]\n",
      "Loss in epoch 468: [84.74614]\n",
      "Loss in epoch 469: [84.764397]\n",
      "Loss in epoch 470: [84.746117]\n",
      "Loss in epoch 471: [84.764198]\n",
      "Loss in epoch 472: [84.746178]\n",
      "Loss in epoch 473: [84.764015]\n",
      "Loss in epoch 474: [84.746223]\n",
      "Loss in epoch 475: [84.763855]\n",
      "Loss in epoch 476: [84.746246]\n",
      "Loss in epoch 477: [84.76371]\n",
      "Loss in epoch 478: [84.746262]\n",
      "Loss in epoch 479: [84.763565]\n",
      "Loss in epoch 480: [84.746277]\n",
      "Loss in epoch 481: [84.763443]\n",
      "Loss in epoch 482: [84.746269]\n",
      "Loss in epoch 483: [84.763329]\n",
      "Loss in epoch 484: [84.746262]\n",
      "Loss in epoch 485: [84.763222]\n",
      "Loss in epoch 486: [84.746262]\n",
      "Loss in epoch 487: [84.762802]\n",
      "Loss in epoch 488: [84.74633]\n",
      "Loss in epoch 489: [84.762543]\n",
      "Loss in epoch 490: [84.74646]\n",
      "Loss in epoch 491: [84.762306]\n",
      "Loss in epoch 492: [84.746582]\n",
      "Loss in epoch 493: [84.762093]\n",
      "Loss in epoch 494: [84.746674]\n",
      "Loss in epoch 495: [84.761894]\n",
      "Loss in epoch 496: [84.746758]\n",
      "Loss in epoch 497: [84.761711]\n",
      "Loss in epoch 498: [84.746811]\n",
      "Loss in epoch 499: [84.761551]\n",
      "[3]\n",
      "Optimal Action Squence:[[ 0.99849999  1.        ]\n",
      " [ 0.9957      1.        ]\n",
      " [ 0.99260002  1.        ]\n",
      " [ 0.86809999  1.        ]\n",
      " [ 0.0851      1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 0.99540001  0.71069998]\n",
      " [ 0.68190002  0.0253    ]\n",
      " [-0.0143     -0.0108    ]\n",
      " [ 0.0369      0.2123    ]]\n",
      "The last state:[ 8.01956558  8.23163795]\n",
      "Rewards each time step:[[-16.        ]\n",
      " [-13.981534  ]\n",
      " [-11.96590328]\n",
      " [ -9.95425797]\n",
      " [ -8.08136082]\n",
      " [ -7.09325886]\n",
      " [ -5.51942348]\n",
      " [ -4.20445347]\n",
      " [ -2.39465904]\n",
      " [ -0.69401121]\n",
      " [ -0.03149176]\n",
      " [ -0.03500366]]\n",
      "Intermediate states:[[ 1.00846779  1.00999856]\n",
      " [ 2.01412272  2.01997352]\n",
      " [ 3.01620245  3.02954006]\n",
      " [ 3.88652492  4.03211403]\n",
      " [ 3.96403575  4.94270515]\n",
      " [ 4.75095367  5.72962284]\n",
      " [ 5.40843868  6.38710785]\n",
      " [ 6.3133359   7.29200506]\n",
      " [ 7.30866385  8.00267506]\n",
      " [ 7.99672842  8.02822018]\n",
      " [ 7.98232079  8.01732445]\n",
      " [ 8.01956558  8.23163795]]\n"
     ]
    }
   ],
   "source": [
    "rnn_inst.Optimize(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
