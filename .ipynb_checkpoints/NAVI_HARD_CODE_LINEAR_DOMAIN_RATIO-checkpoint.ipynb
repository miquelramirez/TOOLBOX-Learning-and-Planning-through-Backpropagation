{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NAVI_HARD_CODE_DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "#Functional coding\n",
    "import functools\n",
    "from functools import partial\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Path..\n",
    "Datapath=\"DATA/Navigation/linear/Navigation_Data.txt\"\n",
    "Labelpath=\"DATA/Navigation/linear/Navigation_Label.txt\"\n",
    "Rewardpath=\"DATA/Navigation/linear/Navigation_Reward.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given local path, find full path\n",
    "def PathFinder(path):\n",
    "    #python 2\n",
    "    #script_dir = os.path.dirname('__file__')\n",
    "    #fullpath = os.path.join(script_dir,path)\n",
    "    #python 3\n",
    "    fullpath=os.path.abspath(path)\n",
    "    print(fullpath)\n",
    "    return fullpath\n",
    "\n",
    "#Read Data for Deep Learning\n",
    "def ReadData(path):\n",
    "    fullpath=PathFinder(path)\n",
    "    return pd.read_csv(fullpath, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Data.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Label.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Reward.txt\n"
     ]
    }
   ],
   "source": [
    "S_A_pd = ReadData(Datapath)\n",
    "SP_pd = ReadData(Labelpath)\n",
    "R_pd = ReadData(Rewardpath)\n",
    "S_A_matrix=S_A_pd.as_matrix()\n",
    "SP_matrix=SP_pd.as_matrix()\n",
    "R_matrix=R_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_settings = {\n",
    "    \"dims\"          : 2,\n",
    "    \"min_maze_bound\": tf.constant(0.0,dtype=tf.float32), \n",
    "    \"max_maze_bound\": tf.constant(10.0,dtype=tf.float32), \n",
    "    \"min_act_bound\": tf.constant(-0.5,dtype=tf.float32), \n",
    "    \"max_act_bound\": tf.constant(0.5,dtype=tf.float32), \n",
    "    \"goal\"    : tf.constant(8.0,dtype=tf.float32),\n",
    "    \"penalty\" : tf.constant(1000000.0,dtype=tf.float32),\n",
    "    \"centre\"  : tf.constant(5.0,dtype=tf.float32)\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NAVI(object):\n",
    "    def __init__(self, \n",
    "                 batch_size,\n",
    "                 default_settings):\n",
    "        self.__dict__.update(default_settings)\n",
    "        self.batch_size = batch_size\n",
    "        self.zero = tf.constant(0,shape=[self.batch_size,2],dtype=tf.float32)\n",
    "        self.four = tf.constant(4.0,dtype=tf.float32)\n",
    "        self.one = tf.constant(1.0,shape=[batch_size],dtype=tf.float32)\n",
    "        self.five = tf.constant(5.0,dtype=tf.float32)\n",
    "        self.onedsix = tf.constant(1.6,dtype=tf.float32)\n",
    "        self.onedtwo = tf.constant(1.2,dtype=tf.float32)\n",
    "        self.doteight = tf.constant(0.8,dtype=tf.float32)\n",
    "        self.dotfour = tf.constant(0.4,dtype=tf.float32)\n",
    "        self.dotofive = tf.constant(0.05,shape=[batch_size],dtype = tf.float32)\n",
    "    \n",
    "    def MINMAZEBOUND(self):\n",
    "        return self.min_maze_bound\n",
    "    \n",
    "    def MAXMAZEBOUND(self):\n",
    "        return self.max_maze_bound\n",
    "    \n",
    "    def MINACTIONBOUND(self):\n",
    "        return self.min_act_bound\n",
    "    \n",
    "    def MAXACTIONBOUND(self):\n",
    "        return self.max_act_bound\n",
    "    \n",
    "    def GOAL(self):\n",
    "        return self.goal\n",
    "    \n",
    "    def CENTER(self):\n",
    "        return self.centre\n",
    "    \n",
    "    def PENALTY(self):\n",
    "        return self.penalty\n",
    "    \n",
    "    def Transition(self, states, actions):\n",
    "        previous_state = states\n",
    "        distance = tf.reduce_sum(tf.abs(states-self.CENTER()),1)\n",
    "        scalefactor = tf.select(tf.logical_and(tf.less(distance,self.four),tf.greater_equal(distance,self.doteight)),\\\n",
    "                                tf.floor(distance*self.five/self.four)/self.five,\\\n",
    "                                tf.select(tf.less(distance,self.doteight),self.dotofive,self.one ))\n",
    "        \n",
    "        proposedLoc = previous_state + tf.matrix_transpose(scalefactor*tf.matrix_transpose(actions))\n",
    "        new_states = tf.select(tf.logical_and(tf.less_equal(proposedLoc,self.MAXMAZEBOUND()),tf.greater_equal(proposedLoc,self.MINMAZEBOUND())),\\\n",
    "                               proposedLoc,\\\n",
    "                              tf.select(tf.greater(proposedLoc,self.MAXMAZEBOUND()),\\\n",
    "                                        self.zero+self.MAXMAZEBOUND(),\\\n",
    "                                        self.zero+self.MINMAZEBOUND())\\\n",
    "                              )\n",
    "        return new_states\n",
    "\n",
    "    def Reward(self, states,actions):\n",
    "        new_reward = -tf.reduce_sum(tf.abs(states-self.GOAL()),1,keep_dims=True)\n",
    "        return new_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAVICell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, batch_size, default_settings):\n",
    "        self._num_state_units = 2\n",
    "        self._num_reward_units = 3\n",
    "        self.navi = NAVI(batch_size, default_settings)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_state_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_reward_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        next_state =  self.navi.Transition(state, inputs)\n",
    "        reward = self.navi.Reward(state, inputs)      \n",
    "        return tf.concat(1,[reward,next_state]), next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    def __init__(self,\n",
    "                a, # Actions\n",
    "                num_step, # Number of RNN step, this is a fixed step RNN sequence, 12 for navigation\n",
    "                num_act, # Number of actions\n",
    "                batch_size, #Batch Size\n",
    "                learning_rate=0.005): \n",
    "        self.action = tf.reshape(a,[-1,num_step,num_act]) #Reshape rewards\n",
    "        print(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_step = num_step\n",
    "        self.learning_rate = learning_rate\n",
    "        #self._p_get_weights()\n",
    "        self._p_create_rnn_graph()\n",
    "        self._p_Q_loss()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _p_create_rnn_graph(self):\n",
    "        cell = NAVICell(self.batch_size,default_settings)\n",
    "        initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        print('action batch size:{0}'.format(array_ops.shape(self.action)[0]))\n",
    "        print('Initial_state shape:{0}'.format(initial_state))\n",
    "        rnn_outputs, state = tf.nn.dynamic_rnn(cell, self.action, dtype=tf.float32,initial_state=initial_state)\n",
    "        #need output intermediate states as well\n",
    "        concated = tf.concat(0,rnn_outputs)\n",
    "        print('concated shape:{0}'.format(concated.get_shape()))\n",
    "        something_unpacked =  tf.unpack(concated, axis=2)\n",
    "        self.outputs = tf.reshape(something_unpacked[0],[-1,self.num_step,1])\n",
    "        print(' self.outputs:{0}'.format(self.outputs.get_shape()))\n",
    "        self.intern_states = tf.pack([something_unpacked[1],something_unpacked[2]], axis=2)\n",
    "        self.last_state = state\n",
    "        self.pred = tf.reduce_sum(self.outputs,1)\n",
    "        self.average_pred = tf.reduce_mean(self.pred)\n",
    "        print(\"self.pred:{0}\".format(self.pred))\n",
    "            \n",
    "    def _p_create_loss(self):\n",
    "\n",
    "        #objective = tf.reduce_mean(tf.reduce_sum(self.outputs*self.weight,1)) \n",
    "        objective = self.average_pred \n",
    "        self.loss = -objective\n",
    "        print(self.loss.get_shape())\n",
    "        #self.loss = -objective\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def _p_Q_loss(self):\n",
    "        objective = tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "        for i in range(self.num_step):\n",
    "            Rt = self.outputs[:,i]\n",
    "            SumRj=tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "            SumRk=tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "            if i<(self.num_step-1):\n",
    "                j = i+1\n",
    "                SumRj = tf.reduce_sum(self.outputs[:,j:],1)\n",
    "            #if i<(self.num_step-1):\n",
    "                #k= i+1\n",
    "                #SumRk = tf.reduce_sum(self.outputs[:,k:],1)\n",
    "            objective+=(Rt*(SumRj-SumRk)+tf.square(Rt))/(self.num_step-i)\n",
    "        self.loss = tf.reduce_mean(tf.square(objective))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def Optimize(self,epoch=100):\n",
    "        \n",
    "        new_loss = self.sess.run([self.average_pred])\n",
    "        print('Loss in epoch {0}: {1}'.format(\"Initial\", new_loss)) \n",
    "        for epoch in range(epoch):\n",
    "            training = self.sess.run([self.optimizer])\n",
    "            self.sess.run(tf.assign(a, tf.clip_by_value(a, default_settings['min_act_bound'], default_settings['max_act_bound'])))\n",
    "            if True:\n",
    "                new_loss = self.sess.run([self.average_pred])\n",
    "                print('Loss in epoch {0}: {1}'.format(epoch, new_loss))  \n",
    "        minimum_costs_id=self.sess.run(tf.argmax(self.pred,0))\n",
    "        print(minimum_costs_id)\n",
    "        best_action = np.round(self.sess.run(self.action)[minimum_costs_id[0]],4)\n",
    "        print('Optimal Action Squence:{0}'.format(best_action))\n",
    "        pred_list = self.sess.run(self.pred)\n",
    "        pred_list=np.sort(pred_list.flatten())[::-1]\n",
    "        pred_list=pred_list[:50]\n",
    "        pred_mean = np.mean(pred_list)\n",
    "        pred_std = np.std(pred_list)\n",
    "        print('Best Cost: {0}'.format(pred_list[0]))\n",
    "        print('Sorted Costs:{0}'.format(pred_list))\n",
    "        print('MEAN: {0}, STD:{1}'.format(pred_mean,pred_std))\n",
    "        print('The last state:{0}'.format(self.sess.run(self.last_state)[minimum_costs_id[0]]))\n",
    "        print('Rewards each time step:{0}'.format(self.sess.run(self.outputs)[minimum_costs_id[0]]))\n",
    "        print('Intermediate states:{0}'.format(self.sess.run(self.intern_states)[minimum_costs_id[0]]))\n",
    "        \n",
    "    def _p_get_weights(self):\n",
    "        weight_list = np.square(np.arange(1,self.num_step+1))/np.sum(np.square(np.arange(1,self.num_step+1)))\n",
    "        self.weight = tf.reshape(tf.constant(weight_list,dtype=tf.float32),[self.num_step,1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(100, 30, 2), dtype=float32)\n",
      "action batch size:Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Initial_state shape:Tensor(\"zeros:0\", shape=(100, 2), dtype=float32)\n",
      "concated shape:(100, 30, 3)\n",
      " self.outputs:(100, 30, 1)\n",
      "self.pred:Tensor(\"Sum:0\", shape=(100, 1), dtype=float32)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.truncated_normal(shape=[6000],mean=0.0, stddev=0.2),name=\"action\")\n",
    "rnn_inst = ActionOptimizer(a, 30,2,100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch Initial: [-456.67804]\n",
      "Loss in epoch 0: [-456.34305]\n",
      "Loss in epoch 1: [-455.9902]\n",
      "Loss in epoch 2: [-455.61835]\n",
      "Loss in epoch 3: [-455.22363]\n",
      "Loss in epoch 4: [-454.80554]\n",
      "Loss in epoch 5: [-454.36304]\n",
      "Loss in epoch 6: [-453.89789]\n",
      "Loss in epoch 7: [-453.40775]\n",
      "Loss in epoch 8: [-452.888]\n",
      "Loss in epoch 9: [-452.34164]\n",
      "Loss in epoch 10: [-451.76773]\n",
      "Loss in epoch 11: [-451.16449]\n",
      "Loss in epoch 12: [-450.52707]\n",
      "Loss in epoch 13: [-449.85645]\n",
      "Loss in epoch 14: [-449.15109]\n",
      "Loss in epoch 15: [-448.40823]\n",
      "Loss in epoch 16: [-447.62329]\n",
      "Loss in epoch 17: [-446.77744]\n",
      "Loss in epoch 18: [-445.88235]\n",
      "Loss in epoch 19: [-444.93616]\n",
      "Loss in epoch 20: [-443.93512]\n",
      "Loss in epoch 21: [-442.86121]\n",
      "Loss in epoch 22: [-441.73999]\n",
      "Loss in epoch 23: [-440.5596]\n",
      "Loss in epoch 24: [-439.31876]\n",
      "Loss in epoch 25: [-438.02621]\n",
      "Loss in epoch 26: [-436.66772]\n",
      "Loss in epoch 27: [-435.23099]\n",
      "Loss in epoch 28: [-433.72791]\n",
      "Loss in epoch 29: [-432.16806]\n",
      "Loss in epoch 30: [-430.5582]\n",
      "Loss in epoch 31: [-428.89178]\n",
      "Loss in epoch 32: [-427.16467]\n",
      "Loss in epoch 33: [-425.38571]\n",
      "Loss in epoch 34: [-423.51254]\n",
      "Loss in epoch 35: [-421.55441]\n",
      "Loss in epoch 36: [-419.53445]\n",
      "Loss in epoch 37: [-417.44235]\n",
      "Loss in epoch 38: [-415.29608]\n",
      "Loss in epoch 39: [-413.0578]\n",
      "Loss in epoch 40: [-410.77814]\n",
      "Loss in epoch 41: [-408.39676]\n",
      "Loss in epoch 42: [-406.00116]\n",
      "Loss in epoch 43: [-403.52005]\n",
      "Loss in epoch 44: [-400.99997]\n",
      "Loss in epoch 45: [-398.42239]\n",
      "Loss in epoch 46: [-395.81427]\n",
      "Loss in epoch 47: [-393.1066]\n",
      "Loss in epoch 48: [-390.40125]\n",
      "Loss in epoch 49: [-387.66449]\n",
      "Loss in epoch 50: [-384.98419]\n",
      "Loss in epoch 51: [-382.19919]\n",
      "Loss in epoch 52: [-379.39276]\n",
      "Loss in epoch 53: [-376.67505]\n",
      "Loss in epoch 54: [-373.88892]\n",
      "Loss in epoch 55: [-371.08054]\n",
      "Loss in epoch 56: [-368.31351]\n",
      "Loss in epoch 57: [-365.59192]\n",
      "Loss in epoch 58: [-362.79312]\n",
      "Loss in epoch 59: [-360.09399]\n",
      "Loss in epoch 60: [-357.33917]\n",
      "Loss in epoch 61: [-354.73788]\n",
      "Loss in epoch 62: [-352.09332]\n",
      "Loss in epoch 63: [-349.5275]\n",
      "Loss in epoch 64: [-347.09274]\n",
      "Loss in epoch 65: [-344.4964]\n",
      "Loss in epoch 66: [-342.16116]\n",
      "Loss in epoch 67: [-339.73691]\n",
      "Loss in epoch 68: [-337.3266]\n",
      "Loss in epoch 69: [-334.92072]\n",
      "Loss in epoch 70: [-332.63663]\n",
      "Loss in epoch 71: [-330.49832]\n",
      "Loss in epoch 72: [-328.26172]\n",
      "Loss in epoch 73: [-326.16998]\n",
      "Loss in epoch 74: [-324.11243]\n",
      "Loss in epoch 75: [-322.15555]\n",
      "Loss in epoch 76: [-320.17975]\n",
      "Loss in epoch 77: [-318.12991]\n",
      "Loss in epoch 78: [-316.23087]\n",
      "Loss in epoch 79: [-314.37231]\n",
      "Loss in epoch 80: [-312.52988]\n",
      "Loss in epoch 81: [-310.76407]\n",
      "Loss in epoch 82: [-308.99515]\n",
      "Loss in epoch 83: [-307.22272]\n",
      "Loss in epoch 84: [-305.52249]\n",
      "Loss in epoch 85: [-303.98148]\n",
      "Loss in epoch 86: [-302.23233]\n",
      "Loss in epoch 87: [-300.71036]\n",
      "Loss in epoch 88: [-299.2193]\n",
      "Loss in epoch 89: [-297.88641]\n",
      "Loss in epoch 90: [-296.44559]\n",
      "Loss in epoch 91: [-295.1456]\n",
      "Loss in epoch 92: [-293.71927]\n",
      "Loss in epoch 93: [-292.35916]\n",
      "Loss in epoch 94: [-291.07971]\n",
      "Loss in epoch 95: [-289.88153]\n",
      "Loss in epoch 96: [-288.6564]\n",
      "Loss in epoch 97: [-287.3158]\n",
      "Loss in epoch 98: [-286.19409]\n",
      "Loss in epoch 99: [-285.05396]\n",
      "Loss in epoch 100: [-284.07718]\n",
      "Loss in epoch 101: [-283.0481]\n",
      "Loss in epoch 102: [-281.84146]\n",
      "Loss in epoch 103: [-280.89456]\n",
      "Loss in epoch 104: [-280.06866]\n",
      "Loss in epoch 105: [-279.06149]\n",
      "Loss in epoch 106: [-278.12161]\n",
      "Loss in epoch 107: [-277.2847]\n",
      "Loss in epoch 108: [-276.41968]\n",
      "Loss in epoch 109: [-275.4263]\n",
      "Loss in epoch 110: [-274.33386]\n",
      "Loss in epoch 111: [-273.61911]\n",
      "Loss in epoch 112: [-272.75595]\n",
      "Loss in epoch 113: [-272.05612]\n",
      "Loss in epoch 114: [-271.44006]\n",
      "Loss in epoch 115: [-270.72067]\n",
      "Loss in epoch 116: [-270.07309]\n",
      "Loss in epoch 117: [-269.36224]\n",
      "Loss in epoch 118: [-268.57748]\n",
      "Loss in epoch 119: [-268.35712]\n",
      "Loss in epoch 120: [-267.67676]\n",
      "Loss in epoch 121: [-267.02194]\n",
      "Loss in epoch 122: [-266.50693]\n",
      "Loss in epoch 123: [-265.78302]\n",
      "Loss in epoch 124: [-265.29144]\n",
      "Loss in epoch 125: [-264.55219]\n",
      "Loss in epoch 126: [-263.67386]\n",
      "Loss in epoch 127: [-263.14316]\n",
      "Loss in epoch 128: [-262.4968]\n",
      "Loss in epoch 129: [-261.94577]\n",
      "Loss in epoch 130: [-261.63586]\n",
      "Loss in epoch 131: [-260.97232]\n",
      "Loss in epoch 132: [-260.65784]\n",
      "Loss in epoch 133: [-260.49146]\n",
      "Loss in epoch 134: [-260.17599]\n",
      "Loss in epoch 135: [-259.86578]\n",
      "Loss in epoch 136: [-259.80157]\n",
      "Loss in epoch 137: [-259.28796]\n",
      "Loss in epoch 138: [-259.19342]\n",
      "Loss in epoch 139: [-259.31467]\n",
      "Loss in epoch 140: [-258.81274]\n",
      "Loss in epoch 141: [-258.33435]\n",
      "Loss in epoch 142: [-258.00861]\n",
      "Loss in epoch 143: [-257.63068]\n",
      "Loss in epoch 144: [-257.2821]\n",
      "Loss in epoch 145: [-256.95972]\n",
      "Loss in epoch 146: [-256.53525]\n",
      "Loss in epoch 147: [-256.35641]\n",
      "Loss in epoch 148: [-256.03226]\n",
      "Loss in epoch 149: [-255.66264]\n",
      "Loss in epoch 150: [-255.4855]\n",
      "Loss in epoch 151: [-255.3596]\n",
      "Loss in epoch 152: [-255.09305]\n",
      "Loss in epoch 153: [-254.91315]\n",
      "Loss in epoch 154: [-254.31425]\n",
      "Loss in epoch 155: [-254.13101]\n",
      "Loss in epoch 156: [-254.04263]\n",
      "Loss in epoch 157: [-254.002]\n",
      "Loss in epoch 158: [-253.64409]\n",
      "Loss in epoch 159: [-253.58698]\n",
      "Loss in epoch 160: [-253.54779]\n",
      "Loss in epoch 161: [-253.55022]\n",
      "Loss in epoch 162: [-253.27826]\n",
      "Loss in epoch 163: [-253.06821]\n",
      "Loss in epoch 164: [-252.88512]\n",
      "Loss in epoch 165: [-252.78413]\n",
      "Loss in epoch 166: [-253.0152]\n",
      "Loss in epoch 167: [-252.75957]\n",
      "Loss in epoch 168: [-252.70255]\n",
      "Loss in epoch 169: [-252.60902]\n",
      "Loss in epoch 170: [-252.44073]\n",
      "Loss in epoch 171: [-252.12878]\n",
      "Loss in epoch 172: [-251.96231]\n",
      "Loss in epoch 173: [-251.67336]\n",
      "Loss in epoch 174: [-251.47891]\n",
      "Loss in epoch 175: [-251.33829]\n",
      "Loss in epoch 176: [-251.44539]\n",
      "Loss in epoch 177: [-251.60201]\n",
      "Loss in epoch 178: [-251.62212]\n",
      "Loss in epoch 179: [-251.58466]\n",
      "Loss in epoch 180: [-251.33061]\n",
      "Loss in epoch 181: [-251.10785]\n",
      "Loss in epoch 182: [-251.13161]\n",
      "Loss in epoch 183: [-251.00981]\n",
      "Loss in epoch 184: [-250.86601]\n",
      "Loss in epoch 185: [-250.73192]\n",
      "Loss in epoch 186: [-250.40424]\n",
      "Loss in epoch 187: [-250.20851]\n",
      "Loss in epoch 188: [-249.89908]\n",
      "Loss in epoch 189: [-249.66029]\n",
      "Loss in epoch 190: [-249.64848]\n",
      "Loss in epoch 191: [-249.48375]\n",
      "Loss in epoch 192: [-249.34161]\n",
      "Loss in epoch 193: [-249.13121]\n",
      "Loss in epoch 194: [-249.05122]\n",
      "Loss in epoch 195: [-249.00125]\n",
      "Loss in epoch 196: [-248.85527]\n",
      "Loss in epoch 197: [-248.67157]\n",
      "Loss in epoch 198: [-248.58286]\n",
      "Loss in epoch 199: [-248.75587]\n",
      "Loss in epoch 200: [-248.61847]\n",
      "Loss in epoch 201: [-248.44453]\n",
      "Loss in epoch 202: [-248.41281]\n",
      "Loss in epoch 203: [-248.25281]\n",
      "Loss in epoch 204: [-248.24649]\n",
      "Loss in epoch 205: [-248.08043]\n",
      "Loss in epoch 206: [-248.06656]\n",
      "Loss in epoch 207: [-247.94949]\n",
      "Loss in epoch 208: [-247.85312]\n",
      "Loss in epoch 209: [-248.14635]\n",
      "Loss in epoch 210: [-248.08205]\n",
      "Loss in epoch 211: [-247.99277]\n",
      "Loss in epoch 212: [-247.87189]\n",
      "Loss in epoch 213: [-247.77469]\n",
      "Loss in epoch 214: [-247.66684]\n",
      "Loss in epoch 215: [-247.66914]\n",
      "Loss in epoch 216: [-247.94693]\n",
      "Loss in epoch 217: [-247.93689]\n",
      "Loss in epoch 218: [-247.89406]\n",
      "Loss in epoch 219: [-247.82784]\n",
      "Loss in epoch 220: [-247.74863]\n",
      "Loss in epoch 221: [-247.70401]\n",
      "Loss in epoch 222: [-247.64859]\n",
      "Loss in epoch 223: [-247.84215]\n",
      "Loss in epoch 224: [-247.8123]\n",
      "Loss in epoch 225: [-247.74712]\n",
      "Loss in epoch 226: [-247.68102]\n",
      "Loss in epoch 227: [-247.67062]\n",
      "Loss in epoch 228: [-247.6479]\n",
      "Loss in epoch 229: [-247.63332]\n",
      "Loss in epoch 230: [-247.57857]\n",
      "Loss in epoch 231: [-247.52521]\n",
      "Loss in epoch 232: [-247.47481]\n",
      "Loss in epoch 233: [-247.47375]\n",
      "Loss in epoch 234: [-247.46309]\n",
      "Loss in epoch 235: [-247.44034]\n",
      "Loss in epoch 236: [-247.39662]\n",
      "Loss in epoch 237: [-247.37437]\n",
      "Loss in epoch 238: [-247.33156]\n",
      "Loss in epoch 239: [-247.33391]\n",
      "Loss in epoch 240: [-247.29266]\n",
      "Loss in epoch 241: [-247.27632]\n",
      "Loss in epoch 242: [-247.24261]\n",
      "Loss in epoch 243: [-247.21417]\n",
      "Loss in epoch 244: [-247.18135]\n",
      "Loss in epoch 245: [-247.15396]\n",
      "Loss in epoch 246: [-247.12543]\n",
      "Loss in epoch 247: [-247.11842]\n",
      "Loss in epoch 248: [-247.09227]\n",
      "Loss in epoch 249: [-247.06476]\n",
      "Loss in epoch 250: [-247.04044]\n",
      "Loss in epoch 251: [-247.04027]\n",
      "Loss in epoch 252: [-247.01395]\n",
      "Loss in epoch 253: [-247.03227]\n",
      "Loss in epoch 254: [-247.01251]\n",
      "Loss in epoch 255: [-246.99483]\n",
      "Loss in epoch 256: [-246.97536]\n",
      "Loss in epoch 257: [-246.98212]\n",
      "Loss in epoch 258: [-246.96758]\n",
      "Loss in epoch 259: [-246.95082]\n",
      "Loss in epoch 260: [-246.95827]\n",
      "Loss in epoch 261: [-246.94489]\n",
      "Loss in epoch 262: [-246.93507]\n",
      "Loss in epoch 263: [-246.9241]\n",
      "Loss in epoch 264: [-246.91817]\n",
      "Loss in epoch 265: [-246.90652]\n",
      "Loss in epoch 266: [-246.90034]\n",
      "Loss in epoch 267: [-246.88895]\n",
      "Loss in epoch 268: [-246.88391]\n",
      "Loss in epoch 269: [-246.87219]\n",
      "Loss in epoch 270: [-246.86804]\n",
      "Loss in epoch 271: [-246.85791]\n",
      "Loss in epoch 272: [-246.8548]\n",
      "Loss in epoch 273: [-246.84515]\n",
      "Loss in epoch 274: [-246.84178]\n",
      "Loss in epoch 275: [-246.83226]\n",
      "Loss in epoch 276: [-246.82887]\n",
      "Loss in epoch 277: [-246.82178]\n",
      "Loss in epoch 278: [-246.82088]\n",
      "Loss in epoch 279: [-246.85654]\n",
      "Loss in epoch 280: [-246.85728]\n",
      "Loss in epoch 281: [-246.85094]\n",
      "Loss in epoch 282: [-246.85068]\n",
      "Loss in epoch 283: [-246.84422]\n",
      "Loss in epoch 284: [-246.84457]\n",
      "Loss in epoch 285: [-246.83578]\n",
      "Loss in epoch 286: [-246.83627]\n",
      "Loss in epoch 287: [-246.82903]\n",
      "Loss in epoch 288: [-246.82896]\n",
      "Loss in epoch 289: [-246.82407]\n",
      "Loss in epoch 290: [-246.82625]\n",
      "Loss in epoch 291: [-246.81335]\n",
      "Loss in epoch 292: [-246.8134]\n",
      "Loss in epoch 293: [-246.80957]\n",
      "Loss in epoch 294: [-246.8129]\n",
      "Loss in epoch 295: [-246.80489]\n",
      "Loss in epoch 296: [-246.80684]\n",
      "Loss in epoch 297: [-246.80153]\n",
      "Loss in epoch 298: [-246.80453]\n",
      "Loss in epoch 299: [-246.79773]\n",
      "Loss in epoch 300: [-246.79965]\n",
      "Loss in epoch 301: [-246.79527]\n",
      "Loss in epoch 302: [-246.79828]\n",
      "Loss in epoch 303: [-246.79027]\n",
      "Loss in epoch 304: [-246.79337]\n",
      "Loss in epoch 305: [-246.78766]\n",
      "Loss in epoch 306: [-246.78493]\n",
      "Loss in epoch 307: [-246.78897]\n",
      "Loss in epoch 308: [-246.78392]\n",
      "Loss in epoch 309: [-246.78485]\n",
      "Loss in epoch 310: [-246.78012]\n",
      "Loss in epoch 311: [-246.78276]\n",
      "Loss in epoch 312: [-246.77832]\n",
      "Loss in epoch 313: [-246.77893]\n",
      "Loss in epoch 314: [-246.77574]\n",
      "Loss in epoch 315: [-246.76752]\n",
      "Loss in epoch 316: [-246.76868]\n",
      "Loss in epoch 317: [-246.76314]\n",
      "Loss in epoch 318: [-246.76797]\n",
      "Loss in epoch 319: [-246.76318]\n",
      "Loss in epoch 320: [-246.78279]\n",
      "Loss in epoch 321: [-246.76376]\n",
      "Loss in epoch 322: [-246.78195]\n",
      "Loss in epoch 323: [-246.76317]\n",
      "Loss in epoch 324: [-246.78139]\n",
      "Loss in epoch 325: [-246.76213]\n",
      "Loss in epoch 326: [-246.78067]\n",
      "Loss in epoch 327: [-246.76149]\n",
      "Loss in epoch 328: [-246.77937]\n",
      "Loss in epoch 329: [-246.76146]\n",
      "Loss in epoch 330: [-246.76976]\n",
      "Loss in epoch 331: [-246.75899]\n",
      "Loss in epoch 332: [-246.75984]\n",
      "Loss in epoch 333: [-246.75706]\n",
      "Loss in epoch 334: [-246.76848]\n",
      "Loss in epoch 335: [-246.75668]\n",
      "Loss in epoch 336: [-246.75877]\n",
      "Loss in epoch 337: [-246.75545]\n",
      "Loss in epoch 338: [-246.75789]\n",
      "Loss in epoch 339: [-246.76906]\n",
      "Loss in epoch 340: [-246.75758]\n",
      "Loss in epoch 341: [-246.75383]\n",
      "Loss in epoch 342: [-246.75539]\n",
      "Loss in epoch 343: [-246.7542]\n",
      "Loss in epoch 344: [-246.75731]\n",
      "Loss in epoch 345: [-246.76808]\n",
      "Loss in epoch 346: [-246.75601]\n",
      "Loss in epoch 347: [-246.76772]\n",
      "Loss in epoch 348: [-246.75522]\n",
      "Loss in epoch 349: [-246.75293]\n",
      "Loss in epoch 350: [-246.7541]\n",
      "Loss in epoch 351: [-246.75319]\n",
      "Loss in epoch 352: [-246.75507]\n",
      "Loss in epoch 353: [-246.7675]\n",
      "Loss in epoch 354: [-246.75574]\n",
      "Loss in epoch 355: [-246.7532]\n",
      "Loss in epoch 356: [-246.75609]\n",
      "Loss in epoch 357: [-246.75264]\n",
      "Loss in epoch 358: [-246.75511]\n",
      "Loss in epoch 359: [-246.75221]\n",
      "Loss in epoch 360: [-246.75497]\n",
      "Loss in epoch 361: [-246.75246]\n",
      "Loss in epoch 362: [-246.75414]\n",
      "Loss in epoch 363: [-246.75258]\n",
      "Loss in epoch 364: [-246.75406]\n",
      "Loss in epoch 365: [-246.7518]\n",
      "Loss in epoch 366: [-246.75426]\n",
      "Loss in epoch 367: [-246.75191]\n",
      "Loss in epoch 368: [-246.7542]\n",
      "Loss in epoch 369: [-246.75156]\n",
      "Loss in epoch 370: [-246.75536]\n",
      "Loss in epoch 371: [-246.75156]\n",
      "Loss in epoch 372: [-246.7543]\n",
      "Loss in epoch 373: [-246.75165]\n",
      "Loss in epoch 374: [-246.75386]\n",
      "Loss in epoch 375: [-246.75154]\n",
      "Loss in epoch 376: [-246.75398]\n",
      "Loss in epoch 377: [-246.75061]\n",
      "Loss in epoch 378: [-246.75264]\n",
      "Loss in epoch 379: [-246.75102]\n",
      "Loss in epoch 380: [-246.75372]\n",
      "Loss in epoch 381: [-246.75078]\n",
      "Loss in epoch 382: [-246.75365]\n",
      "Loss in epoch 383: [-246.75078]\n",
      "Loss in epoch 384: [-246.75572]\n",
      "Loss in epoch 385: [-246.75044]\n",
      "Loss in epoch 386: [-246.75273]\n",
      "Loss in epoch 387: [-246.75082]\n",
      "Loss in epoch 388: [-246.75522]\n",
      "Loss in epoch 389: [-246.75069]\n",
      "Loss in epoch 390: [-246.7551]\n",
      "Loss in epoch 391: [-246.75055]\n",
      "Loss in epoch 392: [-246.75507]\n",
      "Loss in epoch 393: [-246.75043]\n",
      "Loss in epoch 394: [-246.75336]\n",
      "Loss in epoch 395: [-246.75064]\n",
      "Loss in epoch 396: [-246.75397]\n",
      "Loss in epoch 397: [-246.75056]\n",
      "Loss in epoch 398: [-246.75497]\n",
      "Loss in epoch 399: [-246.75021]\n",
      "Loss in epoch 400: [-246.75406]\n",
      "Loss in epoch 401: [-246.75021]\n",
      "Loss in epoch 402: [-246.75301]\n",
      "Loss in epoch 403: [-246.75099]\n",
      "Loss in epoch 404: [-246.75566]\n",
      "Loss in epoch 405: [-246.75058]\n",
      "Loss in epoch 406: [-246.75346]\n",
      "Loss in epoch 407: [-246.75035]\n",
      "Loss in epoch 408: [-246.75362]\n",
      "Loss in epoch 409: [-246.75035]\n",
      "Loss in epoch 410: [-246.75455]\n",
      "Loss in epoch 411: [-246.75043]\n",
      "Loss in epoch 412: [-246.75328]\n",
      "Loss in epoch 413: [-246.75031]\n",
      "Loss in epoch 414: [-246.75383]\n",
      "Loss in epoch 415: [-246.75047]\n",
      "Loss in epoch 416: [-246.75406]\n",
      "Loss in epoch 417: [-246.75058]\n",
      "Loss in epoch 418: [-246.75531]\n",
      "Loss in epoch 419: [-246.74988]\n",
      "Loss in epoch 420: [-246.75264]\n",
      "Loss in epoch 421: [-246.74995]\n",
      "Loss in epoch 422: [-246.7534]\n",
      "Loss in epoch 423: [-246.75037]\n",
      "Loss in epoch 424: [-246.75558]\n",
      "Loss in epoch 425: [-246.75005]\n",
      "Loss in epoch 426: [-246.75313]\n",
      "Loss in epoch 427: [-246.75006]\n",
      "Loss in epoch 428: [-246.75417]\n",
      "Loss in epoch 429: [-246.75012]\n",
      "Loss in epoch 430: [-246.75336]\n",
      "Loss in epoch 431: [-246.75027]\n",
      "Loss in epoch 432: [-246.75369]\n",
      "Loss in epoch 433: [-246.75034]\n",
      "Loss in epoch 434: [-246.75354]\n",
      "Loss in epoch 435: [-246.75041]\n",
      "Loss in epoch 436: [-246.75465]\n",
      "Loss in epoch 437: [-246.7502]\n",
      "Loss in epoch 438: [-246.75264]\n",
      "Loss in epoch 439: [-246.75015]\n",
      "Loss in epoch 440: [-246.75258]\n",
      "Loss in epoch 441: [-246.7502]\n",
      "Loss in epoch 442: [-246.75363]\n",
      "Loss in epoch 443: [-246.75023]\n",
      "Loss in epoch 444: [-246.75493]\n",
      "Loss in epoch 445: [-246.75015]\n",
      "Loss in epoch 446: [-246.75455]\n",
      "Loss in epoch 447: [-246.74995]\n",
      "Loss in epoch 448: [-246.75253]\n",
      "Loss in epoch 449: [-246.75015]\n",
      "Loss in epoch 450: [-246.75462]\n",
      "Loss in epoch 451: [-246.74991]\n",
      "Loss in epoch 452: [-246.7523]\n",
      "Loss in epoch 453: [-246.75015]\n",
      "Loss in epoch 454: [-246.75461]\n",
      "Loss in epoch 455: [-246.75021]\n",
      "Loss in epoch 456: [-246.75414]\n",
      "Loss in epoch 457: [-246.74988]\n",
      "Loss in epoch 458: [-246.75238]\n",
      "Loss in epoch 459: [-246.75]\n",
      "Loss in epoch 460: [-246.75314]\n",
      "Loss in epoch 461: [-246.75008]\n",
      "Loss in epoch 462: [-246.75389]\n",
      "Loss in epoch 463: [-246.75002]\n",
      "Loss in epoch 464: [-246.75403]\n",
      "Loss in epoch 465: [-246.74965]\n",
      "Loss in epoch 466: [-246.75449]\n",
      "Loss in epoch 467: [-246.74988]\n",
      "Loss in epoch 468: [-246.75455]\n",
      "Loss in epoch 469: [-246.74951]\n",
      "Loss in epoch 470: [-246.75307]\n",
      "Loss in epoch 471: [-246.74973]\n",
      "Loss in epoch 472: [-246.75224]\n",
      "Loss in epoch 473: [-246.74971]\n",
      "Loss in epoch 474: [-246.75391]\n",
      "Loss in epoch 475: [-246.74985]\n",
      "Loss in epoch 476: [-246.75453]\n",
      "Loss in epoch 477: [-246.74973]\n",
      "Loss in epoch 478: [-246.75504]\n",
      "Loss in epoch 479: [-246.74977]\n",
      "Loss in epoch 480: [-246.75398]\n",
      "Loss in epoch 481: [-246.74985]\n",
      "Loss in epoch 482: [-246.75459]\n",
      "Loss in epoch 483: [-246.74951]\n",
      "Loss in epoch 484: [-246.752]\n",
      "Loss in epoch 485: [-246.74911]\n",
      "Loss in epoch 486: [-246.75156]\n",
      "Loss in epoch 487: [-246.75096]\n",
      "Loss in epoch 488: [-246.75102]\n",
      "Loss in epoch 489: [-246.75182]\n",
      "Loss in epoch 490: [-246.75175]\n",
      "Loss in epoch 491: [-246.7532]\n",
      "Loss in epoch 492: [-246.7511]\n",
      "Loss in epoch 493: [-246.75278]\n",
      "Loss in epoch 494: [-246.75177]\n",
      "Loss in epoch 495: [-246.75267]\n",
      "Loss in epoch 496: [-246.75113]\n",
      "Loss in epoch 497: [-246.75237]\n",
      "Loss in epoch 498: [-246.75183]\n",
      "Loss in epoch 499: [-246.75235]\n",
      "[91]\n",
      "Optimal Action Squence:[[-0.21070001  0.5       ]\n",
      " [-0.0955      0.5       ]\n",
      " [-0.0307      0.5       ]\n",
      " [-0.1495      0.5       ]\n",
      " [-0.21789999  0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.49950001]\n",
      " [ 0.5         0.49900001]\n",
      " [ 0.5         0.49849999]\n",
      " [ 0.5         0.49790001]\n",
      " [ 0.5         0.49720001]\n",
      " [ 0.5         0.4966    ]\n",
      " [ 0.5         0.49590001]\n",
      " [ 0.5         0.40079999]\n",
      " [ 0.5         0.2098    ]\n",
      " [ 0.5        -0.0011    ]\n",
      " [ 0.49900001 -0.0058    ]\n",
      " [ 0.49770001 -0.0213    ]\n",
      " [ 0.49630001 -0.0007    ]\n",
      " [ 0.3053      0.0208    ]\n",
      " [-0.014      -0.0298    ]\n",
      " [ 0.0076      0.0074    ]\n",
      " [-0.0154     -0.0095    ]\n",
      " [ 0.053       0.0654    ]]\n",
      "Best Cost: -201.9431915283203\n",
      "Sorted Costs:[-201.94319153 -205.31800842 -210.24697876 -218.44802856 -218.45448303\n",
      " -226.19644165 -234.42367554 -237.90003967 -237.90003967 -237.90003967\n",
      " -237.90003967 -237.90003967 -237.90003967 -237.90003967 -237.90003967\n",
      " -237.90003967 -237.90003967 -243.20002747 -243.20002747 -243.20002747\n",
      " -243.20002747 -243.20002747 -243.20002747 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221]\n",
      "MEAN: -238.93263244628906, STD:10.209399223327637\n",
      "The last state:[ 8.02950191  8.02475262]\n",
      "Rewards each time step:[[ -1.60000000e+01]\n",
      " [ -1.55000000e+01]\n",
      " [ -1.50000000e+01]\n",
      " [ -1.45000000e+01]\n",
      " [ -1.40000000e+01]\n",
      " [ -1.35000000e+01]\n",
      " [ -1.25000000e+01]\n",
      " [ -1.15000000e+01]\n",
      " [ -1.05000000e+01]\n",
      " [ -9.50000000e+00]\n",
      " [ -8.69999981e+00]\n",
      " [ -8.10000038e+00]\n",
      " [ -7.49999952e+00]\n",
      " [ -6.90027046e+00]\n",
      " [ -6.30084467e+00]\n",
      " [ -5.70175314e+00]\n",
      " [ -5.10302401e+00]\n",
      " [ -4.50468063e+00]\n",
      " [ -3.90673351e+00]\n",
      " [ -3.30917883e+00]\n",
      " [ -2.76867056e+00]\n",
      " [ -2.20081234e+00]\n",
      " [ -1.80168867e+00]\n",
      " [ -1.30852985e+00]\n",
      " [ -8.32140923e-01]\n",
      " [ -3.36570740e-01]\n",
      " [ -1.05180740e-02]\n",
      " [ -5.42922020e-02]\n",
      " [ -3.92909050e-02]\n",
      " [ -6.42108917e-02]]\n",
      "Intermediate states:[[ 0.          0.5       ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.5       ]\n",
      " [ 0.          2.        ]\n",
      " [ 0.          2.5       ]\n",
      " [ 0.5         3.        ]\n",
      " [ 1.          3.5       ]\n",
      " [ 1.5         4.        ]\n",
      " [ 2.          4.5       ]\n",
      " [ 2.4000001   4.9000001 ]\n",
      " [ 2.70000005  5.20000029]\n",
      " [ 3.          5.50000048]\n",
      " [ 3.29999995  5.79972935]\n",
      " [ 3.5999999   6.09915543]\n",
      " [ 3.89999986  6.39824724]\n",
      " [ 4.19999981  6.69697618]\n",
      " [ 4.5         6.99531937]\n",
      " [ 4.80000019  7.2932663 ]\n",
      " [ 5.10000038  7.59082079]\n",
      " [ 5.40000057  7.83132887]\n",
      " [ 5.80000067  7.99918699]\n",
      " [ 6.20000076  7.99831057]\n",
      " [ 6.69897461  7.99249554]\n",
      " [ 7.19669008  7.97116899]\n",
      " [ 7.69300604  7.97042322]\n",
      " [ 7.99828053  7.9912014 ]\n",
      " [ 7.98430395  7.96140385]\n",
      " [ 7.99189615  7.96881294]\n",
      " [ 7.97646141  7.9593277 ]\n",
      " [ 8.02950191  8.02475262]]\n"
     ]
    }
   ],
   "source": [
    "rnn_inst.Optimize(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'object' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b0651e704537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "results = np.array()\n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
