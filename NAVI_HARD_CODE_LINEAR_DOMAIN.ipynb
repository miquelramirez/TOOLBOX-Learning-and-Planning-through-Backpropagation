{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NAVI_HARD_CODE_DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "#Functional coding\n",
    "import functools\n",
    "from functools import partial\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Path..\n",
    "Datapath=\"DATA/Navigation/linear/Navigation_Data.txt\"\n",
    "Labelpath=\"DATA/Navigation/linear/Navigation_Label.txt\"\n",
    "Rewardpath=\"DATA/Navigation/linear/Navigation_Reward.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given local path, find full path\n",
    "def PathFinder(path):\n",
    "    #python 2\n",
    "    #script_dir = os.path.dirname('__file__')\n",
    "    #fullpath = os.path.join(script_dir,path)\n",
    "    #python 3\n",
    "    fullpath=os.path.abspath(path)\n",
    "    print(fullpath)\n",
    "    return fullpath\n",
    "\n",
    "#Read Data for Deep Learning\n",
    "def ReadData(path):\n",
    "    fullpath=PathFinder(path)\n",
    "    return pd.read_csv(fullpath, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Data.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Label.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Reward.txt\n"
     ]
    }
   ],
   "source": [
    "S_A_pd = ReadData(Datapath)\n",
    "SP_pd = ReadData(Labelpath)\n",
    "R_pd = ReadData(Rewardpath)\n",
    "S_A_matrix=S_A_pd.as_matrix()\n",
    "SP_matrix=SP_pd.as_matrix()\n",
    "R_matrix=R_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_settings = {\n",
    "    \"dims\"          : 2,\n",
    "    \"min_maze_bound\": tf.constant(0.0,dtype=tf.float32), \n",
    "    \"max_maze_bound\": tf.constant(10.0,dtype=tf.float32), \n",
    "    \"min_act_bound\": tf.constant(-1.0,dtype=tf.float32), \n",
    "    \"max_act_bound\": tf.constant(1.0,dtype=tf.float32), \n",
    "    \"goal\"    : tf.constant(8.0,dtype=tf.float32),\n",
    "    \"penalty\" : tf.constant(1000000.0,dtype=tf.float32),\n",
    "    \"centre\"  : tf.constant(5.0,dtype=tf.float32)\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NAVI(object):\n",
    "    def __init__(self, \n",
    "                 default_settings):\n",
    "        self.__dict__.update(default_settings)\n",
    "        self.zero = tf.constant(0,dtype=tf.float32)\n",
    "        self.two = tf.constant(2.0,dtype=tf.float32)\n",
    "        self.one = tf.constant(1.0,dtype=tf.float32)\n",
    "        self.onedsix = tf.constant(1.6,dtype=tf.float32)\n",
    "        self.onedtwo = tf.constant(1.2,dtype=tf.float32)\n",
    "        self.dotnifi = tf.constant(0.95,dtype=tf.float32)\n",
    "        self.doteight = tf.constant(0.8, dtype=tf.float32)\n",
    "        self.dotsix = tf.constant(0.6,dtype=tf.float32)\n",
    "        self.dotfour = tf.constant(0.4,dtype=tf.float32)\n",
    "        self.dottwo = tf.constant(0.2,dtype=tf.float32)\n",
    "        self.dotone = tf.constant(0.1,dtype = tf.float32)\n",
    "    \n",
    "    def MINMAZEBOUND(self, dim):\n",
    "        return self.min_maze_bound\n",
    "    \n",
    "    def MAXMAZEBOUND(self, dim):\n",
    "        return self.max_maze_bound\n",
    "    \n",
    "    def MINACTIONBOUND(self, dim):\n",
    "        return self.min_act_bound\n",
    "    \n",
    "    def MAXACTIONBOUND(self, dim):\n",
    "        return self.max_act_bound\n",
    "    \n",
    "    def GOAL(self, dim):\n",
    "        return self.goal\n",
    "    \n",
    "    def CENTER(self, dim):\n",
    "        return self.centre\n",
    "    \n",
    "    def PENALTY(self):\n",
    "        return self.penalty\n",
    "    \n",
    "    def _transition(self, dim, states_packed, actions_packed):\n",
    "        \n",
    "        states = tf.unpack(states_packed)\n",
    "        actions = tf.unpack(actions_packed)\n",
    "        \n",
    "        previous_state = states[dim]\n",
    "        \n",
    "        #distance to centre Manhattan\n",
    "        distance = self.zero\n",
    "        for i in range(len(states)):\n",
    "            distance+=tf.abs(states[i]-self.CENTER(i))\n",
    "        \n",
    "        #scale factor\n",
    "        #scalefactor = tf.cond(distance<self.two, lambda: distance/self.two, lambda: self.one)\n",
    "        #scalefactor = tf.cond(tf.logical_and(distance<=self.two, distance>self.onedsix),\n",
    "        #                lambda: self.onedsix/self.two,\n",
    "        #               lambda: tf.cond(tf.logical_and(distance<=self.onedsix, distance>self.onedtwo),\n",
    "        #                       lambda: self.onedtwo/self.two,\n",
    "        #                       lambda: tf.cond(tf.logical_and(distance<=self.onedtwo, distance>self.doteight),\n",
    "        #                                lambda: self.doteight/self.two,\n",
    "        #                                lambda: tf.cond(tf.logical_and(distance<=self.doteight, distance>self.dotfour),\n",
    "        #                                        lambda: self.dotfour/self.two,   \n",
    "        #                                        lambda: tf.cond(distance<=self.dotfour,\n",
    "        #                                                lambda: self.dotoofive,\n",
    "        #                                                lambda: self.one\n",
    "        #                                                )      \n",
    "        #                                               \n",
    "        #                                        )\n",
    "        #                                )  \n",
    "        #                        )\n",
    "        #                )\n",
    "        \n",
    "        discountfactor = tf.cond(tf.logical_and(distance<=self.two, distance>self.onedsix),\n",
    "                        lambda: self.dotone,\n",
    "                        lambda: tf.cond(tf.logical_and(distance<=self.onedsix, distance>self.onedtwo),\n",
    "                                lambda: self.dottwo,\n",
    "                                lambda: tf.cond(tf.logical_and(distance<=self.onedtwo, distance>self.doteight),\n",
    "                                        lambda: self.dotfour,\n",
    "                                        lambda: tf.cond(tf.logical_and(distance<=self.doteight, distance>self.dotfour),\n",
    "                                                lambda: self.dotsix,   \n",
    "                                                lambda: tf.cond(distance<=self.dotfour,\n",
    "                                                        lambda: self.dotnifi,\n",
    "                                                        lambda: self.zero\n",
    "                                                        )      \n",
    "                                                       \n",
    "                                                )\n",
    "                                        )  \n",
    "                                )\n",
    "                        )\n",
    "        \n",
    "        #proposed location\n",
    "        proposedLoc = tf.cond(tf.logical_and(actions[dim]>=self.zero, actions[dim]>=discountfactor),\n",
    "                             lambda: previous_state + actions[dim]-discountfactor,\n",
    "                             lambda:tf.cond(tf.logical_and(actions[dim]>=self.zero,actions[dim]<discountfactor),\n",
    "                                            lambda:previous_state,\n",
    "                                            lambda:tf.cond(tf.logical_and(actions[dim]<self.zero,-actions[dim]>=discountfactor),\n",
    "                                                          lambda:previous_state+actions[dim]+discountfactor,\n",
    "                                                          lambda:previous_state\n",
    "                                                          )\n",
    "                                           )\n",
    "                            )\n",
    "        \n",
    "        #new state\n",
    "        new_state = tf.cond(tf.logical_and(proposedLoc <= self.MAXMAZEBOUND(dim), proposedLoc >= self.MINMAZEBOUND(dim)), \\\n",
    "                            lambda: proposedLoc, \\\n",
    "                            lambda: tf.cond(proposedLoc >self.MAXMAZEBOUND(dim), lambda:self.MAXMAZEBOUND(dim), lambda:self.MINMAZEBOUND(dim) ) \\\n",
    "                           )\n",
    "        \n",
    "        return new_state\n",
    "    \n",
    "    # For single data point\n",
    "    def _vector_trans(self, state_size, states_packed, actions_packed):\n",
    "        new_states=[]\n",
    "        for i in range(state_size):\n",
    "            new_states.append(self._transition(i,states_packed,actions_packed))\n",
    "        return tf.pack(new_states)\n",
    "    \n",
    "    def Transition(self, states, actions):\n",
    "        new_states = []\n",
    "        batch_size,state_size = states.get_shape()\n",
    "        states_list = tf.unpack(states)\n",
    "        actions_list = tf.unpack(actions)\n",
    "        for i in range(batch_size):\n",
    "            new_states.append(self._vector_trans(state_size,states_list[i],actions_list[i]))\n",
    "        return tf.pack(new_states)\n",
    "    \n",
    "    def _reward(self, state_size, states_packed, actions_packed):\n",
    "        reward = self.zero\n",
    "        states = tf.unpack(states_packed)\n",
    "        actions = tf.unpack(actions_packed)\n",
    "        \n",
    "        for i in range(state_size):\n",
    "            reward -= tf.abs(states[i]-self.GOAL(i))\n",
    "        return tf.pack([reward])\n",
    "    \n",
    "    def Reward(self, states,actions):\n",
    "        new_rewards = []\n",
    "        batch_size,state_size = states.get_shape()\n",
    "        states_list = tf.unpack(states)\n",
    "        actions_list = tf.unpack(actions)\n",
    "        for i in range(batch_size):\n",
    "            new_rewards.append(self._reward(state_size,states_list[i],actions_list[i]))\n",
    "        return tf.pack(new_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# States\n",
    "states = tf.placeholder(tf.float32,[30, 2],name=\"States\")\n",
    "\n",
    "# Actions\n",
    "actions = tf.placeholder(tf.float32,[30, 2],name=\"Actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "navi_inst = NAVI(default_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.2864389]\n",
      "[array([ 3.58680534,  4.28643894], dtype=float32)]\n",
      "[array([ 3.58680534,  4.28643894], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "states_list=tf.unpack(states)\n",
    "actions_list = tf.unpack(actions)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict={states:S_A_matrix[:30,2:], actions:S_A_matrix[:30,:2]}\n",
    "new_state = navi_inst._transition(1,states_list[8],actions_list[8])\n",
    "print(sess.run([new_state], feed_dict=feed_dict))\n",
    "print(sess.run([states_list[9]], feed_dict=feed_dict))\n",
    "print(sess.run([states_list[9]], feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_list[8].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "feed_dict={states:np.array([[0,0]]*30), actions:np.array([[1.0,1.0]]*30)  }\n",
    "new_state = navi_inst._transition(0,states_list[1],actions_list[1])\n",
    "print(sess.run([new_state], feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rewards = navi_inst.Reward(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-16.        ],\n",
       "       [-15.0486908 ],\n",
       "       [-14.45474052],\n",
       "       [-14.18281746],\n",
       "       [-13.06755447],\n",
       "       [-13.33346748],\n",
       "       [-12.04419136],\n",
       "       [-10.35970879],\n",
       "       [ -9.259552  ],\n",
       "       [ -8.12675571],\n",
       "       [ -7.76328278],\n",
       "       [ -6.64727402],\n",
       "       [-16.        ],\n",
       "       [-16.        ],\n",
       "       [-15.79463196],\n",
       "       [-15.29909897],\n",
       "       [-15.39221764],\n",
       "       [-14.86650181],\n",
       "       [-13.72164154],\n",
       "       [-14.40450287],\n",
       "       [-14.44963074],\n",
       "       [-13.74181557],\n",
       "       [-13.46664429],\n",
       "       [-12.79545021],\n",
       "       [-16.        ],\n",
       "       [-14.33040237],\n",
       "       [-14.17589855],\n",
       "       [-12.88618851],\n",
       "       [-11.73449802],\n",
       "       [-10.57782745]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict={states:S_A_matrix[:30,2:], actions:S_A_matrix[:30,:2]}\n",
    "sess.run(new_rewards,feed_dict=feed_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAVICell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, default_settings):\n",
    "        self._num_state_units = 2\n",
    "        self._num_reward_units = 3\n",
    "        self.navi = NAVI(default_settings)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_state_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_reward_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        next_state =  self.navi.Transition(state, inputs)\n",
    "        reward = self.navi.Reward(state, inputs)      \n",
    "        return tf.concat(1,[reward,next_state]), next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    def __init__(self,\n",
    "                a, # Actions\n",
    "                num_step, # Number of RNN step, this is a fixed step RNN sequence, 12 for navigation\n",
    "                num_act, # Number of actions\n",
    "                batch_size, #Batch Size\n",
    "                learning_rate=0.01): \n",
    "        self.action = tf.reshape(a,[-1,num_step,num_act]) #Reshape rewards\n",
    "        print(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_step = num_step\n",
    "        self.learning_rate = learning_rate\n",
    "        self._p_create_rnn_graph()\n",
    "        self._p_create_loss()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _p_create_rnn_graph(self):\n",
    "        cell = NAVICell(default_settings)\n",
    "        initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        print('action batch size:{0}'.format(array_ops.shape(self.action)[0]))\n",
    "        print('Initial_state shape:{0}'.format(initial_state))\n",
    "        rnn_outputs, state = tf.nn.dynamic_rnn(cell, self.action, dtype=tf.float32,initial_state=initial_state)\n",
    "        #need output intermediate states as well\n",
    "        concated = tf.concat(0,rnn_outputs)\n",
    "        print('concated shape:{0}'.format(concated.get_shape()))\n",
    "        something_unpacked =  tf.unpack(concated, axis=2)\n",
    "        self.outputs = tf.reshape(something_unpacked[0],[-1,self.num_step,1])\n",
    "        print(' self.outputs:{0}'.format(self.outputs.get_shape()))\n",
    "        self.intern_states = tf.pack([something_unpacked[1],something_unpacked[2]], axis=2)\n",
    "        self.last_state = state\n",
    "        self.pred = tf.reduce_sum(self.outputs,1)\n",
    "        print(\"self.pred:{0}\".format(self.pred))\n",
    "            \n",
    "    def _p_create_loss(self):\n",
    "\n",
    "        objective = tf.reduce_mean(self.pred) \n",
    "        self.loss = -objective\n",
    "        print(self.loss.get_shape())\n",
    "        #self.loss = -objective\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def Optimize(self,epoch=100):\n",
    "        \n",
    "        new_loss = self.sess.run([self.loss])\n",
    "        print('Loss in epoch {0}: {1}'.format(\"Initial\", new_loss)) \n",
    "        for epoch in range(epoch):\n",
    "            training = self.sess.run([self.optimizer])\n",
    "            self.sess.run(tf.assign(a, tf.clip_by_value(a, -1, 1)))\n",
    "            if True:\n",
    "                new_loss = self.sess.run([self.loss])\n",
    "                print('Loss in epoch {0}: {1}'.format(epoch, new_loss))  \n",
    "        minimum_costs_id=self.sess.run(tf.argmax(self.pred,0))\n",
    "        print(minimum_costs_id)\n",
    "        best_action = np.round(self.sess.run(self.action)[minimum_costs_id[0]],4)\n",
    "        print('Optimal Action Squence:{0}'.format(best_action))\n",
    "        print('Best Cost: {0}'.format(self.sess.run(self.pred)[minimum_costs_id[0]]))\n",
    "        print('The last state:{0}'.format(self.sess.run(self.last_state)[minimum_costs_id[0]]))\n",
    "        print('Rewards each time step:{0}'.format(self.sess.run(self.outputs)[minimum_costs_id[0]]))\n",
    "        print('Intermediate states:{0}'.format(self.sess.run(self.intern_states)[minimum_costs_id[0]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(10, 12, 2), dtype=float32)\n",
      "action batch size:Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Initial_state shape:Tensor(\"zeros:0\", shape=(10, 2), dtype=float32)\n",
      "concated shape:(10, 12, 3)\n",
      " self.outputs:(10, 12, 1)\n",
      "self.pred:Tensor(\"Sum:0\", shape=(10, 1), dtype=float32)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.truncated_normal(shape=[240],mean=0.0, stddev=0.5),name=\"action\")\n",
    "rnn_inst = ActionOptimizer(a, 12,2,10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch Initial: [175.30144]\n",
      "Loss in epoch 0: [174.77045]\n",
      "Loss in epoch 1: [174.22165]\n",
      "Loss in epoch 2: [173.6534]\n",
      "Loss in epoch 3: [173.04178]\n",
      "Loss in epoch 4: [172.42036]\n",
      "Loss in epoch 5: [171.78891]\n",
      "Loss in epoch 6: [171.14752]\n",
      "Loss in epoch 7: [170.49178]\n",
      "Loss in epoch 8: [169.79788]\n",
      "Loss in epoch 9: [169.09117]\n",
      "Loss in epoch 10: [168.37532]\n",
      "Loss in epoch 11: [167.64957]\n",
      "Loss in epoch 12: [166.90979]\n",
      "Loss in epoch 13: [166.14966]\n",
      "Loss in epoch 14: [165.38274]\n",
      "Loss in epoch 15: [164.61047]\n",
      "Loss in epoch 16: [163.83873]\n",
      "Loss in epoch 17: [163.05852]\n",
      "Loss in epoch 18: [162.27446]\n",
      "Loss in epoch 19: [161.48755]\n",
      "Loss in epoch 20: [160.69441]\n",
      "Loss in epoch 21: [159.89523]\n",
      "Loss in epoch 22: [159.0907]\n",
      "Loss in epoch 23: [158.28146]\n",
      "Loss in epoch 24: [157.46306]\n",
      "Loss in epoch 25: [156.63057]\n",
      "Loss in epoch 26: [155.7907]\n",
      "Loss in epoch 27: [154.94577]\n",
      "Loss in epoch 28: [154.11456]\n",
      "Loss in epoch 29: [153.28305]\n",
      "Loss in epoch 30: [152.44223]\n",
      "Loss in epoch 31: [151.56143]\n",
      "Loss in epoch 32: [150.68074]\n",
      "Loss in epoch 33: [149.71492]\n",
      "Loss in epoch 34: [148.74268]\n",
      "Loss in epoch 35: [147.80893]\n",
      "Loss in epoch 36: [146.89334]\n",
      "Loss in epoch 37: [145.94666]\n",
      "Loss in epoch 38: [144.97285]\n",
      "Loss in epoch 39: [143.94907]\n",
      "Loss in epoch 40: [143.01312]\n",
      "Loss in epoch 41: [142.11288]\n",
      "Loss in epoch 42: [141.19196]\n",
      "Loss in epoch 43: [140.26726]\n",
      "Loss in epoch 44: [139.41187]\n",
      "Loss in epoch 45: [138.46317]\n",
      "Loss in epoch 46: [137.53003]\n",
      "Loss in epoch 47: [136.74551]\n",
      "Loss in epoch 48: [135.86116]\n",
      "Loss in epoch 49: [135.14146]\n",
      "Loss in epoch 50: [134.36368]\n",
      "Loss in epoch 51: [133.49303]\n",
      "Loss in epoch 52: [132.74722]\n",
      "Loss in epoch 53: [132.15015]\n",
      "Loss in epoch 54: [131.33061]\n",
      "Loss in epoch 55: [130.61559]\n",
      "Loss in epoch 56: [129.95273]\n",
      "Loss in epoch 57: [129.26761]\n",
      "Loss in epoch 58: [128.79544]\n",
      "Loss in epoch 59: [128.06062]\n",
      "Loss in epoch 60: [127.42769]\n",
      "Loss in epoch 61: [126.8267]\n",
      "Loss in epoch 62: [126.14406]\n",
      "Loss in epoch 63: [125.63496]\n",
      "Loss in epoch 64: [124.87987]\n",
      "Loss in epoch 65: [124.23651]\n",
      "Loss in epoch 66: [123.81029]\n",
      "Loss in epoch 67: [123.16042]\n",
      "Loss in epoch 68: [122.58063]\n",
      "Loss in epoch 69: [121.88869]\n",
      "Loss in epoch 70: [121.33327]\n",
      "Loss in epoch 71: [120.7416]\n",
      "Loss in epoch 72: [120.34817]\n",
      "Loss in epoch 73: [119.68992]\n",
      "Loss in epoch 74: [119.11406]\n",
      "Loss in epoch 75: [118.53975]\n",
      "Loss in epoch 76: [118.349]\n",
      "Loss in epoch 77: [118.20264]\n",
      "Loss in epoch 78: [117.73268]\n",
      "Loss in epoch 79: [117.13479]\n",
      "Loss in epoch 80: [116.94841]\n",
      "Loss in epoch 81: [116.42295]\n",
      "Loss in epoch 82: [116.37929]\n",
      "Loss in epoch 83: [115.83103]\n",
      "Loss in epoch 84: [115.43296]\n",
      "Loss in epoch 85: [115.20509]\n",
      "Loss in epoch 86: [114.60205]\n",
      "Loss in epoch 87: [114.42322]\n",
      "Loss in epoch 88: [113.89412]\n",
      "Loss in epoch 89: [113.4668]\n",
      "Loss in epoch 90: [112.9718]\n",
      "Loss in epoch 91: [112.88467]\n",
      "Loss in epoch 92: [113.02893]\n",
      "Loss in epoch 93: [112.94714]\n",
      "Loss in epoch 94: [112.54563]\n",
      "Loss in epoch 95: [111.71669]\n",
      "Loss in epoch 96: [111.25283]\n",
      "Loss in epoch 97: [110.97249]\n",
      "Loss in epoch 98: [110.41953]\n",
      "Loss in epoch 99: [110.36517]\n",
      "Loss in epoch 100: [109.98594]\n",
      "Loss in epoch 101: [109.69696]\n",
      "Loss in epoch 102: [109.99658]\n",
      "Loss in epoch 103: [109.7703]\n",
      "Loss in epoch 104: [109.276]\n",
      "Loss in epoch 105: [108.83494]\n",
      "Loss in epoch 106: [108.60545]\n",
      "Loss in epoch 107: [108.44414]\n",
      "Loss in epoch 108: [107.8672]\n",
      "Loss in epoch 109: [108.11568]\n",
      "Loss in epoch 110: [108.85229]\n",
      "Loss in epoch 111: [108.63767]\n",
      "Loss in epoch 112: [108.27303]\n",
      "Loss in epoch 113: [107.96599]\n",
      "Loss in epoch 114: [107.68785]\n",
      "Loss in epoch 115: [107.40578]\n",
      "Loss in epoch 116: [107.11179]\n",
      "Loss in epoch 117: [107.07965]\n",
      "Loss in epoch 118: [107.0135]\n",
      "Loss in epoch 119: [106.75787]\n",
      "Loss in epoch 120: [106.50854]\n",
      "Loss in epoch 121: [106.2607]\n",
      "Loss in epoch 122: [106.02958]\n",
      "Loss in epoch 123: [105.81364]\n",
      "Loss in epoch 124: [105.60561]\n",
      "Loss in epoch 125: [105.40057]\n",
      "Loss in epoch 126: [105.16246]\n",
      "Loss in epoch 127: [104.96622]\n",
      "Loss in epoch 128: [104.88603]\n",
      "Loss in epoch 129: [104.69102]\n",
      "Loss in epoch 130: [104.58559]\n",
      "Loss in epoch 131: [104.39455]\n",
      "Loss in epoch 132: [104.00684]\n",
      "Loss in epoch 133: [103.81047]\n",
      "Loss in epoch 134: [103.78695]\n",
      "Loss in epoch 135: [103.59819]\n",
      "Loss in epoch 136: [103.19141]\n",
      "Loss in epoch 137: [103.01461]\n",
      "Loss in epoch 138: [102.96307]\n",
      "Loss in epoch 139: [101.9734]\n",
      "Loss in epoch 140: [101.7914]\n",
      "Loss in epoch 141: [101.63328]\n",
      "Loss in epoch 142: [101.47746]\n",
      "Loss in epoch 143: [101.32418]\n",
      "Loss in epoch 144: [101.17671]\n",
      "Loss in epoch 145: [101.02604]\n",
      "Loss in epoch 146: [100.8732]\n",
      "Loss in epoch 147: [100.72723]\n",
      "Loss in epoch 148: [99.657562]\n",
      "Loss in epoch 149: [99.482101]\n",
      "Loss in epoch 150: [99.32753]\n",
      "Loss in epoch 151: [99.226234]\n",
      "Loss in epoch 152: [99.074844]\n",
      "Loss in epoch 153: [98.919655]\n",
      "Loss in epoch 154: [99.379715]\n",
      "Loss in epoch 155: [99.06604]\n",
      "Loss in epoch 156: [98.984604]\n",
      "Loss in epoch 157: [98.814011]\n",
      "Loss in epoch 158: [98.683395]\n",
      "Loss in epoch 159: [98.559029]\n",
      "Loss in epoch 160: [98.494858]\n",
      "Loss in epoch 161: [98.350075]\n",
      "Loss in epoch 162: [98.227257]\n",
      "Loss in epoch 163: [98.100601]\n",
      "Loss in epoch 164: [97.981789]\n",
      "Loss in epoch 165: [97.878464]\n",
      "Loss in epoch 166: [98.007202]\n",
      "Loss in epoch 167: [97.890167]\n",
      "Loss in epoch 168: [97.773056]\n",
      "Loss in epoch 169: [97.814529]\n",
      "Loss in epoch 170: [97.705902]\n",
      "Loss in epoch 171: [97.596245]\n",
      "Loss in epoch 172: [97.482849]\n",
      "Loss in epoch 173: [97.38533]\n",
      "Loss in epoch 174: [97.301346]\n",
      "Loss in epoch 175: [97.218887]\n",
      "Loss in epoch 176: [97.137459]\n",
      "Loss in epoch 177: [97.341896]\n",
      "Loss in epoch 178: [97.433716]\n",
      "Loss in epoch 179: [97.340515]\n",
      "Loss in epoch 180: [97.209587]\n",
      "Loss in epoch 181: [97.110931]\n",
      "Loss in epoch 182: [97.029709]\n",
      "Loss in epoch 183: [97.001762]\n",
      "Loss in epoch 184: [96.828293]\n",
      "Loss in epoch 185: [96.737244]\n",
      "Loss in epoch 186: [98.11055]\n",
      "Loss in epoch 187: [98.028381]\n",
      "Loss in epoch 188: [97.947159]\n",
      "Loss in epoch 189: [97.862953]\n",
      "Loss in epoch 190: [97.781921]\n",
      "Loss in epoch 191: [97.697273]\n",
      "Loss in epoch 192: [97.62249]\n",
      "Loss in epoch 193: [97.531921]\n",
      "Loss in epoch 194: [98.046829]\n",
      "Loss in epoch 195: [97.960953]\n",
      "Loss in epoch 196: [97.870979]\n",
      "Loss in epoch 197: [97.910934]\n",
      "Loss in epoch 198: [97.829735]\n",
      "Loss in epoch 199: [97.749062]\n",
      "Loss in epoch 200: [97.667236]\n",
      "Loss in epoch 201: [97.592827]\n",
      "Loss in epoch 202: [97.518402]\n",
      "Loss in epoch 203: [97.452126]\n",
      "Loss in epoch 204: [97.391319]\n",
      "Loss in epoch 205: [97.330101]\n",
      "Loss in epoch 206: [97.2668]\n",
      "Loss in epoch 207: [97.180588]\n",
      "Loss in epoch 208: [97.135559]\n",
      "Loss in epoch 209: [97.083755]\n",
      "Loss in epoch 210: [96.874756]\n",
      "Loss in epoch 211: [96.826874]\n",
      "Loss in epoch 212: [95.53463]\n",
      "Loss in epoch 213: [95.496025]\n",
      "Loss in epoch 214: [95.458862]\n",
      "Loss in epoch 215: [95.415161]\n",
      "Loss in epoch 216: [95.383476]\n",
      "Loss in epoch 217: [95.343658]\n",
      "Loss in epoch 218: [95.306969]\n",
      "Loss in epoch 219: [95.269638]\n",
      "Loss in epoch 220: [95.237778]\n",
      "Loss in epoch 221: [95.201614]\n",
      "Loss in epoch 222: [95.167305]\n",
      "Loss in epoch 223: [95.128433]\n",
      "Loss in epoch 224: [95.099625]\n",
      "Loss in epoch 225: [95.068115]\n",
      "Loss in epoch 226: [95.031204]\n",
      "Loss in epoch 227: [95.003311]\n",
      "Loss in epoch 228: [94.967186]\n",
      "Loss in epoch 229: [94.939232]\n",
      "Loss in epoch 230: [94.908081]\n",
      "Loss in epoch 231: [94.874283]\n",
      "Loss in epoch 232: [94.844574]\n",
      "Loss in epoch 233: [94.813858]\n",
      "Loss in epoch 234: [97.142197]\n",
      "Loss in epoch 235: [97.104652]\n",
      "Loss in epoch 236: [97.067764]\n",
      "Loss in epoch 237: [97.03569]\n",
      "Loss in epoch 238: [97.003937]\n",
      "Loss in epoch 239: [96.974304]\n",
      "Loss in epoch 240: [96.937302]\n",
      "Loss in epoch 241: [96.908066]\n",
      "Loss in epoch 242: [96.876541]\n",
      "Loss in epoch 243: [96.846825]\n",
      "Loss in epoch 244: [96.823204]\n",
      "Loss in epoch 245: [96.794846]\n",
      "Loss in epoch 246: [96.773277]\n",
      "Loss in epoch 247: [96.749107]\n",
      "Loss in epoch 248: [96.721405]\n",
      "Loss in epoch 249: [96.694443]\n",
      "Loss in epoch 250: [96.673134]\n",
      "Loss in epoch 251: [96.644882]\n",
      "Loss in epoch 252: [96.618362]\n",
      "Loss in epoch 253: [96.59446]\n",
      "Loss in epoch 254: [96.569984]\n",
      "Loss in epoch 255: [96.546379]\n",
      "Loss in epoch 256: [96.519844]\n",
      "Loss in epoch 257: [96.498093]\n",
      "Loss in epoch 258: [96.480339]\n",
      "Loss in epoch 259: [96.448463]\n",
      "Loss in epoch 260: [96.424599]\n",
      "Loss in epoch 261: [96.404984]\n",
      "Loss in epoch 262: [96.383728]\n",
      "Loss in epoch 263: [96.355087]\n",
      "Loss in epoch 264: [96.332466]\n",
      "Loss in epoch 265: [96.312286]\n",
      "Loss in epoch 266: [96.29438]\n",
      "Loss in epoch 267: [96.264481]\n",
      "Loss in epoch 268: [96.247841]\n",
      "Loss in epoch 269: [96.224403]\n",
      "Loss in epoch 270: [96.209633]\n",
      "Loss in epoch 271: [96.188461]\n",
      "Loss in epoch 272: [96.179169]\n",
      "Loss in epoch 273: [96.312981]\n",
      "Loss in epoch 274: [96.313873]\n",
      "Loss in epoch 275: [96.306862]\n",
      "Loss in epoch 276: [96.302628]\n",
      "Loss in epoch 277: [96.301308]\n",
      "Loss in epoch 278: [96.303993]\n",
      "Loss in epoch 279: [96.292953]\n",
      "Loss in epoch 280: [96.29184]\n",
      "Loss in epoch 281: [96.290703]\n",
      "Loss in epoch 282: [96.290726]\n",
      "Loss in epoch 283: [96.282883]\n",
      "Loss in epoch 284: [96.281883]\n",
      "Loss in epoch 285: [96.28334]\n",
      "Loss in epoch 286: [96.276772]\n",
      "Loss in epoch 287: [96.274376]\n",
      "Loss in epoch 288: [96.269859]\n",
      "Loss in epoch 289: [96.275742]\n",
      "Loss in epoch 290: [96.266281]\n",
      "Loss in epoch 291: [96.260124]\n",
      "Loss in epoch 292: [96.264938]\n",
      "Loss in epoch 293: [96.262634]\n",
      "Loss in epoch 294: [96.260269]\n",
      "Loss in epoch 295: [96.251358]\n",
      "Loss in epoch 296: [96.254204]\n",
      "Loss in epoch 297: [96.253471]\n",
      "Loss in epoch 298: [96.249855]\n",
      "Loss in epoch 299: [96.242638]\n",
      "Loss in epoch 300: [96.247482]\n",
      "Loss in epoch 301: [96.240936]\n",
      "Loss in epoch 302: [96.237427]\n",
      "Loss in epoch 303: [96.237038]\n",
      "Loss in epoch 304: [96.238182]\n",
      "Loss in epoch 305: [96.231339]\n",
      "Loss in epoch 306: [96.22847]\n",
      "Loss in epoch 307: [96.227921]\n",
      "Loss in epoch 308: [96.228836]\n",
      "Loss in epoch 309: [96.221786]\n",
      "Loss in epoch 310: [96.219437]\n",
      "Loss in epoch 311: [96.220818]\n",
      "Loss in epoch 312: [96.224022]\n",
      "Loss in epoch 313: [96.214859]\n",
      "Loss in epoch 314: [96.211815]\n",
      "Loss in epoch 315: [96.218147]\n",
      "Loss in epoch 316: [96.216721]\n",
      "Loss in epoch 317: [96.211205]\n",
      "Loss in epoch 318: [96.210281]\n",
      "Loss in epoch 319: [96.208954]\n",
      "Loss in epoch 320: [96.211906]\n",
      "Loss in epoch 321: [96.206848]\n",
      "Loss in epoch 322: [96.205284]\n",
      "Loss in epoch 323: [96.204857]\n",
      "Loss in epoch 324: [96.207176]\n",
      "Loss in epoch 325: [96.202553]\n",
      "Loss in epoch 326: [96.200462]\n",
      "Loss in epoch 327: [96.203972]\n",
      "Loss in epoch 328: [96.199013]\n",
      "Loss in epoch 329: [96.198303]\n",
      "Loss in epoch 330: [96.195908]\n",
      "Loss in epoch 331: [96.199944]\n",
      "Loss in epoch 332: [96.194229]\n",
      "Loss in epoch 333: [96.194054]\n",
      "Loss in epoch 334: [96.191353]\n",
      "Loss in epoch 335: [96.195908]\n",
      "Loss in epoch 336: [96.189445]\n",
      "Loss in epoch 337: [96.189819]\n",
      "Loss in epoch 338: [96.186783]\n",
      "Loss in epoch 339: [96.191872]\n",
      "Loss in epoch 340: [96.184677]\n",
      "Loss in epoch 341: [96.185593]\n",
      "Loss in epoch 342: [96.182213]\n",
      "Loss in epoch 343: [96.187828]\n",
      "Loss in epoch 344: [96.179909]\n",
      "Loss in epoch 345: [96.181435]\n",
      "Loss in epoch 346: [96.180832]\n",
      "Loss in epoch 347: [96.180389]\n",
      "Loss in epoch 348: [96.17878]\n",
      "Loss in epoch 349: [96.182083]\n",
      "Loss in epoch 350: [96.17865]\n",
      "Loss in epoch 351: [96.181557]\n",
      "Loss in epoch 352: [96.174812]\n",
      "Loss in epoch 353: [96.178169]\n",
      "Loss in epoch 354: [96.175041]\n",
      "Loss in epoch 355: [96.177902]\n",
      "Loss in epoch 356: [96.171097]\n",
      "Loss in epoch 357: [96.168053]\n",
      "Loss in epoch 358: [96.173988]\n",
      "Loss in epoch 359: [96.172295]\n",
      "Loss in epoch 360: [96.169868]\n",
      "Loss in epoch 361: [96.169075]\n",
      "Loss in epoch 362: [96.170532]\n",
      "Loss in epoch 363: [96.16893]\n",
      "Loss in epoch 364: [96.16626]\n",
      "Loss in epoch 365: [96.165756]\n",
      "Loss in epoch 366: [96.167122]\n",
      "Loss in epoch 367: [96.166672]\n",
      "Loss in epoch 368: [96.170555]\n",
      "Loss in epoch 369: [96.165451]\n",
      "Loss in epoch 370: [96.168785]\n",
      "Loss in epoch 371: [96.1642]\n",
      "Loss in epoch 372: [96.167191]\n",
      "Loss in epoch 373: [96.163162]\n",
      "Loss in epoch 374: [96.165695]\n",
      "Loss in epoch 375: [96.162003]\n",
      "Loss in epoch 376: [96.164268]\n",
      "Loss in epoch 377: [96.161041]\n",
      "Loss in epoch 378: [96.162888]\n",
      "Loss in epoch 379: [96.159935]\n",
      "Loss in epoch 380: [96.161537]\n",
      "Loss in epoch 381: [96.158997]\n",
      "Loss in epoch 382: [96.160217]\n",
      "Loss in epoch 383: [96.157913]\n",
      "Loss in epoch 384: [96.15892]\n",
      "Loss in epoch 385: [96.156998]\n",
      "Loss in epoch 386: [96.157631]\n",
      "Loss in epoch 387: [96.15593]\n",
      "Loss in epoch 388: [96.156349]\n",
      "Loss in epoch 389: [96.155029]\n",
      "Loss in epoch 390: [96.155075]\n",
      "Loss in epoch 391: [96.154053]\n",
      "Loss in epoch 392: [96.157005]\n",
      "Loss in epoch 393: [96.149673]\n",
      "Loss in epoch 394: [96.155777]\n",
      "Loss in epoch 395: [96.148903]\n",
      "Loss in epoch 396: [96.154556]\n",
      "Loss in epoch 397: [96.147881]\n",
      "Loss in epoch 398: [96.153259]\n",
      "Loss in epoch 399: [96.148743]\n",
      "Loss in epoch 400: [96.154198]\n",
      "Loss in epoch 401: [96.14682]\n",
      "Loss in epoch 402: [96.151962]\n",
      "Loss in epoch 403: [96.147499]\n",
      "Loss in epoch 404: [96.15036]\n",
      "Loss in epoch 405: [96.146255]\n",
      "Loss in epoch 406: [96.14856]\n",
      "Loss in epoch 407: [96.148857]\n",
      "Loss in epoch 408: [96.149536]\n",
      "Loss in epoch 409: [96.146629]\n",
      "Loss in epoch 410: [96.147293]\n",
      "Loss in epoch 411: [96.147682]\n",
      "Loss in epoch 412: [96.14827]\n",
      "Loss in epoch 413: [96.14534]\n",
      "Loss in epoch 414: [96.146034]\n",
      "Loss in epoch 415: [96.146645]\n",
      "Loss in epoch 416: [96.147018]\n",
      "Loss in epoch 417: [96.144142]\n",
      "Loss in epoch 418: [96.144775]\n",
      "Loss in epoch 419: [96.14566]\n",
      "Loss in epoch 420: [96.145767]\n",
      "Loss in epoch 421: [96.143005]\n",
      "Loss in epoch 422: [96.143532]\n",
      "Loss in epoch 423: [96.14473]\n",
      "Loss in epoch 424: [96.144516]\n",
      "Loss in epoch 425: [96.141891]\n",
      "Loss in epoch 426: [96.142288]\n",
      "Loss in epoch 427: [96.143814]\n",
      "Loss in epoch 428: [96.143272]\n",
      "Loss in epoch 429: [96.140808]\n",
      "Loss in epoch 430: [96.141037]\n",
      "Loss in epoch 431: [96.142906]\n",
      "Loss in epoch 432: [96.142044]\n",
      "Loss in epoch 433: [96.139732]\n",
      "Loss in epoch 434: [96.139809]\n",
      "Loss in epoch 435: [96.142021]\n",
      "Loss in epoch 436: [96.140793]\n",
      "Loss in epoch 437: [96.138756]\n",
      "Loss in epoch 438: [96.141762]\n",
      "Loss in epoch 439: [96.137634]\n",
      "Loss in epoch 440: [96.139587]\n",
      "Loss in epoch 441: [96.137985]\n",
      "Loss in epoch 442: [96.140556]\n",
      "Loss in epoch 443: [96.136612]\n",
      "Loss in epoch 444: [96.138367]\n",
      "Loss in epoch 445: [96.137192]\n",
      "Loss in epoch 446: [96.139343]\n",
      "Loss in epoch 447: [96.135567]\n",
      "Loss in epoch 448: [96.137131]\n",
      "Loss in epoch 449: [96.136368]\n",
      "Loss in epoch 450: [96.138123]\n",
      "Loss in epoch 451: [96.134521]\n",
      "Loss in epoch 452: [96.13591]\n",
      "Loss in epoch 453: [96.135521]\n",
      "Loss in epoch 454: [96.136894]\n",
      "Loss in epoch 455: [96.133469]\n",
      "Loss in epoch 456: [96.134674]\n",
      "Loss in epoch 457: [96.134666]\n",
      "Loss in epoch 458: [96.135666]\n",
      "Loss in epoch 459: [96.132423]\n",
      "Loss in epoch 460: [96.133438]\n",
      "Loss in epoch 461: [96.133812]\n",
      "Loss in epoch 462: [96.134438]\n",
      "Loss in epoch 463: [96.131371]\n",
      "Loss in epoch 464: [96.132202]\n",
      "Loss in epoch 465: [96.13295]\n",
      "Loss in epoch 466: [96.133194]\n",
      "Loss in epoch 467: [96.130325]\n",
      "Loss in epoch 468: [96.130966]\n",
      "Loss in epoch 469: [96.13208]\n",
      "Loss in epoch 470: [96.131966]\n",
      "Loss in epoch 471: [96.129257]\n",
      "Loss in epoch 472: [96.129738]\n",
      "Loss in epoch 473: [96.13121]\n",
      "Loss in epoch 474: [96.130722]\n",
      "Loss in epoch 475: [96.128212]\n",
      "Loss in epoch 476: [96.128494]\n",
      "Loss in epoch 477: [96.130333]\n",
      "Loss in epoch 478: [96.129494]\n",
      "Loss in epoch 479: [96.127151]\n",
      "Loss in epoch 480: [96.127258]\n",
      "Loss in epoch 481: [96.129463]\n",
      "Loss in epoch 482: [96.12825]\n",
      "Loss in epoch 483: [96.126198]\n",
      "Loss in epoch 484: [96.129219]\n",
      "Loss in epoch 485: [96.125076]\n",
      "Loss in epoch 486: [96.127037]\n",
      "Loss in epoch 487: [96.12545]\n",
      "Loss in epoch 488: [96.127785]\n",
      "Loss in epoch 489: [96.126236]\n",
      "Loss in epoch 490: [96.127762]\n",
      "Loss in epoch 491: [96.125931]\n",
      "Loss in epoch 492: [96.127731]\n",
      "Loss in epoch 493: [96.125717]\n",
      "Loss in epoch 494: [96.127724]\n",
      "Loss in epoch 495: [96.125565]\n",
      "Loss in epoch 496: [96.127708]\n",
      "Loss in epoch 497: [96.12545]\n",
      "Loss in epoch 498: [96.127701]\n",
      "Loss in epoch 499: [96.125366]\n",
      "[8]\n",
      "Optimal Action Squence:[[-0.0782      1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          1.        ]\n",
      " [ 1.          0.99629998]\n",
      " [ 1.          0.9914    ]\n",
      " [ 0.99360001  0.0123    ]\n",
      " [-0.0066     -0.0146    ]\n",
      " [ 0.1008     -0.89499998]]\n",
      "Best Cost: [-85.45031738]\n",
      "The last state:[ 8.08774853  7.09031963]\n",
      "Rewards each time step:[[ -1.60000000e+01]\n",
      " [ -1.50000000e+01]\n",
      " [ -1.30000000e+01]\n",
      " [ -1.10000000e+01]\n",
      " [ -9.00000000e+00]\n",
      " [ -7.00000000e+00]\n",
      " [ -5.80000019e+00]\n",
      " [ -4.60000038e+00]\n",
      " [ -3.00373173e+00]\n",
      " [ -1.01233387e+00]\n",
      " [ -6.49976730e-03]\n",
      " [ -2.77566910e-02]]\n",
      "Intermediate states:[[ 0.          1.        ]\n",
      " [ 1.          2.        ]\n",
      " [ 2.          3.        ]\n",
      " [ 3.          4.        ]\n",
      " [ 4.          5.        ]\n",
      " [ 4.5999999   5.5999999 ]\n",
      " [ 5.19999981  6.19999981]\n",
      " [ 6.          6.99626827]\n",
      " [ 7.          7.98766613]\n",
      " [ 7.99357319  7.99992704]\n",
      " [ 7.9869318   7.98531151]\n",
      " [ 8.08774853  7.09031963]]\n"
     ]
    }
   ],
   "source": [
    "rnn_inst.Optimize(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
