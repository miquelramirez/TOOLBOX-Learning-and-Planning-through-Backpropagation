{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NAVI_HARD_CODE_DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "#Functional coding\n",
    "import functools\n",
    "from functools import partial\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Path..\n",
    "Datapath=\"DATA/Navigation/linear/Navigation_Data.txt\"\n",
    "Labelpath=\"DATA/Navigation/linear/Navigation_Label.txt\"\n",
    "Rewardpath=\"DATA/Navigation/linear/Navigation_Reward.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given local path, find full path\n",
    "def PathFinder(path):\n",
    "    #python 2\n",
    "    #script_dir = os.path.dirname('__file__')\n",
    "    #fullpath = os.path.join(script_dir,path)\n",
    "    #python 3\n",
    "    fullpath=os.path.abspath(path)\n",
    "    print(fullpath)\n",
    "    return fullpath\n",
    "\n",
    "#Read Data for Deep Learning\n",
    "def ReadData(path):\n",
    "    fullpath=PathFinder(path)\n",
    "    return pd.read_csv(fullpath, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Data.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Label.txt\n",
      "/home/wuga/Documents/Notebook/VAE-PLANNING/DATA/Navigation/linear/Navigation_Reward.txt\n"
     ]
    }
   ],
   "source": [
    "S_A_pd = ReadData(Datapath)\n",
    "SP_pd = ReadData(Labelpath)\n",
    "R_pd = ReadData(Rewardpath)\n",
    "S_A_matrix=S_A_pd.as_matrix()\n",
    "SP_matrix=SP_pd.as_matrix()\n",
    "R_matrix=R_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_settings = {\n",
    "    \"dims\"          : 2,\n",
    "    \"min_maze_bound\": tf.constant(0.0,dtype=tf.float32), \n",
    "    \"max_maze_bound\": tf.constant(10.0,dtype=tf.float32), \n",
    "    \"min_act_bound\": tf.constant(-0.5,dtype=tf.float32), \n",
    "    \"max_act_bound\": tf.constant(0.5,dtype=tf.float32), \n",
    "    \"goal\"    : tf.constant(8.0,dtype=tf.float32),\n",
    "    \"penalty\" : tf.constant(1000000.0,dtype=tf.float32),\n",
    "    \"centre\"  : tf.constant(5.0,dtype=tf.float32)\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NAVI(object):\n",
    "    def __init__(self, \n",
    "                 batch_size,\n",
    "                 default_settings):\n",
    "        self.__dict__.update(default_settings)\n",
    "        self.batch_size = batch_size\n",
    "        self.zero = tf.constant(0,shape=[self.batch_size,2],dtype=tf.float32)\n",
    "        self.four = tf.constant(4.0,dtype=tf.float32)\n",
    "        self.one = tf.constant(1.0,shape=[batch_size],dtype=tf.float32)\n",
    "        self.five = tf.constant(5.0,dtype=tf.float32)\n",
    "        self.onedsix = tf.constant(1.6,dtype=tf.float32)\n",
    "        self.onedtwo = tf.constant(1.2,dtype=tf.float32)\n",
    "        self.doteight = tf.constant(0.8,dtype=tf.float32)\n",
    "        self.dotfour = tf.constant(0.4,dtype=tf.float32)\n",
    "        self.dotofive = tf.constant(0.05,shape=[batch_size],dtype = tf.float32)\n",
    "    \n",
    "    def MINMAZEBOUND(self):\n",
    "        return self.min_maze_bound\n",
    "    \n",
    "    def MAXMAZEBOUND(self):\n",
    "        return self.max_maze_bound\n",
    "    \n",
    "    def MINACTIONBOUND(self):\n",
    "        return self.min_act_bound\n",
    "    \n",
    "    def MAXACTIONBOUND(self):\n",
    "        return self.max_act_bound\n",
    "    \n",
    "    def GOAL(self):\n",
    "        return self.goal\n",
    "    \n",
    "    def CENTER(self):\n",
    "        return self.centre\n",
    "    \n",
    "    def PENALTY(self):\n",
    "        return self.penalty\n",
    "    \n",
    "    def Transition(self, states, actions):\n",
    "        previous_state = states\n",
    "        distance = tf.reduce_sum(tf.abs(states-self.CENTER()),1)\n",
    "        scalefactor = tf.select(tf.logical_and(tf.less(distance,self.four),tf.greater_equal(distance,self.doteight)),\\\n",
    "                                tf.floor(distance*self.five/self.four)/self.five,\\\n",
    "                                tf.select(tf.less(distance,self.doteight),self.dotofive,self.one ))\n",
    "        \n",
    "        proposedLoc = previous_state + tf.matrix_transpose(scalefactor*tf.matrix_transpose(actions))\n",
    "        new_states = tf.select(tf.logical_and(tf.less_equal(proposedLoc,self.MAXMAZEBOUND()),tf.greater_equal(proposedLoc,self.MINMAZEBOUND())),\\\n",
    "                               proposedLoc,\\\n",
    "                              tf.select(tf.greater(proposedLoc,self.MAXMAZEBOUND()),\\\n",
    "                                        self.zero+self.MAXMAZEBOUND(),\\\n",
    "                                        self.zero+self.MINMAZEBOUND())\\\n",
    "                              )\n",
    "        return new_states\n",
    "\n",
    "    def Reward(self, states,actions):\n",
    "        new_reward = -tf.reduce_sum(tf.abs(states-self.GOAL()),1,keep_dims=True)\n",
    "        return new_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NAVICell(tf.nn.rnn_cell.RNNCell):\n",
    "\n",
    "    def __init__(self, batch_size, default_settings):\n",
    "        self._num_state_units = 2\n",
    "        self._num_reward_units = 3\n",
    "        self.navi = NAVI(batch_size, default_settings)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_state_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_reward_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        next_state =  self.navi.Transition(state, inputs)\n",
    "        reward = self.navi.Reward(state, inputs)      \n",
    "        return tf.concat(1,[reward,next_state]), next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionOptimizer(object):\n",
    "    def __init__(self,\n",
    "                a, # Actions\n",
    "                num_step, # Number of RNN step, this is a fixed step RNN sequence, 12 for navigation\n",
    "                num_act, # Number of actions\n",
    "                batch_size, #Batch Size\n",
    "                learning_rate=0.005): \n",
    "        self.action = tf.reshape(a,[-1,num_step,num_act]) #Reshape rewards\n",
    "        print(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_step = num_step\n",
    "        self.learning_rate = learning_rate\n",
    "        #self._p_get_weights()\n",
    "        self._p_create_rnn_graph()\n",
    "        self._p_Q_loss()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _p_create_rnn_graph(self):\n",
    "        cell = NAVICell(self.batch_size,default_settings)\n",
    "        initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        print('action batch size:{0}'.format(array_ops.shape(self.action)[0]))\n",
    "        print('Initial_state shape:{0}'.format(initial_state))\n",
    "        rnn_outputs, state = tf.nn.dynamic_rnn(cell, self.action, dtype=tf.float32,initial_state=initial_state)\n",
    "        #need output intermediate states as well\n",
    "        concated = tf.concat(0,rnn_outputs)\n",
    "        print('concated shape:{0}'.format(concated.get_shape()))\n",
    "        something_unpacked =  tf.unpack(concated, axis=2)\n",
    "        self.outputs = tf.reshape(something_unpacked[0],[-1,self.num_step,1])\n",
    "        print(' self.outputs:{0}'.format(self.outputs.get_shape()))\n",
    "        self.intern_states = tf.pack([something_unpacked[1],something_unpacked[2]], axis=2)\n",
    "        self.last_state = state\n",
    "        self.pred = tf.reduce_sum(self.outputs,1)\n",
    "        self.average_pred = tf.reduce_mean(self.pred)\n",
    "        print(\"self.pred:{0}\".format(self.pred))\n",
    "            \n",
    "    def _p_create_loss(self):\n",
    "\n",
    "        #objective = tf.reduce_mean(tf.reduce_sum(self.outputs*self.weight,1)) \n",
    "        objective = self.average_pred \n",
    "        self.loss = -objective\n",
    "        print(self.loss.get_shape())\n",
    "        #self.loss = -objective\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def _p_Q_loss(self):\n",
    "        objective = tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "        for i in range(self.num_step):\n",
    "            Rt = self.outputs[:,i]\n",
    "            SumRj=tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "            SumRk=tf.constant(0.0, shape=[self.batch_size, 1])\n",
    "            if i<(self.num_step-1):\n",
    "                j = i+1\n",
    "                SumRj = tf.reduce_sum(self.outputs[:,j:],1)\n",
    "            #if i<(self.num_step-1):\n",
    "                #k= i+1\n",
    "                #SumRk = tf.reduce_sum(self.outputs[:,k:],1)\n",
    "            objective+=(Rt*(SumRj-SumRk)+tf.square(Rt))/(self.num_step-i)\n",
    "        self.loss = tf.reduce_mean(tf.square(objective))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss, var_list=[a])\n",
    "        \n",
    "    def Optimize(self,epoch=100):\n",
    "        \n",
    "        new_loss = self.sess.run([self.average_pred])\n",
    "        print('Loss in epoch {0}: {1}'.format(\"Initial\", new_loss)) \n",
    "        for epoch in range(epoch):\n",
    "            training = self.sess.run([self.optimizer])\n",
    "            self.sess.run(tf.assign(a, tf.clip_by_value(a, default_settings['min_act_bound'], default_settings['max_act_bound'])))\n",
    "            if True:\n",
    "                new_loss = self.sess.run([self.average_pred])\n",
    "                print('Loss in epoch {0}: {1}'.format(epoch, new_loss))  \n",
    "        minimum_costs_id=self.sess.run(tf.argmax(self.pred,0))\n",
    "        print(minimum_costs_id)\n",
    "        best_action = np.round(self.sess.run(self.action)[minimum_costs_id[0]],4)\n",
    "        print('Optimal Action Squence:{0}'.format(best_action))\n",
    "        pred_list = self.sess.run(self.pred)\n",
    "        pred_list=np.sort(pred_list.flatten())[::-1]\n",
    "        pred_list=pred_list[:50]\n",
    "        pred_mean = np.mean(pred_list)\n",
    "        pred_std = np.std(pred_list)\n",
    "        print('Best Cost: {0}'.format(pred_list[0]))\n",
    "        print('Sorted Costs:{0}'.format(pred_list))\n",
    "        print('MEAN: {0}, STD:{1}'.format(pred_mean,pred_std))\n",
    "        print('The last state:{0}'.format(self.sess.run(self.last_state)[minimum_costs_id[0]]))\n",
    "        print('Rewards each time step:{0}'.format(self.sess.run(self.outputs)[minimum_costs_id[0]]))\n",
    "        print('Intermediate states:{0}'.format(self.sess.run(self.intern_states)[minimum_costs_id[0]]))\n",
    "        \n",
    "    def _p_get_weights(self):\n",
    "        weight_list = np.square(np.arange(1,self.num_step+1))/np.sum(np.square(np.arange(1,self.num_step+1)))\n",
    "        self.weight = tf.reshape(tf.constant(weight_list,dtype=tf.float32),[self.num_step,1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(100, 30, 2), dtype=float32)\n",
      "action batch size:Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Initial_state shape:Tensor(\"zeros:0\", shape=(100, 2), dtype=float32)\n",
      "concated shape:(100, 30, 3)\n",
      " self.outputs:(100, 30, 1)\n",
      "self.pred:Tensor(\"Sum:0\", shape=(100, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.truncated_normal(shape=[6000],mean=0.0, stddev=0.2),name=\"action\")\n",
    "rnn_inst = ActionOptimizer(a, 30,2,100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch Initial: [-455.95844]\n",
      "Loss in epoch 0: [-448.45981]\n",
      "Loss in epoch 1: [-441.64819]\n",
      "Loss in epoch 2: [-435.45523]\n",
      "Loss in epoch 3: [-429.60837]\n",
      "Loss in epoch 4: [-424.03009]\n",
      "Loss in epoch 5: [-418.83038]\n",
      "Loss in epoch 6: [-413.8779]\n",
      "Loss in epoch 7: [-409.07831]\n",
      "Loss in epoch 8: [-404.50415]\n",
      "Loss in epoch 9: [-400.15082]\n",
      "Loss in epoch 10: [-395.99774]\n",
      "Loss in epoch 11: [-392.00888]\n",
      "Loss in epoch 12: [-388.19348]\n",
      "Loss in epoch 13: [-384.45609]\n",
      "Loss in epoch 14: [-380.88617]\n",
      "Loss in epoch 15: [-377.47699]\n",
      "Loss in epoch 16: [-374.11371]\n",
      "Loss in epoch 17: [-370.94067]\n",
      "Loss in epoch 18: [-367.81116]\n",
      "Loss in epoch 19: [-364.74384]\n",
      "Loss in epoch 20: [-361.82068]\n",
      "Loss in epoch 21: [-359.01144]\n",
      "Loss in epoch 22: [-356.31921]\n",
      "Loss in epoch 23: [-353.74841]\n",
      "Loss in epoch 24: [-351.20175]\n",
      "Loss in epoch 25: [-348.73523]\n",
      "Loss in epoch 26: [-346.39178]\n",
      "Loss in epoch 27: [-344.02618]\n",
      "Loss in epoch 28: [-341.75638]\n",
      "Loss in epoch 29: [-339.6441]\n",
      "Loss in epoch 30: [-337.55652]\n",
      "Loss in epoch 31: [-335.45282]\n",
      "Loss in epoch 32: [-333.35193]\n",
      "Loss in epoch 33: [-331.36807]\n",
      "Loss in epoch 34: [-329.47009]\n",
      "Loss in epoch 35: [-327.62558]\n",
      "Loss in epoch 36: [-325.72415]\n",
      "Loss in epoch 37: [-324.03311]\n",
      "Loss in epoch 38: [-322.30862]\n",
      "Loss in epoch 39: [-320.52982]\n",
      "Loss in epoch 40: [-318.92523]\n",
      "Loss in epoch 41: [-317.18433]\n",
      "Loss in epoch 42: [-315.55081]\n",
      "Loss in epoch 43: [-314.12155]\n",
      "Loss in epoch 44: [-312.61844]\n",
      "Loss in epoch 45: [-311.07126]\n",
      "Loss in epoch 46: [-309.6998]\n",
      "Loss in epoch 47: [-308.25653]\n",
      "Loss in epoch 48: [-306.92184]\n",
      "Loss in epoch 49: [-305.60019]\n",
      "Loss in epoch 50: [-304.29083]\n",
      "Loss in epoch 51: [-302.99033]\n",
      "Loss in epoch 52: [-301.74759]\n",
      "Loss in epoch 53: [-300.37878]\n",
      "Loss in epoch 54: [-299.22455]\n",
      "Loss in epoch 55: [-298.02328]\n",
      "Loss in epoch 56: [-296.91174]\n",
      "Loss in epoch 57: [-295.8866]\n",
      "Loss in epoch 58: [-294.64328]\n",
      "Loss in epoch 59: [-293.49219]\n",
      "Loss in epoch 60: [-292.43823]\n",
      "Loss in epoch 61: [-291.28442]\n",
      "Loss in epoch 62: [-290.42657]\n",
      "Loss in epoch 63: [-289.32114]\n",
      "Loss in epoch 64: [-288.4032]\n",
      "Loss in epoch 65: [-287.51727]\n",
      "Loss in epoch 66: [-286.6178]\n",
      "Loss in epoch 67: [-285.63153]\n",
      "Loss in epoch 68: [-284.79922]\n",
      "Loss in epoch 69: [-283.84637]\n",
      "Loss in epoch 70: [-283.02469]\n",
      "Loss in epoch 71: [-282.08655]\n",
      "Loss in epoch 72: [-281.38712]\n",
      "Loss in epoch 73: [-280.53198]\n",
      "Loss in epoch 74: [-279.70706]\n",
      "Loss in epoch 75: [-278.99371]\n",
      "Loss in epoch 76: [-278.3385]\n",
      "Loss in epoch 77: [-277.6998]\n",
      "Loss in epoch 78: [-276.88632]\n",
      "Loss in epoch 79: [-276.20526]\n",
      "Loss in epoch 80: [-275.52899]\n",
      "Loss in epoch 81: [-274.71201]\n",
      "Loss in epoch 82: [-274.11853]\n",
      "Loss in epoch 83: [-273.4249]\n",
      "Loss in epoch 84: [-272.83118]\n",
      "Loss in epoch 85: [-272.18347]\n",
      "Loss in epoch 86: [-271.55716]\n",
      "Loss in epoch 87: [-271.04575]\n",
      "Loss in epoch 88: [-270.56357]\n",
      "Loss in epoch 89: [-269.75555]\n",
      "Loss in epoch 90: [-269.18173]\n",
      "Loss in epoch 91: [-268.53897]\n",
      "Loss in epoch 92: [-267.95526]\n",
      "Loss in epoch 93: [-267.49243]\n",
      "Loss in epoch 94: [-267.02277]\n",
      "Loss in epoch 95: [-266.4238]\n",
      "Loss in epoch 96: [-265.841]\n",
      "Loss in epoch 97: [-265.36539]\n",
      "Loss in epoch 98: [-265.00519]\n",
      "Loss in epoch 99: [-264.61044]\n",
      "Loss in epoch 100: [-264.16339]\n",
      "Loss in epoch 101: [-263.78003]\n",
      "Loss in epoch 102: [-263.21976]\n",
      "Loss in epoch 103: [-262.75888]\n",
      "Loss in epoch 104: [-262.30225]\n",
      "Loss in epoch 105: [-262.01755]\n",
      "Loss in epoch 106: [-261.74139]\n",
      "Loss in epoch 107: [-261.30539]\n",
      "Loss in epoch 108: [-260.91879]\n",
      "Loss in epoch 109: [-260.6636]\n",
      "Loss in epoch 110: [-260.20108]\n",
      "Loss in epoch 111: [-259.77579]\n",
      "Loss in epoch 112: [-259.40768]\n",
      "Loss in epoch 113: [-259.01523]\n",
      "Loss in epoch 114: [-258.64597]\n",
      "Loss in epoch 115: [-258.38843]\n",
      "Loss in epoch 116: [-257.88925]\n",
      "Loss in epoch 117: [-257.67407]\n",
      "Loss in epoch 118: [-257.49042]\n",
      "Loss in epoch 119: [-257.15024]\n",
      "Loss in epoch 120: [-256.81992]\n",
      "Loss in epoch 121: [-256.40973]\n",
      "Loss in epoch 122: [-255.90645]\n",
      "Loss in epoch 123: [-255.7151]\n",
      "Loss in epoch 124: [-255.58516]\n",
      "Loss in epoch 125: [-255.48683]\n",
      "Loss in epoch 126: [-254.97371]\n",
      "Loss in epoch 127: [-254.69112]\n",
      "Loss in epoch 128: [-254.4117]\n",
      "Loss in epoch 129: [-253.91153]\n",
      "Loss in epoch 130: [-253.79504]\n",
      "Loss in epoch 131: [-253.31856]\n",
      "Loss in epoch 132: [-252.935]\n",
      "Loss in epoch 133: [-252.89125]\n",
      "Loss in epoch 134: [-252.58977]\n",
      "Loss in epoch 135: [-252.49155]\n",
      "Loss in epoch 136: [-252.22528]\n",
      "Loss in epoch 137: [-252.19734]\n",
      "Loss in epoch 138: [-252.0582]\n",
      "Loss in epoch 139: [-251.94344]\n",
      "Loss in epoch 140: [-251.58055]\n",
      "Loss in epoch 141: [-251.42084]\n",
      "Loss in epoch 142: [-251.43423]\n",
      "Loss in epoch 143: [-251.26724]\n",
      "Loss in epoch 144: [-250.99023]\n",
      "Loss in epoch 145: [-250.97707]\n",
      "Loss in epoch 146: [-250.90181]\n",
      "Loss in epoch 147: [-250.7811]\n",
      "Loss in epoch 148: [-250.54547]\n",
      "Loss in epoch 149: [-250.35785]\n",
      "Loss in epoch 150: [-250.17691]\n",
      "Loss in epoch 151: [-249.94823]\n",
      "Loss in epoch 152: [-249.95911]\n",
      "Loss in epoch 153: [-249.73695]\n",
      "Loss in epoch 154: [-249.53262]\n",
      "Loss in epoch 155: [-249.38237]\n",
      "Loss in epoch 156: [-249.34688]\n",
      "Loss in epoch 157: [-249.14195]\n",
      "Loss in epoch 158: [-249.06503]\n",
      "Loss in epoch 159: [-249.15106]\n",
      "Loss in epoch 160: [-249.10625]\n",
      "Loss in epoch 161: [-249.19176]\n",
      "Loss in epoch 162: [-249.14423]\n",
      "Loss in epoch 163: [-248.93378]\n",
      "Loss in epoch 164: [-248.79808]\n",
      "Loss in epoch 165: [-248.72362]\n",
      "Loss in epoch 166: [-248.52356]\n",
      "Loss in epoch 167: [-248.40332]\n",
      "Loss in epoch 168: [-248.25333]\n",
      "Loss in epoch 169: [-248.19223]\n",
      "Loss in epoch 170: [-248.10051]\n",
      "Loss in epoch 171: [-248.07149]\n",
      "Loss in epoch 172: [-247.92152]\n",
      "Loss in epoch 173: [-247.84207]\n",
      "Loss in epoch 174: [-247.76256]\n",
      "Loss in epoch 175: [-247.72359]\n",
      "Loss in epoch 176: [-247.49023]\n",
      "Loss in epoch 177: [-247.495]\n",
      "Loss in epoch 178: [-247.41847]\n",
      "Loss in epoch 179: [-247.43109]\n",
      "Loss in epoch 180: [-247.47633]\n",
      "Loss in epoch 181: [-247.49896]\n",
      "Loss in epoch 182: [-247.3427]\n",
      "Loss in epoch 183: [-247.33894]\n",
      "Loss in epoch 184: [-247.2317]\n",
      "Loss in epoch 185: [-247.26886]\n",
      "Loss in epoch 186: [-247.15549]\n",
      "Loss in epoch 187: [-247.21664]\n",
      "Loss in epoch 188: [-247.08722]\n",
      "Loss in epoch 189: [-247.0976]\n",
      "Loss in epoch 190: [-247.07106]\n",
      "Loss in epoch 191: [-247.26006]\n",
      "Loss in epoch 192: [-247.14211]\n",
      "Loss in epoch 193: [-247.16393]\n",
      "Loss in epoch 194: [-247.05206]\n",
      "Loss in epoch 195: [-247.07848]\n",
      "Loss in epoch 196: [-246.9689]\n",
      "Loss in epoch 197: [-247.00465]\n",
      "Loss in epoch 198: [-246.90044]\n",
      "Loss in epoch 199: [-246.95711]\n",
      "Loss in epoch 200: [-246.85326]\n",
      "Loss in epoch 201: [-246.97739]\n",
      "Loss in epoch 202: [-246.87781]\n",
      "Loss in epoch 203: [-246.92287]\n",
      "Loss in epoch 204: [-246.82568]\n",
      "Loss in epoch 205: [-246.86858]\n",
      "Loss in epoch 206: [-246.77296]\n",
      "Loss in epoch 207: [-246.81876]\n",
      "Loss in epoch 208: [-246.74855]\n",
      "Loss in epoch 209: [-246.79773]\n",
      "Loss in epoch 210: [-246.70746]\n",
      "Loss in epoch 211: [-246.76277]\n",
      "Loss in epoch 212: [-246.67586]\n",
      "Loss in epoch 213: [-246.73016]\n",
      "Loss in epoch 214: [-246.64195]\n",
      "Loss in epoch 215: [-246.72043]\n",
      "Loss in epoch 216: [-246.63506]\n",
      "Loss in epoch 217: [-246.6915]\n",
      "Loss in epoch 218: [-246.60454]\n",
      "Loss in epoch 219: [-246.66489]\n",
      "Loss in epoch 220: [-246.57637]\n",
      "Loss in epoch 221: [-246.64133]\n",
      "Loss in epoch 222: [-246.55988]\n",
      "Loss in epoch 223: [-246.62238]\n",
      "Loss in epoch 224: [-246.54132]\n",
      "Loss in epoch 225: [-246.605]\n",
      "Loss in epoch 226: [-246.52484]\n",
      "Loss in epoch 227: [-246.58972]\n",
      "Loss in epoch 228: [-246.548]\n",
      "Loss in epoch 229: [-246.61252]\n",
      "Loss in epoch 230: [-246.53297]\n",
      "Loss in epoch 231: [-246.59894]\n",
      "Loss in epoch 232: [-246.51982]\n",
      "Loss in epoch 233: [-246.58777]\n",
      "Loss in epoch 234: [-246.51167]\n",
      "Loss in epoch 235: [-246.5798]\n",
      "Loss in epoch 236: [-246.504]\n",
      "Loss in epoch 237: [-246.57364]\n",
      "Loss in epoch 238: [-246.49792]\n",
      "Loss in epoch 239: [-246.56796]\n",
      "Loss in epoch 240: [-246.49223]\n",
      "Loss in epoch 241: [-246.56219]\n",
      "Loss in epoch 242: [-246.48438]\n",
      "Loss in epoch 243: [-246.55704]\n",
      "Loss in epoch 244: [-246.47781]\n",
      "Loss in epoch 245: [-246.5515]\n",
      "Loss in epoch 246: [-246.47264]\n",
      "Loss in epoch 247: [-246.548]\n",
      "Loss in epoch 248: [-246.46945]\n",
      "Loss in epoch 249: [-246.54297]\n",
      "Loss in epoch 250: [-246.46637]\n",
      "Loss in epoch 251: [-246.58336]\n",
      "Loss in epoch 252: [-246.50696]\n",
      "Loss in epoch 253: [-246.58052]\n",
      "Loss in epoch 254: [-246.50354]\n",
      "Loss in epoch 255: [-246.57689]\n",
      "Loss in epoch 256: [-246.50426]\n",
      "Loss in epoch 257: [-246.57426]\n",
      "Loss in epoch 258: [-246.49742]\n",
      "Loss in epoch 259: [-246.5713]\n",
      "Loss in epoch 260: [-246.49405]\n",
      "Loss in epoch 261: [-246.56816]\n",
      "Loss in epoch 262: [-246.49117]\n",
      "Loss in epoch 263: [-246.5658]\n",
      "Loss in epoch 264: [-246.48802]\n",
      "Loss in epoch 265: [-246.56236]\n",
      "Loss in epoch 266: [-246.48531]\n",
      "Loss in epoch 267: [-246.55945]\n",
      "Loss in epoch 268: [-246.48199]\n",
      "Loss in epoch 269: [-246.55684]\n",
      "Loss in epoch 270: [-246.47873]\n",
      "Loss in epoch 271: [-246.55336]\n",
      "Loss in epoch 272: [-246.47603]\n",
      "Loss in epoch 273: [-246.55086]\n",
      "Loss in epoch 274: [-246.47308]\n",
      "Loss in epoch 275: [-246.5477]\n",
      "Loss in epoch 276: [-246.46988]\n",
      "Loss in epoch 277: [-246.54399]\n",
      "Loss in epoch 278: [-246.46786]\n",
      "Loss in epoch 279: [-246.5417]\n",
      "Loss in epoch 280: [-246.46449]\n",
      "Loss in epoch 281: [-246.53926]\n",
      "Loss in epoch 282: [-246.4613]\n",
      "Loss in epoch 283: [-246.53563]\n",
      "Loss in epoch 284: [-246.45879]\n",
      "Loss in epoch 285: [-246.53299]\n",
      "Loss in epoch 286: [-246.45453]\n",
      "Loss in epoch 287: [-246.5325]\n",
      "Loss in epoch 288: [-246.45531]\n",
      "Loss in epoch 289: [-246.53075]\n",
      "Loss in epoch 290: [-246.44868]\n",
      "Loss in epoch 291: [-246.52492]\n",
      "Loss in epoch 292: [-246.44711]\n",
      "Loss in epoch 293: [-246.52824]\n",
      "Loss in epoch 294: [-246.45078]\n",
      "Loss in epoch 295: [-246.52629]\n",
      "Loss in epoch 296: [-246.44644]\n",
      "Loss in epoch 297: [-246.52489]\n",
      "Loss in epoch 298: [-246.44606]\n",
      "Loss in epoch 299: [-246.52489]\n",
      "Loss in epoch 300: [-246.4492]\n",
      "Loss in epoch 301: [-246.52617]\n",
      "Loss in epoch 302: [-246.4456]\n",
      "Loss in epoch 303: [-246.52455]\n",
      "Loss in epoch 304: [-246.44536]\n",
      "Loss in epoch 305: [-246.52437]\n",
      "Loss in epoch 306: [-246.44852]\n",
      "Loss in epoch 307: [-246.52565]\n",
      "Loss in epoch 308: [-246.44438]\n",
      "Loss in epoch 309: [-246.524]\n",
      "Loss in epoch 310: [-246.44424]\n",
      "Loss in epoch 311: [-246.52432]\n",
      "Loss in epoch 312: [-246.44746]\n",
      "Loss in epoch 313: [-246.52539]\n",
      "Loss in epoch 314: [-246.44376]\n",
      "Loss in epoch 315: [-246.52429]\n",
      "Loss in epoch 316: [-246.44691]\n",
      "Loss in epoch 317: [-246.52583]\n",
      "Loss in epoch 318: [-246.44305]\n",
      "Loss in epoch 319: [-246.52397]\n",
      "Loss in epoch 320: [-246.44644]\n",
      "Loss in epoch 321: [-246.52582]\n",
      "Loss in epoch 322: [-246.44258]\n",
      "Loss in epoch 323: [-246.52426]\n",
      "Loss in epoch 324: [-246.44571]\n",
      "Loss in epoch 325: [-246.52582]\n",
      "Loss in epoch 326: [-246.44199]\n",
      "Loss in epoch 327: [-246.52422]\n",
      "Loss in epoch 328: [-246.44522]\n",
      "Loss in epoch 329: [-246.52614]\n",
      "Loss in epoch 330: [-246.44133]\n",
      "Loss in epoch 331: [-246.52402]\n",
      "Loss in epoch 332: [-246.44496]\n",
      "Loss in epoch 333: [-246.52589]\n",
      "Loss in epoch 334: [-246.4411]\n",
      "Loss in epoch 335: [-246.52419]\n",
      "Loss in epoch 336: [-246.44446]\n",
      "Loss in epoch 337: [-246.52589]\n",
      "Loss in epoch 338: [-246.44061]\n",
      "Loss in epoch 339: [-246.52437]\n",
      "Loss in epoch 340: [-246.44377]\n",
      "Loss in epoch 341: [-246.52592]\n",
      "Loss in epoch 342: [-246.4402]\n",
      "Loss in epoch 343: [-246.52472]\n",
      "Loss in epoch 344: [-246.44341]\n",
      "Loss in epoch 345: [-246.52614]\n",
      "Loss in epoch 346: [-246.43996]\n",
      "Loss in epoch 347: [-246.52449]\n",
      "Loss in epoch 348: [-246.44293]\n",
      "Loss in epoch 349: [-246.52611]\n",
      "Loss in epoch 350: [-246.43958]\n",
      "Loss in epoch 351: [-246.52448]\n",
      "Loss in epoch 352: [-246.44252]\n",
      "Loss in epoch 353: [-246.52628]\n",
      "Loss in epoch 354: [-246.43903]\n",
      "Loss in epoch 355: [-246.52446]\n",
      "Loss in epoch 356: [-246.44212]\n",
      "Loss in epoch 357: [-246.52628]\n",
      "Loss in epoch 358: [-246.43867]\n",
      "Loss in epoch 359: [-246.52446]\n",
      "Loss in epoch 360: [-246.44191]\n",
      "Loss in epoch 361: [-246.52608]\n",
      "Loss in epoch 362: [-246.43832]\n",
      "Loss in epoch 363: [-246.52446]\n",
      "Loss in epoch 364: [-246.44153]\n",
      "Loss in epoch 365: [-246.52606]\n",
      "Loss in epoch 366: [-246.43797]\n",
      "Loss in epoch 367: [-246.52443]\n",
      "Loss in epoch 368: [-246.44118]\n",
      "Loss in epoch 369: [-246.52602]\n",
      "Loss in epoch 370: [-246.43764]\n",
      "Loss in epoch 371: [-246.52457]\n",
      "Loss in epoch 372: [-246.44061]\n",
      "Loss in epoch 373: [-246.52597]\n",
      "Loss in epoch 374: [-246.4375]\n",
      "Loss in epoch 375: [-246.52435]\n",
      "Loss in epoch 376: [-246.44025]\n",
      "Loss in epoch 377: [-246.52596]\n",
      "Loss in epoch 378: [-246.43719]\n",
      "Loss in epoch 379: [-246.52432]\n",
      "Loss in epoch 380: [-246.4399]\n",
      "Loss in epoch 381: [-246.52594]\n",
      "Loss in epoch 382: [-246.43689]\n",
      "Loss in epoch 383: [-246.52428]\n",
      "Loss in epoch 384: [-246.43954]\n",
      "Loss in epoch 385: [-246.52606]\n",
      "Loss in epoch 386: [-246.4364]\n",
      "Loss in epoch 387: [-246.52441]\n",
      "Loss in epoch 388: [-246.43878]\n",
      "Loss in epoch 389: [-246.52629]\n",
      "Loss in epoch 390: [-246.43573]\n",
      "Loss in epoch 391: [-246.52451]\n",
      "Loss in epoch 392: [-246.43854]\n",
      "Loss in epoch 393: [-246.52615]\n",
      "Loss in epoch 394: [-246.43547]\n",
      "Loss in epoch 395: [-246.52464]\n",
      "Loss in epoch 396: [-246.43826]\n",
      "Loss in epoch 397: [-246.52603]\n",
      "Loss in epoch 398: [-246.43521]\n",
      "Loss in epoch 399: [-246.5247]\n",
      "Loss in epoch 400: [-246.43777]\n",
      "Loss in epoch 401: [-246.52594]\n",
      "Loss in epoch 402: [-246.43513]\n",
      "Loss in epoch 403: [-246.52443]\n",
      "Loss in epoch 404: [-246.4375]\n",
      "Loss in epoch 405: [-246.52583]\n",
      "Loss in epoch 406: [-246.43491]\n",
      "Loss in epoch 407: [-246.52344]\n",
      "Loss in epoch 408: [-246.43729]\n",
      "Loss in epoch 409: [-246.52478]\n",
      "Loss in epoch 410: [-246.4348]\n",
      "Loss in epoch 411: [-246.52321]\n",
      "Loss in epoch 412: [-246.43707]\n",
      "Loss in epoch 413: [-246.52489]\n",
      "Loss in epoch 414: [-246.4343]\n",
      "Loss in epoch 415: [-246.52348]\n",
      "Loss in epoch 416: [-246.43507]\n",
      "Loss in epoch 417: [-246.52687]\n",
      "Loss in epoch 418: [-246.43234]\n",
      "Loss in epoch 419: [-246.52242]\n",
      "Loss in epoch 420: [-246.43401]\n",
      "Loss in epoch 421: [-246.52406]\n",
      "Loss in epoch 422: [-246.43124]\n",
      "Loss in epoch 423: [-246.52567]\n",
      "Loss in epoch 424: [-246.43385]\n",
      "Loss in epoch 425: [-246.52399]\n",
      "Loss in epoch 426: [-246.43092]\n",
      "Loss in epoch 427: [-246.52245]\n",
      "Loss in epoch 428: [-246.43306]\n",
      "Loss in epoch 429: [-246.52379]\n",
      "Loss in epoch 430: [-246.43039]\n",
      "Loss in epoch 431: [-246.52179]\n",
      "Loss in epoch 432: [-246.43288]\n",
      "Loss in epoch 433: [-246.52324]\n",
      "Loss in epoch 434: [-246.43039]\n",
      "Loss in epoch 435: [-246.52155]\n",
      "Loss in epoch 436: [-246.43242]\n",
      "Loss in epoch 437: [-246.52289]\n",
      "Loss in epoch 438: [-246.42999]\n",
      "Loss in epoch 439: [-246.52121]\n",
      "Loss in epoch 440: [-246.43211]\n",
      "Loss in epoch 441: [-246.52283]\n",
      "Loss in epoch 442: [-246.42963]\n",
      "Loss in epoch 443: [-246.52109]\n",
      "Loss in epoch 444: [-246.4319]\n",
      "Loss in epoch 445: [-246.52272]\n",
      "Loss in epoch 446: [-246.42946]\n",
      "Loss in epoch 447: [-246.52095]\n",
      "Loss in epoch 448: [-246.4317]\n",
      "Loss in epoch 449: [-246.5226]\n",
      "Loss in epoch 450: [-246.42926]\n",
      "Loss in epoch 451: [-246.5209]\n",
      "Loss in epoch 452: [-246.43121]\n",
      "Loss in epoch 453: [-246.52277]\n",
      "Loss in epoch 454: [-246.42867]\n",
      "Loss in epoch 455: [-246.52107]\n",
      "Loss in epoch 456: [-246.43088]\n",
      "Loss in epoch 457: [-246.52258]\n",
      "Loss in epoch 458: [-246.42856]\n",
      "Loss in epoch 459: [-246.52118]\n",
      "Loss in epoch 460: [-246.43071]\n",
      "Loss in epoch 461: [-246.5224]\n",
      "Loss in epoch 462: [-246.42842]\n",
      "Loss in epoch 463: [-246.52101]\n",
      "Loss in epoch 464: [-246.43085]\n",
      "Loss in epoch 465: [-246.52237]\n",
      "Loss in epoch 466: [-246.42863]\n",
      "Loss in epoch 467: [-246.52072]\n",
      "Loss in epoch 468: [-246.43062]\n",
      "Loss in epoch 469: [-246.52226]\n",
      "Loss in epoch 470: [-246.42845]\n",
      "Loss in epoch 471: [-246.52063]\n",
      "Loss in epoch 472: [-246.43044]\n",
      "Loss in epoch 473: [-246.52217]\n",
      "Loss in epoch 474: [-246.4283]\n",
      "Loss in epoch 475: [-246.52051]\n",
      "Loss in epoch 476: [-246.43024]\n",
      "Loss in epoch 477: [-246.52206]\n",
      "Loss in epoch 478: [-246.42813]\n",
      "Loss in epoch 479: [-246.52045]\n",
      "Loss in epoch 480: [-246.42975]\n",
      "Loss in epoch 481: [-246.52882]\n",
      "Loss in epoch 482: [-246.42946]\n",
      "Loss in epoch 483: [-246.52887]\n",
      "Loss in epoch 484: [-246.42934]\n",
      "Loss in epoch 485: [-246.52875]\n",
      "Loss in epoch 486: [-246.42905]\n",
      "Loss in epoch 487: [-246.52882]\n",
      "Loss in epoch 488: [-246.42892]\n",
      "Loss in epoch 489: [-246.52872]\n",
      "Loss in epoch 490: [-246.42865]\n",
      "Loss in epoch 491: [-246.52882]\n",
      "Loss in epoch 492: [-246.42856]\n",
      "Loss in epoch 493: [-246.52872]\n",
      "Loss in epoch 494: [-246.42844]\n",
      "Loss in epoch 495: [-246.52863]\n",
      "Loss in epoch 496: [-246.42818]\n",
      "Loss in epoch 497: [-246.52873]\n",
      "Loss in epoch 498: [-246.42809]\n",
      "Loss in epoch 499: [-246.52866]\n",
      "[0]\n",
      "Optimal Action Squence:[[-0.0747      0.5       ]\n",
      " [-0.1649      0.5       ]\n",
      " [-0.0044      0.5       ]\n",
      " [-0.0807      0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.5         0.37529999]\n",
      " [ 0.5         0.0106    ]\n",
      " [ 0.5        -0.0021    ]\n",
      " [ 0.5        -0.0017    ]\n",
      " [ 0.4012      0.0039    ]\n",
      " [ 0.0839     -0.15880001]]\n",
      "Best Cost: -218.4390106201172\n",
      "Sorted Costs:[-218.43901062 -218.46208191 -218.46221924 -218.49240112 -219.12142944\n",
      " -234.44099426 -234.44386292 -237.90003967 -237.90003967 -237.90003967\n",
      " -237.90003967 -237.90003967 -237.90003967 -237.90003967 -237.90003967\n",
      " -237.90003967 -237.90003967 -243.20002747 -243.20002747 -243.20002747\n",
      " -243.20002747 -243.20002747 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221\n",
      " -244.20001221 -244.20001221 -244.20001221 -244.20001221 -244.20001221]\n",
      "MEAN: -239.88925170898438, STD:7.670082092285156\n",
      "The last state:[ 8.08511734  7.85213041]\n",
      "Rewards each time step:[[ -1.60000000e+01]\n",
      " [ -1.55000000e+01]\n",
      " [ -1.50000000e+01]\n",
      " [ -1.45000000e+01]\n",
      " [ -1.40000000e+01]\n",
      " [ -1.30000000e+01]\n",
      " [ -1.20000000e+01]\n",
      " [ -1.10000000e+01]\n",
      " [ -1.00000000e+01]\n",
      " [ -9.00000000e+00]\n",
      " [ -8.39999962e+00]\n",
      " [ -8.00000000e+00]\n",
      " [ -7.60000038e+00]\n",
      " [ -7.20000029e+00]\n",
      " [ -6.80000019e+00]\n",
      " [ -6.40000057e+00]\n",
      " [ -6.00000095e+00]\n",
      " [ -5.60000134e+00]\n",
      " [ -5.20000172e+00]\n",
      " [ -4.80000210e+00]\n",
      " [ -4.40000248e+00]\n",
      " [ -4.00000286e+00]\n",
      " [ -3.60000324e+00]\n",
      " [ -3.20000362e+00]\n",
      " [ -2.60000324e+00]\n",
      " [ -1.90020895e+00]\n",
      " [ -1.41084242e+00]\n",
      " [ -9.08746243e-01]\n",
      " [ -4.07047749e-01]\n",
      " [ -1.21421814e-02]]\n",
      "Intermediate states:[[ 0.          0.5       ]\n",
      " [ 0.          1.        ]\n",
      " [ 0.          1.5       ]\n",
      " [ 0.          2.        ]\n",
      " [ 0.5         2.5       ]\n",
      " [ 1.          3.        ]\n",
      " [ 1.5         3.5       ]\n",
      " [ 2.          4.        ]\n",
      " [ 2.5         4.5       ]\n",
      " [ 2.79999995  4.80000019]\n",
      " [ 3.          5.        ]\n",
      " [ 3.20000005  5.19999981]\n",
      " [ 3.4000001   5.39999962]\n",
      " [ 3.60000014  5.59999943]\n",
      " [ 3.80000019  5.79999924]\n",
      " [ 4.          5.99999905]\n",
      " [ 4.19999981  6.19999886]\n",
      " [ 4.39999962  6.39999866]\n",
      " [ 4.59999943  6.59999847]\n",
      " [ 4.79999924  6.79999828]\n",
      " [ 4.99999905  6.99999809]\n",
      " [ 5.19999886  7.1999979 ]\n",
      " [ 5.39999866  7.39999771]\n",
      " [ 5.69999886  7.6999979 ]\n",
      " [ 6.09999895  8.0002079 ]\n",
      " [ 6.59999895  8.01084137]\n",
      " [ 7.09999895  8.00874519]\n",
      " [ 7.59999895  8.0070467 ]\n",
      " [ 8.00124264  8.01089954]\n",
      " [ 8.08511734  7.85213041]]\n"
     ]
    }
   ],
   "source": [
    "rnn_inst.Optimize(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'object' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b0651e704537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "results = np.array()\n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
